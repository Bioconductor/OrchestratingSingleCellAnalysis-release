[
["index.html", "Orchestrating Single-Cell Analysis with Bioconductor Welcome", " Orchestrating Single-Cell Analysis with Bioconductor Robert A. Amezquita Stephanie C. Hicks 2019-06-19 Welcome This is the website for “Orchestrating Single-Cell Analysis with Bioconductor”, a book that teaches users some common workflows for the analysis of single-cell RNA-seq data (scRNA-seq). This book will teach you how to make use of cutting-edge Bioconductor tools to process, analyze, visualize, and explore scRNA-seq data. Additionally, it serves as an online companion for the manuscript “Orchestrating Single-Cell Analysis with Bioconductor”. While we focus here on scRNA-seq data, a newer technology that profiles transcriptomes at the single-cell level, many of the tools, conventions, and analysis strategies utilized throughout this book are broadly applicable to other types of assays. By learning the grammar of Bioconductor workflows, we hope to provide you a starting point for the exploration of your own data, whether it be scRNA-seq or otherwise. This book is organized into two parts. In the Preamble, we introduce the book and dive into resources for learning R and Bioconductor (both at a beginner and developer level). Part I ends with a tutorial for a key data infrastructure, the SingleCellExperiment class, that is used throughout Bioconductor for single-cell analysis and in the subsequent section. This section can be safely skipped by readers already familiar with R. The second part, Workflows, provides templates for performing single-cell RNA-seq analyses across various objectives. In these templates, we take various datasets from raw data through to preprocessing and finally to the objective at hand, using packages that are referred to in the main manuscript. The book is written in RMarkdown with bookdown. OSCA is a collaborative effort, supported by various folks from the Bioconductor team who have contributed workflows, fixes, and improvements. This website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License. "],
["introduction.html", "Chapter 1 Introduction 1.1 What you will learn 1.2 What you won’t learn 1.3 Who we wrote this for 1.4 Why we wrote this 1.5 Acknowledgements", " Chapter 1 Introduction Bioconductor is an open source, open development software project to provide tools for the analysis and comprehension of high-throughput genomic data. It is based primarily on the R programming language. 1.1 What you will learn The goal of this book is to provide a solid foundation in the usage of Bioconductor tools for single-cell RNA-seq analysis by walking through various steps of typical workflows using example datasets. We strive to tackle key concepts covered in the manuscript, “Orchestrating Single-Cell Analysis with Bioconductor”, with each workflow covering these in varying detail, as well as essential preliminaries that are important for following along with the workflows on your own. 1.1.1 Preliminaries For those unfamiliar with R (and those looking to learn more), we recommend reading the Learning R and More chapter, which first and foremost covers how to get started with R. We point to many great online resources for learning R, as well as related tools that are nice to know for bioinformatic analysis. For advanced users, we also point to some extra resources that go beyond the basics. While we provide an extensive list of learning resources for the interested audience in this chapter, we only ask for some familiarity with R before going to the next section. We then briefly cover getting started with Using R and Bioconductor. Bioconductor, being its own repository, has a unique set of tools, documentation, resources, and practices that benefit from some extra explanation. Data Infrastructure merits a separate chapter. The reason for this is that common data containers are an essential part of Bioconductor workflows because they enable interoperability across packages, allowing for “plug and play” usage of cutting-edge tools. Specifically, here we cover the SingleCellExperiment class in depth, as it has become the working standard for Bioconductor based single-cell analysis packages. Finally, before diving into the various workflows, armed with knowledge about the SingleCellExperiment class, we briefly discuss the datasets that will be used throughout the book in About the Data. 1.1.2 Workflows All workflows begin with data import and subsequent quality control and normalization, going from a raw (count) expression matrix to a clean one. This includes adjusting for experimental factors and possibly even latent factors. Using the clean expression matrix, feature selection strategies can be applied to select the features (genes) driving heterogeneity. Furthermore, these features can then be used to perform dimensionality reduction, which enables downstream analysis that would not otherwise be possible and visualization on 2- or 3-dimensions. From there, the workflows largely focus on differing downstream analyses. Clustering details how to segment a scRNA-seq dataset, and differential expression provides a means to determine what drives the differences between different groups of cells. Integrating datasets walks through merging scRNA-seq datasets, an area of need as the number of scRNA-seq datasets continues to grow and comparisons between datasets must be done. Finally, we touch upon how to work with large scale data, specifically where it becomes impractical or impossible to work with data solely in-memory. As an added bonus, we dedicate a chapter to interactive visualization, which focuses on using the iSEE package to enable active exploration of a single cell experiment’s data. 1.2 What you won’t learn The field of bioinformatic analysis is large and filled with many potential trajectories depending on the biological system being studied and technology being deployed. Here, we only briefly survey some of the many tools available for the analysis of scRNA-seq, focusing on Bioconductor packages. It is impossible to thoroughly review the plethora of tools available through R and Bioconductor for biological analysis in one book, but we hope to provide the means for further exploration on your own. Thus, it goes without saying that you may not learn the optimal workflow for your own data from our examples - while we strive to provide high quality templates, they should be treated as just that - a template from which to extend upon for your own analyses. 1.3 Who we wrote this for We’ve written this book with the interested experimental biologist in mind, and do our best to make few assumptions on previous programming or statistical experience. Likewise, we also welcome more seasoned bioinformaticians who are looking for a starting point from which to dive into single-cell RNA-seq analysis. As such, we welcome any and all feedback for improving this book to help increase accessibility and refine technical details. 1.4 Why we wrote this This book was conceived in the fall of 2018, as single-cell RNA-seq analysis continued its rise in prominence in the field of biology. With its rapid growth, and the ongoing developments within Bioconductor tailored specifically for scRNA-seq, it became apparent that an update to the Orchestrating high-throughput genomic analysis with Bioconductor paper was necessary for the age of single-cell studies. We strive to highlight the fantastic software by people who call Bioconductor home for their tools, and in the process hope to showcase the Bioconductor community at large in continually pushing forward the field of biological analysis. 1.5 Acknowledgements We would like to thank all Bioconductor contributors for their efforts in creating the definitive leading-edge repository of software for biological analysis. It is truly extraordinary to chart the growth of Bioconductor over the years. We are thankful for the wonderful community of scientists and developers alike that together make the Bioconductor community special. We would first and foremost like to thank the Bioconductor core team and the emerging targets subcommittee for commissioning this work, Stephanie Hicks and Raphael Gottardo for their continuous mentorship, and all our contributors to the companion manuscript of this book. We’d also like to thank Garret Grolemund and Hadley Wickham for their book, R for Data Science, from which we drew stylistic and teaching inspiration. We also thank Levi Waldron and Aaron Lun for advice on the code-related aspects of managing the online version of this book. "],
["learning-r-and-more.html", "Chapter 2 Learning R and Bioconductor 2.1 The Benefits of R and Bioconductor 2.2 Learning R Online 2.3 Running R Locally 2.4 Getting Help In (and Out) of R 2.5 Bioconductor Help", " Chapter 2 Learning R and Bioconductor In this chapter, we outline various resources for learning R and Bioconductor. We provide a brief set of instructions for installing R on your own machine, and then cover how to get help for functions, packages, and Bioconductor-specific resources for learning more. 2.1 The Benefits of R and Bioconductor R is a high-level programming language that was initially designed for statistical applications. While there is much to be said about R as a programming language, one of the key advantages of using R is that it is highly extensible through packages. Packages are collections of functions, data, and documentation that extend the capabilities of base R. The ease of development and distribution of packages for R has made it a rich environment for many fields of study and application. One of the primary ways in which packages are distributed is through centralized repositories. The first R repository a user typically runs into is the Comprehensive R Archive Network (CRAN), which hosts over 13,000 packages to date, and is home to many of the most popular R packages. Similar to CRAN, Bioconductor is a repository of R packages as well. However, whereas CRAN is a general purpose repository, Bioconductor focuses on software tailored for genomic analysis. Furthermore, Bioconductor has stricter requirements for a package to be accepted into the repository. Of particular interest to us is the inclusion of high quality documentation and the use of common data infrastructure to promote package interoperability. In order to use these packages from CRAN and Bioconductor, and start programming with R to follow along in these workflows, some knowledge of R is helpful. Here we outline resources to guide you through learning the basics. 2.2 Learning R Online To learn more about programming with R, we highly recommend checking out the online courses offered by Datacamp, which includes both introductory and advanced courses within the R track. Datacamp is all online with many free courses, with videos and a code editor/console that promotes an interactive learning experience. What we like about Datacamp is that it is more focused on topics and programming paradigms that center around data science, which is especially helpful for getting started with R. Beyond just Datacamp, a mainstay resource for learning R is the R for Data Science book. This book illustrates R programming through the exploration of various data science concepts - transformation, visualization, exploration, and more. 2.3 Running R Locally While learning R through online resources is a great way to start with R, as it requires minimal knowledge to start up, at some point, it will be desirable to have a local installation - on your own hardware - of R. This will allow you to install and maintain your own software and code, and furthermore allow you to create a personalized workspace. 2.3.1 Installing R Prior to getting started with this book, some prior programming experience with R is helpful. Check out the Learning R and More chapter for a list of resources to get started with R and other useful tools for bioinformatic analysis. To follow along with the analysis workflows in this book on your personal computer, it is first necessary to install the R programming language. Additionally, we recommend a graphical user interface such as RStudio for programming in R and visualization. RStudio features many helpful tools, such as code completion and an interactive data viewer to name but two. For more details, please see the online book R for Data Science prerequisites section for more information about installing R and using RStudio. 2.3.1.1 For MacOS/Linux Users A special note for MacOS/Linux users: we highly recommend using a package manager to manage your R installation. This differs across different Linux distributions, but for MacOS we highly recommend the Homebrew package manager. Follow the website directions to install homebrew, and install R via the commandline with brew install R, and it will automatically configure your installation for you. Upgrading to new R versions can be done by running brew upgrade. 2.3.2 Installing R &amp; Bioconductor Packages After installing R, the next step is to install R packages. In the R console, you can install packages from CRAN via the install.packages() function. In order to install Bioconductor packages, we will first need the BiocManager package which is hosted on CRAN. This can be done by running: install.packages(&quot;BiocManager&quot;) The BiocManager package makes it easy to install packages from the Bioconductor repository. For example, to install the SingleCellExperiment package, we run: ## the command below is a one-line shortcut for: ## library(BiocManager) ## install(&quot;SingleCellExperiment&quot;) BiocManager::install(&quot;SingleCellExperiment&quot;) Throughout the book, we can load packages via the library() function, which by convention usually comes at the top of scripts to alert readers as to what packages are required. For example, to load the SingleCellExperiment package, we run: library(SingleCellExperiment) Many packages will be referenced throughout the book within the workflows, and similar to the above, can be installed using the BiocManager::install() function. 2.4 Getting Help In (and Out) of R One of the most helpful parts of R is being able to get help inside of R. For example, to get the manual associated with a function, class, dataset, or package, you can prepend the code of interest with a ? to retrieve the relevant help page. For example, to get information about the data.frame() function, the SingleCellExperiment class, the in-built iris dataset, or for the BiocManager package, you can type: ?data.frame ?SingleCellExperiment ?iris ?BiocManager Beyond the R console, there are myriad online resources to get help. The R for Data Science book has a great section dedicated to looking for help outside of R. In particular, Stackoverflow’s R tag is a helpful resource for asking and exploring general R programming questions. 2.5 Bioconductor Help One of the key tenets of Bioconductor software that makes it stand out from CRAN is the required documentation of packages and workflows. In addition, Bioconductor hosts a Bioconductor-specific support site that has grown into a valuable resource of its own, thanks to the work of dedicated volunteers. 2.5.1 Bioconductor Packages Each package hosted on Bioconductor has a dedicated page with various resources. For an example, looking at the scater package page on Bioconductor, we see that it contains: a brief description of the package at the top, in addition to the authors, maintainer, and an associated citation installation instructions that can be cut and paste into your R console documentation - vignettes, reference manual, news Here, the most important information comes from the documentation section. Every package in Bioconductor is required to be submitted with a vignette - a document showcasing basic functionality of the package. Typically, these vignettes have a descriptive title that summarizes the main objective of the vignette. These vignettes are a great resource for learning how to operate the essential functionality of the package. The reference manual contains a comprehensive listing of all the functions available in the package. This is a compilation of each function’s manual, aka help pages, which can be accessed programmatically in the R console via ?&lt;function&gt;. Finally, the NEWS file contains notes from the authors which highlight changes across different versions of the package. This is a great way of tracking changes, especially functions that are added, removed, or deprecated, in order to keep your scripts current with new versions of dependent packages. Below this, the Details section covers finer nuances of the package, mostly relating to its relationship to other packages: upstream dependencies (Depends, Imports, Suggests fields): packages that are imported upon loading the given package downstream dependencies (Depends On Me, Imports Me, Suggests Me): packages that import the given package when loaded For example, we can see that an entry called simpleSingle in the Depends On Me field on the scater page takes us to a step-by-step workflow for low-level analysis of single-cell RNA-seq data. One additional Details entry, the biocViews, is helpful for looking at how the authors annotate their package. For example, for the scater package, we see that it is associated with DataImport, DimensionReduction, GeneExpression, RNASeq, and SingleCell, to name but some of its many annotations. We cover biocViews in more detail. 2.5.2 biocViews To find packages via the Bioconductor website, one useful resource is the BiocViews page, which provides a hierarchically organized view of annotations associated with Bioconductor packages. Under the “Software” label for example (which is comprised of most of the Bioconductor packages), there exist many different views to explore packages. For example, we can inspect based on the associated “Technology”, and explore “Sequencing” associated packages, and furthermore subset based on “RNASeq”. Another area of particular interest is the “Workflow” view, which provides Bioconductor packages that illustrate an analytical workflow. For example, the “SingleCellWorkflow” contains the aforementioned tutorial, encapsulated in the simpleSingleCell package. 2.5.3 Bioconductor Forums The Bioconductor support site contains a Stackoverflow-style question and answer support site that is actively contributed to from both users and package developers. Thanks to the work of dedicated volunteers, there are ample questions to explore to learn more about Bioconductor specific workflows. Another way to connect with the Bioconductor community is through Slack, which hosts various channels dedicated to packages and workflows. The Bioc-community Slack is a great way to stay in the loop on the latest developments happening across Bioconductor, and we recommend exploring the “Channels” section to find topics of interest. "],
["beyond-r-basics.html", "Chapter 3 Beyond R Basics 3.1 Becoming an R Expert 3.2 Becoming an R/Bioconductor Developer 3.3 Nice Companions for R", " Chapter 3 Beyond R Basics Here we briefly outline resources for taking your R programming to the next level, including resources for learning about package development. We also outline some companions to R that are good to know not only for package development, but also for running your own bioinformatic pipelines, enabling you to use a broader array of tools to go from raw data to preprocessed data before working in R. 3.1 Becoming an R Expert For a deeper dive into the finer details of the R programming language, the Advanced R. While targeted at more experienced R users and programmers, this book represents a comprehensive compendium of more advanced concepts, and touches on some of the paradigms used extensively by developers throughout Bioconductor, specifically programming with S4. Eventually, you’ll reach the point where you have your own collection of functions, datasets, and reach the point where you will be writing your own packages. Luckily, there’s a guide for just that, with the book R Packages. Packages are great even if just for personal use, and of course, with some polishing, can eventually become a package available on CRAN or Bioconductor. Furthermore, they are also a great way of putting together code associated with a manuscript, promoting reproducible, accessible computing practices, something we all strive for in our work. For many of the little details that are oft forgotten learning about R, the aptly named What They Forgot to Teach You About R is a great read for learning about the little things such as file naming, maintaining an R installation, and reproducible analysis habits. Finally, we save the most intriguing resource for last - another book for those on the road to becoming an R expert is R Inferno, which dives into many of the unique quirks of R. Warning: this book goes very deep into the painstaking details of R. 3.2 Becoming an R/Bioconductor Developer While learning to use Bioconductor tools is a very welcoming experience, unfortunately there is no central resource for navigating the plethora of gotchas and paradigms associated with developing for Bioconductor. Based on conversations with folks involved in developing for Bioconductor, much of this knowledge is hard won and fairly spread out. This however is beginning to change with more recent efforts led by the Bioconductor team, and while this book represents an earnest effort towards addressing the user perspective, it is currently out of scope to include a deep dive about the developer side. For those looking to get started with developing packages for Bioconductor, it is important to first become acquainted with developing standalone R packages. To this end, the R Packages book provides a deep dive into the details of constructing your own package, as well as details regarding submission of a package to CRAN. For programming practices, With that, some resources that are worth looking into to get started are the BiocWorkshops repository under the Bioconductor Github provides a book composed of workshops that have been hosted by Bioconductor team members and contributors. These workshops center around learning, using, and developing for Bioconductor. A host of topics are also available via the Learn module on the Bioconductor website as well. Finally, the Bioconductor developers portal contains a bevy of individual resources and guides for experienced R developers. 3.3 Nice Companions for R While not essential for our purposes, many bioinformatic tools for processing raw sequencing data require knowledge beyond just R to install, run, and import their results into R for further analysis. The most important of which are basic knowledge of the Shell/Bash utilities, for working with bioinformatic pipelines and troubleshooting (R package) installation issues. Additionally, for working with packages or software that are still in development and not hosted on an official repository like CRAN or Bioconductor, knowledge of Git - a version control system - and the popular Github repository is helpful. This enables you to not only work with other people’s code, but also better manage your own code to keep track of changes. 3.3.1 Shell/Bash Datacamp and other interactive online resources such as Codecademy are great places to learn some of these extra skills. We highly recommend learning Shell/Bash, as it is the starting point for most bioinformatic processing pipelines. 3.3.2 Git We would recommend learning Git next, a system for code versioning control which underlies the popular Github repository, where many of the most popular open source tools are hosted. Learning Git is essential for not only keeping track of your own code, but also for using, managing, and contributing to open source software projects. For a more R centric look at using Git (and Github), we highly recommend checking out Happy Git and Github for the useR. 3.3.3 Other Languages A frequent question that comes up is “What else should I learn besides R?” Firstly, we believe that honing your R skills is first and foremost, and beyond just R, learning Shell/Bash and Git covered in the Nice Friends for R section are already a great start. For those just getting started, these skills should become comfortable in practice before moving on. However, there are indeed benefits to going beyond just R. At a basic level, learning other programming languages helps broaden one’s perspective - similar to learning multiple spoken or written languages, learning about other programming languages (even if only in a cursory manner) helps one identify broader patterns that may be applicable across languages. At an applied level, work within and outside of R has made it ever more friendly now than ever before with multi-lingual setups and teams, enabling the use of the best tool for the job at hand. For example, Python is another popular language used in both data science and a broader array of applications as well. R now supports a native Python interface via the reticulate package, enabling access to tools developed originally in Python such as the popular TensorFlow framework for machine learning applications. C++ is frequently used natively in R as well via Rcpp in packages to massively accelerate computations. Finally, multiple langauges are supported in code documents and reports through R Markdown. "],
["data-infrastructure.html", "Chapter 4 Data Infrastructure 4.1 Prerequisites 4.2 The SingleCellExperiment Class 4.3 A Brief Recap: From se to sce 4.4 The reducedDims Slot 4.5 One More Thing: metadata Slot 4.6 About Spike-Ins 4.7 Working with SingleCellExperiment 4.8 Multimodal Data: MultiAssayExperiment", " Chapter 4 Data Infrastructure One of the advantages of using Bioconductor packages is that they utilize common data infrastructures which makes analyses interoperable across various packages. Furthermore, much engineering effort is put into making this infrastructure robust and scalable. Here, we describe the SingleCellExperiment object (or sce in shorthand) in detail to describe how it is constructed, utilized in downstream analysis, and how it stores various types of primary data and metadata. 4.1 Prerequisites The Bioconductor package SingleCellExperiment provides the SingleCellExperiment class for usage. While the package is implicitly installed and loaded when using any package that depends on the sce class, it can be explicitly installed (and loaded) as follows: BiocManager::install(&#39;SingleCellExperiment&#39;) Additionally, we use some functions from the scater and scran packages, as well as the CRAN package uwot (which conveniently can also be installed through BiocManager). These functions will be accessed through the &lt;package&gt;::&lt;function&gt; convention as needed. BiocManager::install(c(&#39;scater&#39;, &#39;scran&#39;, &#39;uwot&#39;)) For this session, all we will need loaded is the SingleCellExperiment package: library(SingleCellExperiment) 4.2 The SingleCellExperiment Class Figure 1. Overview of the SingleCellExperiment class object 4.2.1 Primary Data: The assays Slot The SingleCellExperiment (sce) object is the basis of single-cell analytical applications based in Bioconductor. The sce object is an S4 object, which in essence provides a more formalized approach towards construction and accession of data compared to other methods available in R. The utility of S4 comes from validity checks that ensure that safe data manipulation, and most important for our discussion, from its extensibility through slots. If we imagine the sce object to be a ship, the slots of sce can be thought of as individual cargo boxes - each exists as a separate entity within the sce object. Furthermore, each slot contains data that arrives in its own format. To extend the metaphor, we can imagine that different variations of cargo boxes are required for fruits versus bricks. In the case of sce, certain slots expect numeric matrices, whereas others may expect data frames. To construct a rudimentary sce object, all we need is a single slot: assays slot: contains primary data such as counts in list, where each entry of the list is in a matrix format, where rows correspond to features (genes) and columns correspond to samples (cells) (Figure 1A, blue box) Let’s start simple by generating three cells worth of count data across ten genes. counts_matrix &lt;- data.frame(cell_1 = rpois(10, 10), cell_2 = rpois(10, 10), cell_3 = rpois(10, 30)) rownames(counts_matrix) &lt;- paste0(&quot;gene_&quot;, 1:10) counts_matrix &lt;- as.matrix(counts_matrix) # must be a matrix object! From this, we can now construct our first SingleCellExperiment object, using the defined constructor, SingleCellExperiment(). Note that we provide our data as a named list, and each entry of the list is a matrix. Here, we name the counts_matrix entry as simply counts within the list. sce &lt;- SingleCellExperiment(assays = list(counts = counts_matrix)) To inspect the object, we can simply type sce into the console to see some pertinent information, which will display an overview of the various slots available to us (which may or may not have any data). sce ## class: SingleCellExperiment ## dim: 10 3 ## metadata(0): ## assays(1): counts ## rownames(10): gene_1 gene_2 ... gene_9 gene_10 ## rowData names(0): ## colnames(3): cell_1 cell_2 cell_3 ## colData names(0): ## reducedDimNames(0): ## spikeNames(0): To access the count data we just supplied, we can do any one of the following: assay(sce, &quot;counts&quot;) - this is the most general method, where we can supply the name of the assay as the second argument. counts(sce) - this is the same as the above, but only works for assays with the special name &quot;counts&quot;. counts(sce) ## cell_1 cell_2 cell_3 ## gene_1 7 13 24 ## gene_2 14 11 29 ## gene_3 13 10 25 ## gene_4 7 9 24 ## gene_5 13 8 35 ## gene_6 8 8 26 ## gene_7 12 14 28 ## gene_8 7 12 32 ## gene_9 6 9 33 ## gene_10 14 14 22 ## assay(sce, &quot;counts&quot;) ## same as above in this special case 4.2.2 Extending the assays Slot What makes the assay slot especially powerful is that it can hold multiple representations of the primary data. This is especially useful for storing a normalized version of the data. We can do just that as shown below, using the scran and scater packages to compute a log-count normalized representation of the initial primary data. Note that here, we overwrite our previous sce upon reassigning the results to sce - this is because these functions return a SingleCellExperiment object. Some functions - especially those outside of single-cell oriented Bioconductor packages - do not, in which case you will need to append your results to the sce object (see below). sce &lt;- scran::computeSumFactors(sce) sce &lt;- scater::normalize(sce) Viewing the object again, we see that these functions added some new entries: sce ## class: SingleCellExperiment ## dim: 10 3 ## metadata(1): log.exprs.offset ## assays(2): counts logcounts ## rownames(10): gene_1 gene_2 ... gene_9 gene_10 ## rowData names(0): ## colnames(3): cell_1 cell_2 cell_3 ## colData names(0): ## reducedDimNames(0): ## spikeNames(0): Specifically, we see that the assays slot has grown to be comprised of two entries: counts (our initial data) and logcounts (the normalized data). Similar to counts, the logcounts name is a special name which lets us access it simply by typing logcounts(sce), although the longhand version works just as well. logcounts(sce) ## cell_1 cell_2 cell_3 ## gene_1 3.61 4.36 3.91 ## gene_2 4.55 4.13 4.16 ## gene_3 4.45 4.00 3.96 ## gene_4 3.61 3.86 3.91 ## gene_5 4.45 3.70 4.42 ## gene_6 3.79 3.70 4.02 ## gene_7 4.34 4.46 4.12 ## gene_8 3.61 4.25 4.30 ## gene_9 3.41 3.86 4.34 ## gene_10 4.55 4.46 3.79 ## assay(sce, &quot;logcounts&quot;) ## same as above Notice that the data before had a severe discrepancy in counts between cells 1/2 versus 3, and that normalization has ameliorated this difference. To look at all the available assays within sce, we can type: assays(sce) ## List of length 2 ## names(2): counts logcounts While the functions above demonstrate automatic addition of assays to our sce object, there may be cases where we want to perform our own calculations and save the result into the assays slot. In particular, this is important for using functions that do not return your SingleCellExperiment object. Let’s append a new version of the data that has been offset by +100. counts_100 &lt;- assay(sce, &quot;counts&quot;) + 100 assay(sce, &quot;counts_100&quot;) &lt;- counts_100 # assign a new entry to assays slot Then we can use the accessor assays() (notice this is plural!) to see all our entries into the assay slot that we have made so far. Note that to see the names of all the assays, we use the plural assays() accessor, and to retrieve a single assay entry (as a matrix) we use the singular assay() accessor, providing the name of the assay we wish to retrieve as above. assays(sce) ## List of length 3 ## names(3): counts logcounts counts_100 These entries are also seen on the default view of sce: sce ## class: SingleCellExperiment ## dim: 10 3 ## metadata(1): log.exprs.offset ## assays(3): counts logcounts counts_100 ## rownames(10): gene_1 gene_2 ... gene_9 gene_10 ## rowData names(0): ## colnames(3): cell_1 cell_2 cell_3 ## colData names(0): ## reducedDimNames(0): ## spikeNames(0): This sort of extension of the assays slot is represented graphically in Figure 1B (dark blue box), showing the addition of the logcounts matrix into the assays slot. In a similar manner, many of the slots of sce are extendable through assignment as shown above, thus allowing for myriad custom functionality as needed for interoperability with functions outside of single-cell oriented Bioconductor packages. 4.2.3 Column (Meta)Data: colData Slot To further annotate our sce object, one of the first and most useful pieces of information is adding on metadata that describes the columns of our primary data, e.g. describing the samples or cells of our experiment. This data is entered into the colData slot: colData slot: metadata that describes that samples (cells) provided as a data.frame or (DataFrame if appending), where rows correspond to cells, and columns correspond to the sample (cells) metadata features (e.g. id, batch, author, etc.) (Figure 1A, orange box). So, let’s come up with some metadata for the cells, starting with a batch variable, where cells 1 and 2 are in batch 1, and cell 3 is from batch 2. cell_metadata &lt;- data.frame(batch = c(1, 1, 2)) rownames(cell_metadata) &lt;- paste0(&quot;cell_&quot;, 1:3) Now, we can take two approaches - either append the cell_metadata to our existing sce, or start from scratch via the SingleCellExperiment() constructor and provide it from the get go. We’ll start from scratch for now, but will also show how to append the data as well: ## From scratch: sce &lt;- SingleCellExperiment(assays = list(counts = counts_matrix), colData = cell_metadata) ## Appending to existing object (requires DataFrame() coercion) ## colData(sce) &lt;- DataFrame(cell_metadata) Similar to assays, we can see our colData is now populated from the default view of sce: sce ## class: SingleCellExperiment ## dim: 10 3 ## metadata(0): ## assays(1): counts ## rownames(10): gene_1 gene_2 ... gene_9 gene_10 ## rowData names(0): ## colnames(3): cell_1 cell_2 cell_3 ## colData names(1): batch ## reducedDimNames(0): ## spikeNames(0): And furthermore access our column (meta)data with the accessor, colData(): colData(sce) ## DataFrame with 3 rows and 1 column ## batch ## &lt;numeric&gt; ## cell_1 1 ## cell_2 1 ## cell_3 2 Finally, some packages automatically add to the colData slot, for example, the scater package features a function, calculateQCMetrics(), which appends a lot of quality control data. Here we show the first five columns of colData(sce) with the quality control metrics appended to it. sce &lt;- scater::calculateQCMetrics(sce) colData(sce)[, 1:5] ## DataFrame with 3 rows and 5 columns ## batch is_cell_control total_features_by_counts ## &lt;numeric&gt; &lt;logical&gt; &lt;integer&gt; ## cell_1 1 FALSE 10 ## cell_2 1 FALSE 10 ## cell_3 2 FALSE 10 ## log10_total_features_by_counts total_counts ## &lt;numeric&gt; &lt;integer&gt; ## cell_1 1.04139268515823 101 ## cell_2 1.04139268515823 108 ## cell_3 1.04139268515823 278 4.2.3.1 Using colData for Subsetting A common operation with colData is its use in subsetting. One simple way to access colData is through the use of the $ operator, which is a shortcut for accessing a variable within the colData slot: sce$batch ## [1] 1 1 2 ## colData(sce)$batch # same as above If we only wanted cells within batch 1, we could subset our sce object as follows (remember, we subset on the columns in this case because we are filtering by cells/samples here). sce[, sce$batch == 1] ## class: SingleCellExperiment ## dim: 10 2 ## metadata(0): ## assays(1): counts ## rownames(10): gene_1 gene_2 ... gene_9 gene_10 ## rowData names(7): is_feature_control mean_counts ... total_counts ## log10_total_counts ## colnames(2): cell_1 cell_2 ## colData names(10): batch is_cell_control ... ## pct_counts_in_top_200_features pct_counts_in_top_500_features ## reducedDimNames(0): ## spikeNames(0): 4.2.4 Feature Metadata: rowData/rowRanges Lastly, the rows also have their own metadata slot to store information that pertains to the features of the sce object: rowData slot: contains data in a data.frame (DataFrame) format that describes aspects of the data corresponding to the rows of the primary data (Figure 1A, green box). Furthermore, there is a special slot which pertains to features with genomic coordinates: rowRanges slot: contains data in a GRangesList (where each entry is a GenomicRanges format) that describes the chromosome, start, and end coordinates of the features (genes, genomic regions). Both of these can be accessed via their respective accessors, rowRanges() and rowData(). In our case, rowRanges(sce) produces an empty list: rowRanges(sce) # empty ## GRangesList object of length 10: ## $gene_1 ## GRanges object with 0 ranges and 0 metadata columns: ## seqnames ranges strand ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; ## ## $gene_2 ## GRanges object with 0 ranges and 0 metadata columns: ## seqnames ranges strand ## ## $gene_3 ## GRanges object with 0 ranges and 0 metadata columns: ## seqnames ranges strand ## ## ... ## &lt;7 more elements&gt; ## ------- ## seqinfo: no sequences However, our call to calculateQCMetrics(sce) in the prior section filled in the rowData slot of our sce object, as we can see below (only the first three columns are shown for brevity): rowData(sce)[, 1:3] ## DataFrame with 10 rows and 3 columns ## is_feature_control mean_counts log10_mean_counts ## &lt;logical&gt; &lt;numeric&gt; &lt;numeric&gt; ## gene_1 FALSE 14.6666666666667 1.19497660321605 ## gene_2 FALSE 18 1.27875360095283 ## gene_3 FALSE 16 1.23044892137827 ## gene_4 FALSE 13.3333333333333 1.15634720085992 ## gene_5 FALSE 18.6666666666667 1.29373075692248 ## gene_6 FALSE 14 1.17609125905568 ## gene_7 FALSE 18 1.27875360095283 ## gene_8 FALSE 17 1.25527250510331 ## gene_9 FALSE 16 1.23044892137827 ## gene_10 FALSE 16.6666666666667 1.24715461488113 In a similar fashion to the colData slot, such feature metadata could be provided at the onset when creating the SingleCellExperiment object, which we leave up to the reader as an exercise. 4.2.4.1 Subsetting with on Rows To subset an sce object down at the feature/gene level, we can do a row subsetting operation similar to other R objects, by supplying either numeric indices or a vector of names: sce[c(&quot;gene_1&quot;, &quot;gene_4&quot;), ] ## class: SingleCellExperiment ## dim: 2 3 ## metadata(0): ## assays(1): counts ## rownames(2): gene_1 gene_4 ## rowData names(7): is_feature_control mean_counts ... total_counts ## log10_total_counts ## colnames(3): cell_1 cell_2 cell_3 ## colData names(10): batch is_cell_control ... ## pct_counts_in_top_200_features pct_counts_in_top_500_features ## reducedDimNames(0): ## spikeNames(0): ## sce[c(1, 4), ] # same as above in this case 4.2.5 Size Factors Slot: sizeFactors Briefly, we already encountered this via the scran::computeSumFactors(sce) call, which adds a sizeFactors slot: sizeFactors slot: contains information in a numeric vector regarding the sample/cell normalization factors used to produce a normalize data representation (Figure 1B, brown box) sce &lt;- scran::computeSumFactors(sce) sce &lt;- scater::normalize(sce) sizeFactors(sce) ## [1] 0.622 0.665 1.713 4.3 A Brief Recap: From se to sce So far, we have covered the assays (primary data), colData (sample metadata), rowData/rowRanges (feature metadata), and sizeFactors slots of SingleCellExperiment. What is important to note is that the SingleCellExperiment class derives from the SummarizedExperiment (se) class, its predecessor, and in particular inherits the aforementioned slots. As such, much of the SummarizedExperiment functionality is retained in SingleCellExperiment. This allows existing methods that work with SummarizedExperiment to work similarly on SingleCellExperiment objects. So what’s new about the SingleCellExperiment class then? For our discussion, the most important change is the addition of a new slot called reducedDims. 4.4 The reducedDims Slot The reducedDims slot is a new addition which is specially designed to store the reduced dimensionality representations of primary data, such as PCA, tSNE, UMAP, and others. reducedDims slot: contains a list of numeric matrix entries which describe dimensionality reduced representations of the primary data, such that rows represent the columns of the primary data (aka the samples/cells), and columns represent the dimensions Most importantly, just like the assays slot, the reducedDims slot can hold a list of many entries. So, it can hold a PCA, TSNE, and UMAP representation of a given dataset all within the reducedDims slot. In our example, we can calculate a PCA representation of our data as follows using the scater package function runPCA(). We see that the sce now shows a new reducedDim and that the accessor reducedDim() produces the results of running PCA on the normalized data from logcounts(sce). sce &lt;- scater::runPCA(sce) reducedDim(sce, &quot;PCA&quot;) ## PC1 PC2 ## cell_1 -2.781 0.582 ## cell_2 0.744 -2.169 ## cell_3 2.037 1.587 ## attr(,&quot;percentVar&quot;) ## [1] 0.622 0.378 From this, we can also calculate a tSNE representation using the scater package function runTSNE(), and see that it can be seen both in the default view of sce and via accession: sce &lt;- scater::runTSNE(sce, perplexity = 0.1) ## Perplexity should be lower than K! reducedDim(sce, &quot;TSNE&quot;) ## [,1] [,2] ## cell_1 5674 -487 ## cell_2 -2415 5158 ## cell_3 -3260 -4670 We can view the names of all our entries in the reducedDims slot via the accessor, reducedDims() (notice that this is plural, and thus not the same as reducedDim(): reducedDims(sce) ## List of length 2 ## names(2): PCA TSNE Now, say we have a different dimensionality reduction approach which has not yet been implemented with SingleCellExperiment objects in mind. For example, let’s say we want to try the umap() function as implemented in the uwot package (which is a much faster version of the default umap implementation currently in scater). Similar to how we extended the assays slot with our own custom entry of counts_100, we can do similarly for the reducedDims slot: u &lt;- uwot::umap(t(logcounts(sce)), n_neighbors = 2) reducedDim(sce, &quot;UMAP_uwot&quot;) &lt;- u reducedDim(sce, &quot;UMAP_uwot&quot;) ## [,1] [,2] ## cell_1 0.483 -0.4687 ## cell_2 0.161 0.3888 ## cell_3 -0.644 0.0798 ## attr(,&quot;scaled:center&quot;) ## [1] -5.36 -12.15 And we can also see its entry when we look at the reducedDims() accessor output: reducedDims(sce) ## List of length 3 ## names(3): PCA TSNE UMAP_uwot 4.5 One More Thing: metadata Slot Some analyses produce results that do not fit into the aforementioned slots. Thankfully, there is a slot just for this type of messy data, and in fact, can accommodate any type of data, so long as it is in a named list: metadata slot: a named list of entries, where each entry in the list can be anything you want it to be For example, say we have some favorite genes, such as highly variable genes, we want to save inside of sce for use in our analysis at a later point. We can do this simply by appending to the metadata slot as follows: my_genes &lt;- c(&quot;gene_1&quot;, &quot;gene_5&quot;) metadata(sce) &lt;- list(favorite_genes = my_genes) metadata(sce) ## $favorite_genes ## [1] &quot;gene_1&quot; &quot;gene_5&quot; Similarly, we can append more information via the $ operator: your_genes &lt;- c(&quot;gene_4&quot;, &quot;gene_8&quot;) metadata(sce)$your_genes &lt;- your_genes metadata(sce) ## $favorite_genes ## [1] &quot;gene_1&quot; &quot;gene_5&quot; ## ## $your_genes ## [1] &quot;gene_4&quot; &quot;gene_8&quot; 4.6 About Spike-Ins You might have noticed that the sce default view produces an entry with spikeNames. The SingleCellExperiment object contains some special considerations for experiments with spike-in (ERCC) controls. We leave this to the interested reader to learn more about in the SingleCellExperiment introductory vignette. 4.7 Working with SingleCellExperiment Figure 1C shows an example workflow that uses the SingleCellExperiment object as its base, and similar to our walkthrough of the sce class above, continually appends new entries to save the results of the analysis. In the workflows that follow in this book, we will be similarly appending to an initial sce object many of our analytical results. 4.8 Multimodal Data: MultiAssayExperiment Recent advances in technology and protocols allow the simultaneous collection of DNA and RNA from the same cells, enabling single-cell multi-modal analysis. These data present new challenges in the complexity of statistical analyses, which are addressed in Bioconductor through the MultiAssayExperiment container. The MultiAssayExperiment class integrates all major Bioconductor experimental data containers, and any containers derived from those, including SingleCellExperiment. It provides harmonized data management for heterogeneous assays, including subsetting by genomic identifiers, genomic coordinates, or sample/cell attributes such as cell type. The user interface mimics that of SingleCellExperiment, with comparable actions working across all assays. Multi-modal profiling is an emergent area of single-cell biology with many exciting technologies coming online, such as gene expression profiling in tandem with protein via CITE-seq/REAP-seq and adaptive repertoire sequencing. While we won’t cover multimodal data analysis further in this online book as of this writing, we anticipate infrastructure and statistical methodology advances in this area in the near future. "],
["analytical-workflow-overview.html", "Chapter 5 Analytical Workflow Overview 5.1 Experimental Design 5.2 Preprocessing 5.3 Import to R 5.4 Data Processing 5.5 Downstream Statistical Analyses 5.6 Accessible &amp; Reproducible Analysis", " Chapter 5 Analytical Workflow Overview Figure 1. Overview of a typical scRNA-seq workflow. In this chapter, we will orient you to the framework of scRNA-seq, shown in the figure above. This chapter will primarily be textual to give a brief conceptual basis to each of the steps presented below. In the subsequent chapter we will walk through a minimal analysis of a simple scRNA-seq dataset to put code to the concepts presented here, before breaking down individual steps and highlighting alternate or more advanced ways of performing specific tasks. 5.1 Experimental Design Before getting started with actual code, a brief word on the importance of experimental design is warranted. This step is done prior to any sequencing, and involves the proper annotation of associated experimental metadata. This metadata is essential, if not for downstream analyses then at minimum for publication of the dataset and submission to public repositories such as NCBI GEO. Some key metadata to record includes, but is not limited to: Experimental aspects: batch, operator/author, date collected, date processed, date submitted for sequencing Biological traits: organism, sex, age, tissue, isolate, disease Perturbations: genotype, treatment Sequencing strategy: molecule, source, strategy, target, instrument model, read length, single vs paired-end sequencing, barcodes And finally, long form descriptions should be provided alongside informative sample names that encompass the key variables of the experiment. This metadata will often be read in and supplied as colData into the constructed SingleCellExperiment class object. 5.2 Preprocessing Once an experiment has been processed and sequenced, scRNA-seq experiments must be aligned to the transcriptome, and subsequently the reads must be quantified into a counts matrix of expression values consisting of cells versus the features of interest (genes or transcripts). While the specific bioinformatic choices defining this preprocessing pipeline are often technology- or platform-dependent, they are worth discussing briefly. Many popular preprocessing methods are available as command line software that is run outside of R: For the 10X Genomics platform, 10X provides Cell Ranger, which executes a custom pipeline compatible with gene expression as well as feature barcoding techniques such as CITE-seq. Under the hood, it uses the STAR aligner. For droplet-based scRNA-seq as well as 10X, Salmon - a tool originally designed for bulk RNA-seq processing - has a submodule specifically designed for scRNA-seq called Alevin. For other scRNA-seq platforms - most often plate-based methods that demultiplex into a directory per well - methods first created for bulk RNA-seq such as Salmon, Kallisto, and STAR will work. In addition to the above, there also exist Bioconductor packages tailored for scRNA-seq processing within R. The scPipe package uses the Rsubread package under the hood to process droplet and plate-based protocols. In addition, the scruff Bioconductor package is especially designed for the processing of CEL-Seq/CEL-Seq2. One important note: some software such as Cell Ranger automatically filters cells silently based on quality control metrics such as via the “knee plot” method (aka barcode ranks, log-counts by log-rank). Such algorithmic filtering methods may not always work. Thus, we recommend forcing an expected number of cells to be output. In Cell Ranger, this takes the form of supplying the additional argument --force-cells. That way, you can determine your own filtering criteria and assess its efficacy, as well as perform analyses on ambient RNA using empty droplets present in your raw data. 5.3 Import to R In all the cases of above, the end result is a counts matrix of expression values. For the command line preprocessing methods, this matrix will need to be imported into R. This can be done with the help of DropletUtils (which directly creates a SingleCellExperiment object from 10X data via the read10xCounts() function), or tximeta/tximport. In cases where the counts matrix is not directly instantiated into a SingleCellExperiment class object, a basic one can be created manually. 5.4 Data Processing Once the scRNA-seq data has been imported into R and a SingleCellExperiment class object constructed, the next step is to create a clean expression matrix and fundamental dimensionality reduced representations of the data that can together be used in subsequent downstream analyses. 5.4.1 Quality Control Metrics Quality control metrics are utilized to assess not only the overall success of the experiment, but also to determine individual cell level reaction successes and failures. This step ultimately leads to the calculation of various quality control metrics, which can be used either to completely exclude poor-quality droplets/cells or be accounted for in downstream analyses. Some example metrics include total UMI counts, doublet identification, number of mitochondrial reads as a surrogate for cell damage, and complexity (number of genes assigned at least one read). 5.4.2 Normalizing Data Transforming the counts data into a normalized representation allows for cell and gene-specific biases to be eliminated prior to downstream analyses that depend on explicit gene expression value comparisons. This transformation is important for visualizing gene expression data across clusters (where cells may be of different sizes, e.g. have different library sizes) and for tasks such as differential expression analyses. 5.4.3 Feature Selection In most experiments, only a subset of genes drive observed heterogeneity across the population of cells profiled in an scRNA-seq experiment. The aim of performing feature selection is to both reduce computational overhead and increase statistical power in downstream analyses. While it is possible to employ supervised learning approaches in experiments with labeled cells (via an input of sorted populations or other markers), most scRNA-seq experiments do not have a priori knowledge on the identity of the cells. Thus, this necessitates unsupervised learning approaches to identify informative features. Metrics such as variance, deviance, and dropout are often used and fitted against gene expression to select for highly informative genes relative to their expression level. 5.4.4 Imputation Imputation methods have been proposed to address the challenge of the large amount of zeros observed in data from scRNA-seq. In general, these methods rely on inherent structure in the dataset. However, extra care should be taken in applying these methods, as imputation has been shown to generative false signals and decrease the reproducibility of cell-type specific markers. 5.4.5 Dimensionality Reduction While feature selection can ameliorate the complexity of scRNA-seq to some extent, it is often insufficient to make many analyses tractable. Dimensionality reduction can thus be applied to create low-dimensional representations that nonetheless preserve meaningful structure. Principal components analysis (PCA) often serves as the first step, calculating a large number of components which are then trimmed down to those that explain a high amount of variance. Following this, the PCA results serve as the basis for other dimensionality reduction approaches that are often used to visualize the data in 2 or 3-dimensions, including t-SNE, UMAP, and diffusion maps. Alternative, more statistically oriented approaches such as the zero-inflated negative binomial (ZiNB) transform may also be used to produce dimensionality reduced representations that account for confounding factors. 5.4.6 Integrating Datasets In cases where its necessary to bring together multiple distinct scRNA-seq experiments, a new avenue of approaches specifically designed to solve this problem by leveraging the richness inherent to this high-dimensional data have emerged. These new approaches specifically bypass the assumption required by traditional statistical models that the composition of the populations is either known or identical across batches, thus improving the end result of integration. Integration approaches provide a new (potentially dimensionality reduced) representation of the data that allows for the identification of biologically similar cells between batches. This improves the performance of clustering, annotation, and the consequently the interpretability of 2- or 3-dimensional visualizations. 5.5 Downstream Statistical Analyses 5.5.1 Clustering 5.5.2 Differential Expression 5.5.3 Trajectory Analysis 5.5.4 Annotation 5.6 Accessible &amp; Reproducible Analysis 5.6.1 Interactive Data Visualization 5.6.2 Report Generation "],
["a-basic-analysis.html", "Chapter 6 A Basic Analysis 6.1 Preprocessing &amp; Import to R 6.2 Constructing the SingleCellExperiment 6.3 Data Processing 6.4 Downstream Statistical Analyses 6.5 Accessible &amp; Reproducible Analysis", " Chapter 6 A Basic Analysis In this chapter, we will walk through a minimal analysis of a simple scRNA-seq dataset in order to acquaint you with the overall framework of scRNA-seq in code terms. Where relevant within each part of this basic workflow, we will refer the interested reader to the associated chapter to learn advanced or alternate ways of performing a given task. Put another way, the workflow demonstrated in this chapter is written with the aim of simplicity, and thus will likely require nontrivial tweaking of parameters or alternate methods in real-world analyses. One note: in this workflow, we will be loading libraries as they become necessary to clearly link libraries to their respective functions, which usually runs counter to the norm of loading libraries first, at the top of the analysis script. 6.1 Preprocessing &amp; Import to R We will assume here that sequencing alignment and quantification of the data into a counts matrix, as well as the subsequent import to R has already been performed since this is highly platform- or technology-dependent. Note that for 10X Genomics data (which is used in this example workflow), the counts matrix and associated metadata (cell barcodes, data path, etc.) can be imported via the DropletUtils package’s read10xCounts() function. For data processed through Salmon/Alevin/Kallisto, we recommend checking out the tximport/tximeta Bioconductor packages. These are either imported as SingleCellExperiment or as a counts matrix which can be then coerced into a SingleCellExperiment object as demonstrated below. 6.2 Constructing the SingleCellExperiment 6.2.1 From Scratch Below we show an example of creating a SingleCellExperiment class object from a counts matrix and associated experimental metadata. library(SingleCellExperiment) ## More realistic: read in your experimental design metadata ## If its per cell metadata, make sure it lines up with your ## counts matrix row IDs correctly ## my_metadata &lt;- read.csv(&quot;my_metadata.csv&quot;) ## Example data ncells &lt;- 100 my_counts_matrix &lt;- matrix(rpois(20000, 5), ncol = ncells) my_metadata &lt;- data.frame(genotype = rep(c(&#39;A&#39;, &#39;B&#39;), each = 50), experiment_id = &#39;Experiment1&#39;) ## Construct the sce object manually sce &lt;- SingleCellExperiment(assays = list(counts = my_counts_matrix), colData = my_metadata) ## Manually adding a variable that is the same across all cells colData(sce) &lt;- cbind(colData(sce), date = &#39;2020-01-01&#39;) sce ## class: SingleCellExperiment ## dim: 200 100 ## metadata(0): ## assays(1): counts ## rownames: NULL ## rowData names(0): ## colnames: NULL ## colData names(3): genotype experiment_id date ## reducedDimNames(0): ## spikeNames(0): 6.2.2 From Publicly Available Data From here on out, we will be working with a small example dataset from the TENxPBMCData Bioconductor package which has already been packaged into a SingleCellExperiment class object: library(TENxPBMCData) sce &lt;- TENxPBMCData(&#39;pbmc3k&#39;) sce ## class: SingleCellExperiment ## dim: 32738 2700 ## metadata(0): ## assays(1): counts ## rownames(32738): ENSG00000243485 ENSG00000237613 ... ## ENSG00000215616 ENSG00000215611 ## rowData names(3): ENSEMBL_ID Symbol_TENx Symbol ## colnames: NULL ## colData names(11): Sample Barcode ... Individual Date_published ## reducedDimNames(0): ## spikeNames(0): One decision that should be made early on in the analysis is what row identifier to identify genes. Depending on how the data is imported, the rowData component may already have additional annotation information, such as multiple row mappings. For our new sce object from the pbmc3k dataset, we can take a look at rowData to see our options: rowData(sce) ## DataFrame with 32738 rows and 3 columns ## ENSEMBL_ID Symbol_TENx Symbol ## &lt;character&gt; &lt;character&gt; &lt;character&gt; ## ENSG00000243485 ENSG00000243485 MIR1302-10 NA ## ENSG00000237613 ENSG00000237613 FAM138A FAM138A ## ENSG00000186092 ENSG00000186092 OR4F5 OR4F5 ## ENSG00000238009 ENSG00000238009 RP11-34P13.7 LOC100996442 ## ENSG00000239945 ENSG00000239945 RP11-34P13.8 NA ## ... ... ... ... ## ENSG00000215635 ENSG00000215635 AC145205.1 NA ## ENSG00000268590 ENSG00000268590 BAGE5 NA ## ENSG00000251180 ENSG00000251180 CU459201.1 NA ## ENSG00000215616 ENSG00000215616 AC002321.2 NA ## ENSG00000215611 ENSG00000215611 AC002321.1 NA We see that we could choose between ENSEMBL_ID (the default), Symbol_TENx, and Symbol. For ease of readability and subsetting, we will utilize the Symbol_TENx identifier as our object’s rownames, making it possible to subset the sce with gene symbols as in sce[&quot;CD8A&quot;, ]. ## reassign rownames rownames(sce) &lt;- rowData(sce)[, &quot;Symbol_TENx&quot;] Now, while this seems to work just fine, eventually we may run into an issue because we actually have duplicated row names here. Depending on how a downstream function is coded, this may cause an esoteric error to pop-up. In fact, here we have about 100 duplicates. We can avoid future errors (and many headaches) by removing duplicates before any analysis: ## counts dupes from top to bottom to make a logical vector dupes &lt;- duplicated(rownames(sce)) sce &lt;- sce[!dupes, ] Keep in mind, the above is likely the most inelegant solution to the problem. Other methods could include, from the duplicated set of genes, choosing the one with the highest expression, aggregating the counts per cell, or keeping them all by adding an additional suffix to make the row names unique. Each has its own tradeoffs, so we leave this choice up to the diligent reader. And one more bit of preprocessing to prevent a potential downstream error is to assign our columns proper names. We can grab the barcodes of each cell from colData and assign them as column names as follows: colnames(sce) &lt;- sce$Barcode 6.3 Data Processing The aim of this section is to form the basis for more interesting downstream analyses. Thus, the objective here is to transform the data into a “clean” expression matrix that has been normalized and freed of technical artifacts, as well as a dimensionality reduction representation that can be used in subsequent analyses and visualization. 6.3.1 Quality Control Metrics The first step is to ensure that our dataset only contains viable cells, e.g. droplets that contain proper mRNA libraries. One way to do that is to use the popular “knee plot”, which shows the relationship between the log rank vs the log total counts, and then calculate where the “knee” of the plot is. We use the DropletUtils package to demonstrate this in our example PBMC dataset. library(DropletUtils) ## Calculate the rank vs total counts per cell br &lt;- barcodeRanks(counts(sce)) ## Create the knee plot plot(log10(br$rank), log10(br$total)) abline(h = log10(metadata(br)$knee)) Figure 6.1: Barcode rank (aka knee) plot showing log10-rank by log10-total counts relationship and calculated knee (horizontal line). ## Save the calculated knee from `barcodeRanks()` knee &lt;- log10(metadata(br)$knee) We see that the knee calculated via this method (horizontal line) is at 1740, or on the log scale, 3.2405. This can be used as a filter to remove cells that are likely to be empty droplets. Before we do that, we will finish calculating other quality control (QC) metrics via the scater package and show the results from the first three cells. library(scater) sce &lt;- calculateQCMetrics(sce) We can display some of the calculated QC metrics appended to the colData component - there are a number of other columns present, but for brevity will only show two pertinent ones. colData(sce)[1:3, c(&quot;log10_total_features_by_counts&quot;, &quot;log10_total_counts&quot;)] ## DataFrame with 3 rows and 2 columns ## log10_total_features_by_counts log10_total_counts ## &lt;numeric&gt; &lt;numeric&gt; ## AAACATACAACCAC-1 2.89 3.38 ## AAACATTGAGCTAC-1 3.13 3.69 ## AAACATTGATCAGC-1 3.05 3.5 We can further inspect these cells based on their total counts as well as vs the total features detected by counts (e.g. the number of genes that have nonzero counts). hist(sce$log10_total_counts, breaks = 100) abline(v = knee) Figure 6.2: Histogram of the log10 total counts with the calculated knee from above (vertical line). smoothScatter(sce$log10_total_counts, sce$log10_total_features_by_counts, nbin = 250) abline(v = knee) Figure 6.3: Smoothed scatter plot of the log10-total counts vs the log10-total features detected by counts with the calculated knee from above (vertical line). While there are various ways to filter cells, here we actually will not need to perform any filtering, as the data has already undergone a stringent quality control, and thus all the cells can be considered high quality. For the sake of completeness, we will demonstrate here - without evaluating - how to subset based on the previously calculated barcode ranks knee: ## not run sce &lt;- sce[, sce$log10_total_counts &gt; knee] 6.3.2 Normalizing Data Next up we will transform the primary data, the counts, into a (log) normalized version. In this section, we will use the scran package throughout. First however, we will need to calculate scaling factors per cell. This function relies on an initial “quick and dirty” clustering to get roughly similar pools of cells. These are used to generate pool-based estimates, from which the subsequent cell-based size factors are generated. To learn more about the method, see the ?computeSumFactors documentation. For now, we will perform a simpler normalization, using the library sizes per cell to create a log-normalized expression matrix: sce &lt;- scater::normalize(sce) We can see below that we now have two assays, counts and logcounts. assays(sce) ## List of length 2 ## names(2): counts logcounts 6.3.3 Feature Selection This section will use the scran package, as we select for informative genes by selecting for those with high coefficients of biological variation. Since this experiment does not have spike-ins, we will fit the mean-variance trend across the endogenous genes. library(scran) fit &lt;- trendVar(sce, use.spikes = FALSE) plot(fit$mean, fit$var) curve(fit$trend(x), col = &#39;red&#39;, lwd = 2, add = TRUE) Figure 6.4: Mean-variance trend line fit by scran package trendVar() function. We can see that the trend line goes through the central mass of genes, and thus continue on with looking at the decomposed variance. In this method, it is assumed that the total variance is the sum of the technical and biological variance, where the technical variance can be determined by interpolating the fitted trend at the mean log-count for that gene. Thus the biological variance is the total variance minus this interpolated (technical) variance. We can then rank and choose genes which have a biological coefficient of variance greater than zero. dec &lt;- decomposeVar(sce, fit) dec &lt;- dec[order(dec$bio, decreasing = TRUE), ] # order by bio var dec[1:5, ] ## mean total bio tech p.value FDR ## LYZ 1.629 3.925 3.116 0.809 0 0 ## S100A9 1.068 3.352 2.628 0.725 0 0 ## HLA-DRA 1.543 3.071 2.278 0.793 0 0 ## FTL 3.637 2.941 2.203 0.738 0 0 ## CD74 2.209 2.696 1.900 0.796 0 0 The total number of genes with biological variance greater than zero as 3770. Alternatively, we could use the p-value/FDR as a way to rank our genes, but do note the following (from the simpleSingleCell vignette: “Ranking based on p-value tends to prioritize HVGs that are more likely to be true positives but, at the same time, less likely to be interesting. This is because the ratio can be very large for HVGs that have very low total variance and do not contribute much to the cell-cell heterogeneity.” However we choose, we can save these highly variable genes and use them for subsequent analyses: hvg_genes &lt;- rownames(dec)[dec$bio &gt; 0] For the purpose of sharing and saving this list of genes, we can stash the result into the metadata component of our sce object as follows: metadata(sce)$hvg_genes &lt;- hvg_genes metadata(sce)$hvg_genes[1:10] ## [1] &quot;LYZ&quot; &quot;S100A9&quot; &quot;HLA-DRA&quot; &quot;FTL&quot; &quot;CD74&quot; &quot;CST3&quot; &quot;S100A8&quot; ## [8] &quot;TYROBP&quot; &quot;NKG7&quot; &quot;FTH1&quot; The metadata component can hold any object, as it is a list container. Any results that you’d like to keep are safe to store here, and a great way to save or share intermediate results that would otherwise be kept in separate objects. 6.3.4 Dimensionality Reduction We now can perform dimensionality reduction using our highly variable genes (hvg_genes) subset. To do this, we will first calculate the PCA representation via the runPCA() function from the scater package. We will calculate 50 components on our highly variable genes: sce &lt;- runPCA(sce, ncomponents = 50, feature_set = hvg_genes) The results of these calculations will be stored in the reducedDims component. This method saves the percent variance explained per component as an attribute, which can be accessed as follows, and subsequently plot the “elbow plot”: ## access the attribute where percentVar is saved in reducedDim pct_var_explained &lt;- attr(reducedDim(sce, &#39;PCA&#39;), &#39;percentVar&#39;) plot(pct_var_explained) # elbow plot To calculate a 2-dimensional representation of the data, we will use the top 20 components of our PCA result to compute the UMAP representation. sce &lt;- runUMAP(sce, use_dimred = &#39;PCA&#39;, n_dimred = 20) plotUMAP(sce) Figure 6.5: UMAP plot. With that, we have a canvas on which to paint our downstream analyses. 6.4 Downstream Statistical Analyses There are a plethora of potential downstream analyses to run, the choice of which is highly dependent on the biological objective. For this example dataset, our aim will be to identify the key cell types via a combination of clustering and differential expression. 6.4.1 Clustering Based on our earlier UMAP plot, it appears that we have a few distinct clusters. To do this computationally, we can utilize the scran package to: build a shared nearest neighbor (SNN) graph calculate based on the SNN graph the most representative clusters In this first step, we will specify that we will consider k nearest neighbors, and d dimensions from the PCA calculation as follows: set.seed(1234) # to make results reproducible snng &lt;- buildSNNGraph(sce, k = 50, d = 20) Following the graph construction, we can calculate the clusters using a variety of different graph-based methods from the igraph package. Here, we use the louvain method to determine our cell’s cluster memberships. snng_clusters &lt;- igraph::cluster_louvain(snng) We see that we have the following numbers of cells per cluster: table(snng_clusters$membership) ## ## 1 2 3 4 5 ## 687 350 556 528 579 To view this result graphically on the UMAP plot, we first assign the result to the colData component as a new column, and specify this as our color variable in the plotUMAP() function: colData(sce)$clusters &lt;- as.factor(snng_clusters$membership) plotUMAP(sce, colour_by = &#39;clusters&#39;) Figure 6.6: UMAP plot showing calculated clusters. Naturally, this result will change as we tweak the number of k neighbors to consider and with the specific clustering algorithm, but for now we will go onwards to find markers of each of our clusters. 6.4.2 Differential Expression In this section, we will look to identify genes that are unique to each of our clusters. To accomplish this, we will lean on the scran package to perform the analysis, and then the scater package to visualize the results. For this analysis, we will limit ourselves to a top subset of highly variable genes in our hvg_genes set, purely for the sake of computation time. Furthermore, we will limit our consideration to genes with an increased log fold-change of at least 1.5 versus other clusters. We will also use the BiocParallel package to parallelize the computation and speed up our processing via the BPPARAM argument. markers &lt;- findMarkers(sce, clusters = colData(sce)$clusters, subset.row = hvg_genes[1:250], lfc = 1.5, direction = &#39;up&#39;, log.p = TRUE, BPPARAM = BiocParallel::MulticoreParam()) We can view the top 5 markers that are differentially expressed (by our specified metrics): markers[[1]][1:5, ] ## Top log.p.value log.FDR logFC.2 logFC.3 logFC.4 logFC.5 ## CST3 1 -831.4 -825.9 3.446 3.470 3.413 3.480 ## TYROBP 1 -778.7 -773.8 3.325 3.324 2.691 3.301 ## LYZ 2 -526.9 -522.8 4.028 4.011 4.072 4.000 ## FTL 3 -667.3 -662.9 3.110 3.634 3.463 3.534 ## FTH1 4 -474.3 -470.6 2.771 3.118 3.100 2.755 We can see that CD3D, a marker of T cells, is one of our top differentially expressed genes in cluster 1. We can plot the expression of this gene across all our clusters as follows: plotExpression(sce, &#39;CD3D&#39;, x = &#39;clusters&#39;) Figure 6.7: Violin plots of CD3D expression across clusters. This plot highlights that CD3D is more highly expressed in cluster 1 relative to some of the other clusters, but not all. This can also be seen from our raw output above, where the log fold-change is calculated with respect to each cluster. There, we see that the log fold-change for CD3D is very high only relative to clusters 2 and 3 (meeting our cutoff of 1.5). 6.4.3 Annotation 6.4.3.1 A Manual Approach To finish off our the downstream analysis section here, we will look to annotate our clusters with a cell type designation, based on publicly available knowledge. Before we do that, let’s get a broader view of our top differentially expressed genes. To do this, we can iterate over the list-object returned by findMarkers to get the top 10 genes per cluster, and then plot these genes in a heatmap. ## grab the top 10 genes per cluster (e.g. within each list component) genes &lt;- lapply(markers, function(x) { rownames(x)[x$Top &lt;= 10] }) ## uniqify the set of genes returned, after coercing to a vector genes &lt;- unique(unlist(genes)) plotHeatmap(sce, genes, colour_columns_by = &quot;clusters&quot;, show_colnames = FALSE, clustering_method = &#39;ward.D2&#39;, fontsize_row = 6) Figure 6.8: Heatmap showing top differentially expressed genes across the clusters. Based on the heatmap output (and a priori knowledge), we can make some observations: CD79A/CD79B, markers of B cells, are uniquely and highly expressed in cluster 2 HLA genes, present on antigen presenting cells (APCs), are highly expressed across clusters 2 and 3 LYZ, a marker of dendritic cells (an APC), is highly expressed in cluster 3 Granzymes A and B (GZMA/GZMB), and NKG7, markers of cytotoxic cells such as CD8s and NK cells, are highly expressed within (a subset of cluster 4) CD3D/CD3E, markers of T cells, are expressed across clusters 5, 1, and 4 Finally, we can view a selection of the genes mentioned above on our previous UMAP plot: plotUMAP(sce, colour_by = &quot;CD79A&quot;) Figure 6.9: Various UMAP plots showing the expression of select cell-type specific genes. plotUMAP(sce, colour_by = &quot;LYZ&quot;) Figure 6.10: Various UMAP plots showing the expression of select cell-type specific genes. plotUMAP(sce, colour_by = &quot;NKG7&quot;) Figure 6.11: Various UMAP plots showing the expression of select cell-type specific genes. plotUMAP(sce, colour_by = &quot;CD3D&quot;) Figure 6.12: Various UMAP plots showing the expression of select cell-type specific genes. Combining the information derived from our heatmap and viewing these genes on our UMAP, we can come to the following conclusion: Cluster 2 is likely to be B cells Cluster 3 is likely to be dendritic cells Clusters 1, 5, 4 appear to represent a spectrum of cells with cytotoxic capabilities, likely composed of a combination of T cells and NK cells, Cluster 4 exhibits an strong NK cell signature on the basis of NKG7 Now that we’ve manually sorted our dataset on the basis of prior knowledge, let’s try a more automated approach using publicly available markers. 6.4.3.2 An Automated Approach Manually classifying cell types present in an scRNA-seq experiment can be prone to bias in terms of how a label is selected. Thus have emerged automated classification approaches which take a measured approach to the labeling of cell types. One such approach - cellassign - applies labels in a single-cell manner based on a gene by cell type “marker matrix”. Here, we utilize an existing gene by cell type annotation from a publication by Becht et al. (2016) which categorizes genes into cell types based on the specificity of their expression. Let’s first construct a marker matrix loosely inspired by the Seurat PBMC 3k tutorial: anno &lt;- data.frame( SYMBOL = c( &#39;IL7R&#39;, &#39;CCR7&#39;, &#39;CD4&#39;, &#39;CD3D&#39;, &#39;CD3E&#39;, &#39;CD14&#39;, &#39;LYZ&#39;, &#39;MS4A1&#39;, &#39;CD79A&#39;, &#39;CD79B&#39;, &#39;CD8A&#39;, &#39;CD8B&#39;, &#39;CD3D&#39;, &#39;CD3E&#39;, &#39;GNLY&#39;, &#39;NKG7&#39;, &#39;FCER1A&#39;, &#39;CST3&#39;, &#39;ITGAX&#39; ), cell_type = c( rep(&#39;CD4 T cell&#39;, 5), rep(&#39;Monocyte&#39;, 2), rep(&#39;B cell&#39;, 3), rep(&#39;CD8 T cell&#39;, 4), rep(&#39;NK cell&#39;, 2), rep(&#39;Dendritic cell&#39;, 3) ) ) Lastly, we’ll need to reformat this matrix to fit the expectations of cellassign, chiefly to convert the annotation into a binary matrix of genes (rows) by cell types (columns): ## construct rho (binary marker matrix) tmp &lt;- tidyr::spread(anno, cell_type, cell_type) rho &lt;- ifelse(is.na(tmp[, -1]), 0, 1) rownames(rho) &lt;- tmp$SYMBOL ## remove entries that are not present in our dataset rho &lt;- rho[rownames(rho) %in% rownames(sce), ] rho[1:3, ] ## B cell CD4 T cell CD8 T cell Dendritic cell Monocyte NK cell ## CCR7 0 1 0 0 0 0 ## CD14 0 0 0 0 1 0 ## CD3D 0 1 1 0 0 0 We can then run the cellassign method to produce cell type labels on a per cell basis: ## not run - results pulled from a prior run ## devtools::install_github(&#39;Irrationone/cellassign&#39;) library(cellassign) library(tensorflow) set.seed(1234) reticulate::py_set_seed(1234) fit &lt;- cellassign(sce[rownames(rho), ], marker_gene_info = rho, s = sizeFactors(sce)) ## add cell type info into colData colData(sce)$cellassign_type &lt;- fit$cell_type ## plot the cellassign results on UMAP plotUMAP(sce, colour_by = &#39;cellassign_type&#39;) Figure 6.13: UMAP showing the results of automated label assignment as performed by cellassign. In practice, some combination of the above manual and automated classification schema will likely be necessary to properly annotate an scRNA-seq dataset. 6.5 Accessible &amp; Reproducible Analysis In collaborative settings, it is essential to share data and analyses. Thanks to the SingleCellExperiment class, most of if not all analysis steps performed can be recorded. These outputs are accessible through not only R, but also via graphical user interfaces as well that broaden the potential viewing audience. 6.5.1 Interactive Data Visualization Interactive exploration and visualization is a great way for collaborators to learn more about scRNA-seq data and analyses. In particular the iSEE package has been especially designed for viewing and sharing scRNA-seq. ## not run library(iSEE) iSEE(sce) Based on the example analyses, we task the interested reader to assess the previous section’s automatic annotation relative to the clustering results using iSEE. "],
["adaptations-for-large-scale-data.html", "Chapter 7 Adaptations for Large-scale Data 7.1 Approximate Methods 7.2 Parallelization 7.3 On-Disk Data", " Chapter 7 Adaptations for Large-scale Data Large datasets such as the Human Cell Atlas (with over 1.3 million cells) have the potential to benefit from special adaptations that enable analysis in compute-constrained environments (such as personal laptops). Here, we briefly cover topics that aim to ease working with scRNA-seq data to make it faster and more tractable. Do note however that these adaptations do not universally result in improved computational efficiency. For example, parallelization does incur costs, and disk-backed data representation will generally be slower than purely in-memory representations. In both cases, hardware specifics may dictate any potential gains in efficiency, as can be imagined in the case of a solid state drive (SSD) being faster for disk-backed data representations relative to a hard disk drive (HDD). Thus, with the right compute environment, data, and task, these adaptations can yield significant computational improvements. 7.1 Approximate Methods A general approach that works across all scales of data involves changing the task itself. Some methods - such as PCA or nearest neighbor searches - have been extended to include versions that provide approximate results. Generally, these adaptations result in acceptable losses in accuracy for significant computational gains. In some cases, approximate methods may even be desirable for the results themselves, as has been shown by the FIt-SNE approach. Some example packages that provide approximate versions of popular methods include: BiocSingular via the IrlbaParam() and RandomParam() for approximate singular value decomposition (SVD) BiocNeighbors via the AnnoyParam() for approximate nearest neighbor searches These packages provide users (and developers) a common interface, enabling modular swapping of key algorithms within functions. For example, we can see the immediate benefit of using an approximate method for PCA as provided by the BSPARAM argument in the following code, which utilizes the bench package for profiling: library(SingleCellExperiment) library(scater) ## Simulate a dataset with 1k genes and 1k cells mat &lt;- matrix(rpois(1e6, 100), nrow = 1000) tiny_sce &lt;- SingleCellExperiment(assays = list(counts = mat)) tiny_sce &lt;- normalize(tiny_sce) library(bench) library(BiocSingular) ## simple function to show only cols of interest .show_bench &lt;- function(b) { b[, c(&#39;expression&#39;, &#39;min&#39;, &#39;median&#39;, &#39;mem_alloc&#39;)] } bm &lt;- bench::mark( runPCA(tiny_sce, BSPARAM = IrlbaParam()), runPCA(tiny_sce, BSPARAM = ExactParam()), check = FALSE) .show_bench(bm) ## # A tibble: 2 x 4 ## expression min median mem_alloc ## &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt; &lt;bch:byt&gt; ## 1 runPCA(tiny_sce, BSPARAM = IrlbaParam()) 49.4ms 52.6ms 0B ## 2 runPCA(tiny_sce, BSPARAM = ExactParam()) 718ms 718ms 0B We can see that the approximate, irlba based implementation is about 6 times faster than the exact version in this case. We can also try this with nearest neighbor searches. Here we provide the BNPARAM argument to build a shared nearest neighbors graph using different algorithms under the hood: library(scran) library(BiocNeighbors) ## Calculate PCA before testing NN back-ends tiny_sce_pca &lt;- runPCA(tiny_sce, BSPARAM = IrlbaParam()) bm &lt;- bench::mark( buildSNNGraph(tiny_sce_pca, BNPARAM = AnnoyParam()), buildSNNGraph(tiny_sce_pca, BNPARAM = KmknnParam()), check = FALSE) .show_bench(bm) ## # A tibble: 2 x 4 ## expression min median ## &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt; ## 1 buildSNNGraph(tiny_sce_pca, BNPARAM = AnnoyParam()) 1.31s 1.31s ## 2 buildSNNGraph(tiny_sce_pca, BNPARAM = KmknnParam()) 769.95ms 769.95ms ## # … with 1 more variable: mem_alloc &lt;bch:byt&gt; We can see from the above benchmark that in our tiny dataset, we don’t see much, if any, benefit of using an approximate method (via the AnnoyParam(), which uses the Annoy library). However, if we increase our dataset to something larger.. ## Simulate a dataset with 1k genes and 10k cells mat &lt;- matrix(rpois(10e6, 100), nrow = 1000) big_sce &lt;- SingleCellExperiment(assays = list(counts = mat)) big_sce &lt;- normalize(big_sce) ## Calculate PCA before testing NN back-ends big_sce &lt;- runPCA(big_sce, BSPARAM = IrlbaParam()) ## NN search bm &lt;- bench::mark( buildSNNGraph(big_sce, BNPARAM = AnnoyParam()), buildSNNGraph(big_sce, BNPARAM = KmknnParam()), check = FALSE) .show_bench(bm) ## # A tibble: 2 x 4 ## expression min median mem_alloc ## &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:t&gt; &lt;bch:byt&gt; ## 1 buildSNNGraph(big_sce, BNPARAM = AnnoyParam()) 8.2s 8.2s 0B ## 2 buildSNNGraph(big_sce, BNPARAM = KmknnParam()) 14.4s 14.4s 0B We see (more consistently) that we have realized some gains from using the approximate-based nearest neighbors search. 7.2 Parallelization Many tasks that arise in the analysis of scRNA-seq data are able to be parallelized. In other words, the problem can be broken down into smaller pieces that can be solved independently. Parallel computing can be performed in computing environments with access to multiple cores. Bioconductor has reduced the complexity of implementing parallelized software via the BiocParallel package. This enables a common interface across Bioconductor software packages for parallel computing. Across many Bioconductor packages, an argument that will often be present in functions that are parallelizable is the BPPARAM argument. Here, it is possible to specify the parallel back-end that can be used for evaluation. Again, note that parallelization does incur an overhead cost in splitting up the data, sending it off, and combining the results, and thus your mileage may vary depending on the specifics. Below is an example usage of the BiocParallel library supplying BPPARAM argument for constructing the UMAP representation from the PCA results: library(BiocParallel) bm &lt;- bench::mark( runUMAP(tiny_sce_pca, BPPARAM = SerialParam()), runUMAP(tiny_sce_pca, BPPARAM = MulticoreParam()), check = FALSE) .show_bench(bm) ## # A tibble: 2 x 4 ## expression min median mem_alloc ## &lt;bch:expr&gt; &lt;bch:&gt; &lt;bch:&gt; &lt;bch:byt&gt; ## 1 runUMAP(tiny_sce_pca, BPPARAM = SerialParam()) 3.82s 3.82s 0B ## 2 runUMAP(tiny_sce_pca, BPPARAM = MulticoreParam()) 4s 4s 0B On this dataset and equipment, there’s not much, if any, benefit to parallelization, but your mileage may vary dependent on those aspects. 7.3 On-Disk Data The matrix of data from a single-cell experiment can be on the order of tens to hundreds of gigabytes, depending on the number of features and cells measured. A standard approach to import and represent these matrices has been to load the entire data set into memory using either the matrix object in base R or sparse and dense matrix classes from the Matrix R package. This approach however may prove intractable in computing environments with limited memory. Disk-backed representations such as HDF5 free us from having to load an entire dataset into memory, and thus make it possible to work with large-scale scRNA-seq data. library(rhdf5) library(HDF5Array) "],
["annotation-2.html", "Chapter 8 Annotation", " Chapter 8 Annotation "],
["clustering-2.html", "Chapter 9 Clustering 9.1 Understanding the Data 9.2 Manual Clustering 9.3 Automated Clustering", " Chapter 9 Clustering 9.1 Understanding the Data To follow along with this chapter, we will be using the 5 cell line data from the CellBench_data repository. We will load the original dataset, and then subsample down to 1000 cells to make performing downstream calculations faster: library(here) sce &lt;- readRDS(here(&#39;_rfiles/_data/cellbench_sce_sc_10x_5cl_qc.rds&#39;)) sce &lt;- sce[, sample(ncol(sce), 1000)] sce ## class: SingleCellExperiment ## dim: 11786 1000 ## metadata(3): scPipe Biomart log.exprs.offset ## assays(3): counts cpm logcounts ## rownames(11786): COL27A1 SLC35B1 ... RAC2 ANKH ## rowData names(9): is_feature_control mean_counts ... ## n_cells_counts pct_dropout_counts ## colnames(1000): Lib90_03432 Lib90_03122 ... Lib90_02656 ## Lib90_03619 ## colData names(30): unaligned aligned_unmapped ... ## cell_line_demuxlet demuxlet_cls ## reducedDimNames(0): ## spikeNames(0): The per cell labels pertaining to the 5 cell lines assayed can be found within the colData component under the column name cell_line. We can count the number of instances of each cell line present in our subsampled dataset as follows: table(sce$cell_line) ## ## A549 H1975 H2228 H838 HCC827 ## 303 120 199 216 162 Before continuing, we will perform feature selection and dimensionality reduction that will be used downstream by the clustering methods and visualizations. library(BiocSingular) library(scater) library(scran) ## feature selection fit &lt;- trendVar(sce, use.spikes = FALSE) dec &lt;- decomposeVar(sce, fit) hvg_genes &lt;- rownames(dec[dec$bio &gt; 0, ]) # ~4k genes ## dimensionality reduction set.seed(1234) sce &lt;- runPCA(sce, ncomponents = 20, feature_set = hvg_genes, BSPARAM = IrlbaParam()) ## plot(attr(reducedDim(sce, &#39;PCA&#39;), &#39;percentVar&#39;)) ## elbow plot sce &lt;- runUMAP(sce, n_dimred = 20) We can now visualize the 5 cell lines via UMAP: plotUMAP(sce, colour_by = &#39;cell_line&#39;) One thing to note is that, per our UMAP visualization, cell line H1975 appears to be two primary clusters, with some cells resembling the H838 cell line. 9.2 Manual Clustering Low-level clustering relies on building the shared- or k-nearest neighbors graphs manually, and then applying a graph-based clustering algorithm based on the resulting graph. One such wrapper to construct the SNN/KNN graph comes from the scran package’s buildSNNGraph() and buildKNNGraph() functions, respectively. The resulting igraph object from these functions can then be fed into any number of clustering algorithms provided by igraph. For example, louvain clustering is a popular algorithm that is implemented in the cluster_louvain() function. Further, one additional parameter to note in the buildSNNGraph() function below is the BNPARAM, which provides even finer control over nearest-neighbors detection via the BiocNeighbors package. This parameter allows the user to specify an implementation from BiocNeighbors to use that has been designed for high-dimensional data. Here, we highlight the use of an approximate method via the Annoy algorithm by way of providing AnnoyParam(). g &lt;- buildSNNGraph(sce, k = 50, use.dimred = &#39;PCA&#39;) louvain_clusters &lt;- igraph::cluster_louvain(g)$membership sce$louvain_clusters &lt;- as.factor(louvain_clusters) We can then inspect the results of our clustering both on the UMAP as well as via a confusion matrix: plotUMAP(sce, colour_by = &#39;louvain_clusters&#39;) Figure 9.1: Louvain clustering applied to an SNN graph constructed with k=50. table(sce$louvain_clusters, sce$cell_line) ## ## A549 H1975 H2228 H838 HCC827 ## 1 0 8 0 0 162 ## 2 0 12 0 216 0 ## 3 0 4 199 0 0 ## 4 303 1 0 0 0 ## 5 0 95 0 0 0 Overall at this k, we see that the clusters align very sensibly with the cell lines of origins. 9.2.1 Varying k While our first try appeared to work fairly well, this may not always be the case. Furthermore, checking the stability of clustering is sensible to ensure that a clustering result is robust. To iterate over the varying k parameter (the number of nearest neighbors to draw the graph with), we utilize the purrr functional programming library in this example, but note that lapply works equally well. library(purrr) ## Iterate over varying k for SNNGraph construction k_v &lt;- seq(5, 50, by = 15) g_l &lt;- map(k_v, function(x) { buildSNNGraph(sce, k = x, use.dimred = &#39;PCA&#39;) }) names(g_l) &lt;- paste0(&#39;k_&#39;, k_v) ## Iterate over resulting graphs to apply louvain clustering lc_l &lt;- map(g_l, function(g) { igraph::cluster_louvain(g)$membership }) names(lc_l) &lt;- paste0(&#39;k_&#39;, k_v) ## Coerce louvain cluster results list to dataframe of factors lc_df &lt;- data.frame(do.call(cbind, lc_l)) lc_df &lt;- apply(lc_df, 2, as.factor) ## Append results to colData for plotting colData(sce) &lt;- cbind(colData(sce), lc_df) Now we can plot across our various k’s on the UMAP for ease of interpretation. Note that we use the patchwork package to combine the multiple ggplot objects that are generated by the plotUMAP() function. library(patchwork) p_l &lt;- map(colnames(lc_df), ~ plotUMAP(sce, colour_by = .)) wrap_plots(p_l, ncol = 2) Figure 9.2: Varying k, from top left clockwise, k is set to 5, 20, 50, and 35. 9.2.2 Varying Clustering Algorithms Another area for tweaking is in the clustering algorithm applied, as there are various community detection methods available through packages such as igraph. Here, we demonstrate a few select clustering methods - louvain, walktrap, and fast/greedy - across two different sets of graphs generated above, where k equaled 5 or 20: ## Clustering functions (cf) cf_l &lt;- list(louvain = igraph::cluster_louvain, walktrap = igraph::cluster_walktrap, fastgreedy = igraph::cluster_fast_greedy) ## Cluster assignments (ca) per function with k_5 and k_20 graphs ca_l &lt;- map2(c(cf_l, cf_l), c(g_l[rep(c(1:2), each = length(cf_l))]), function(f, g) { f(g)$membership }) names(ca_l) &lt;- paste0(rep(names(cf_l), 2), &#39;__&#39;, rep(names(g_l)[1:2], each = 3)) ## Coerce clustering results list to dataframe of factors ca_df &lt;- data.frame(do.call(cbind, ca_l)) ca_df &lt;- apply(ca_df, 2, as.factor) ## Append results to colData for plotting colData(sce) &lt;- cbind(colData(sce), ca_df) p_l &lt;- map(colnames(ca_df), ~ plotUMAP(sce, colour_by = .)) wrap_plots(p_l, ncol = 2, byrow = FALSE) Figure 9.3: Three clustering methods (by row, top to bottom: louvain, walktrap, and fast/greedy) were applied to two graphs (by column, left to right: k=5, k=20). Clustering results are shown as different colors on the UMAP projection. We can see from this result that the choice of clustering method was largely a non-factor in the k=20 case, whereas the more granular clustering method of k=5 did indeed produce some variation, particularly between the louvain/walktrap vs the fast-greedy. 9.3 Automated Clustering Automated clustering frameworks seek to find an “optimal” number of clusters. The SC3 package provides a simple framework that allows users to test for a variable number of clusters. Additionally, the SC3 package provides handy visualizations to qualitatively assess the clustering results. Before we use the SC3 package, we first set a required rowData column for SC3 to work: ## SC3 requires this column to be appended rowData(sce)$feature_symbol &lt;- rownames(sce) And now we run the sc3() function to test for variable numbers of clusters by setting the ks argument. Here we search for 3 to 7 clusters: library(SC3) ## SC3 will return an SCE object with appended &quot;sc3_&quot; columns sce &lt;- sc3(sce, ks = 3:6, k_estimator = TRUE) After using sc3(), the function returns the original SingleCellExperiment object, but with new columns in colData(sce) corresponding to the different ks supplied to the function, as well as a full representation of the analysis that is stored in metadata(sce)$sc3, which includes an estimate of the optimal k (as dictated by the k_estimator = TRUE argument above). Below, we show the clustering results of the ks we supplied, 3 through 6, shown on the UMAP representation of the data. sc3_cols &lt;- paste0(&#39;sc3_&#39;, 3:6, &#39;_clusters&#39;) p_l &lt;- map(sc3_cols, ~ plotUMAP(sce, colour_by = .)) wrap_plots(p_l, ncol = 2) To access all the output generated by sc3(), we can inspect the metadata component of our sce object: str(metadata(sce)$sc3, 1) ## List of 8 ## $ kmeans_iter_max: num 1e+09 ## $ kmeans_nstart : num 1000 ## $ n_dim : int [1:15] 54 59 44 64 45 62 68 70 63 57 ... ## $ rand_seed : num 1 ## $ n_cores : num 15 ## $ k_estimation : num 14 ## $ transformations:List of 6 ## ..- attr(*, &quot;rng&quot;)=List of 6 ## $ consensus :List of 4 The parameters and various outputs from sc3() are saved shown to be saved in a list. For example, we can access the estimated k (as we asked for via the k_estimator = TRUE argument to sc3()) by inspecting this list as follows below. Based on our data, sc3() estimates the optimal k to be: metadata(sce)$sc3$k_estimation ## [1] 14 The SC3 package contains many more utilities for exploring the stability of clustering and can even produce differential expression analysis results using the biology = TRUE argument within the sc3() function. We leave it to the interested reader to learn more advanced features in SC3 via their vignette. "],
["differential-expression-2.html", "Chapter 10 Differential Expression", " Chapter 10 Differential Expression "],
["imputation-1.html", "Chapter 11 Imputation", " Chapter 11 Imputation "],
["integrating-datasets-1.html", "Chapter 12 Integrating Datasets", " Chapter 12 Integrating Datasets "],
["quality-control.html", "Chapter 13 Quality Control", " Chapter 13 Quality Control ##libsize.drop &lt;- isOutlier(sce.hsc$total_counts, nmads=3, type=&quot;lower&quot;, log=TRUE) ##feature.drop &lt;- isOutlier(sce.hsc$total_features_by_counts, nmads=3, type=&quot;lower&quot;, log=TRUE) "],
["sharing-and-interactive-interfaces.html", "Chapter 14 Sharing and Interactive Interfaces", " Chapter 14 Sharing and Interactive Interfaces "],
["trajectory-analysis-1.html", "Chapter 15 Trajectory Analysis", " Chapter 15 Trajectory Analysis "],
["about-the-data.html", "Chapter 16 About the Data 16.1 10X Genomics PBMC Data 16.2 Cellbench_data 16.3 Human Cell Atlas", " Chapter 16 About the Data 16.1 10X Genomics PBMC Data Bioconductor versions, PBMC4k/PBMC3k. 16.2 Cellbench_data The 5 cell line scRNA-seq data from the CellBench_data repository is used in examples where having a cell label is important. In the CellBench_data repository, the data is stored within the workspace data/sincell_with_class_5cl.RData as sc_10x_5cl_qc. We have extracted it and saved it under this book’s Github repo within the _rfiles/_data folder for ease of use. The data arrives as a SingleCellExperiment class object, and has already undergone basic preprocessing, including quality control and normalization. The most important aspect to this data - the cell labels pertaining to the 5 cell lines sequenced - can be found within the colData component under the column name cell_line. Note that this data will be made available on Bioconductor via the scRNAseq package in the near future. 16.3 Human Cell Atlas Bioconductor access. d = c(&#39;a&#39;, &#39;b&#39;) "],
["about-the-contributors.html", "Chapter 17 About the Contributors", " Chapter 17 About the Contributors "]
]
