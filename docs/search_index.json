[
["index.html", "Orchestrating Single-Cell Analysis with Bioconductor Welcome", " Orchestrating Single-Cell Analysis with Bioconductor 2019-07-23 Welcome This is the website for “Orchestrating Single-Cell Analysis with Bioconductor”, a book that teaches users some common workflows for the analysis of single-cell RNA-seq data (scRNA-seq). This book will teach you how to make use of cutting-edge Bioconductor tools to process, analyze, visualize, and explore scRNA-seq data. Additionally, it serves as an online companion for the manuscript “Orchestrating Single-Cell Analysis with Bioconductor”. While we focus here on scRNA-seq data, a newer technology that profiles transcriptomes at the single-cell level, many of the tools, conventions, and analysis strategies utilized throughout this book are broadly applicable to other types of assays. By learning the grammar of Bioconductor workflows, we hope to provide you a starting point for the exploration of your own data, whether it be scRNA-seq or otherwise. This book is organized into three parts. In the Preamble, we introduce the book and dive into resources for learning R and Bioconductor (both at a beginner and developer level). Part I ends with a tutorial for a key data infrastructure, the SingleCellExperiment class, that is used throughout Bioconductor for single-cell analysis and in the subsequent section. The second part, Workflows, begins with an overview of the framework for analysis of scRNA-seq data - from purely prose, to just code, to a combination of the two. Deeper dives into specific topics are presented in each subsequent chapter. Finally, the appendix has information about the data used throughout the book, and highlights our contributors. If you would like to cite this work, please use the reference “Orchestrating Single-Cell Analysis with Bioconductor”. The book is written in RMarkdown with bookdown. OSCA is a collaborative effort, supported by various folks from the Bioconductor team who have contributed workflows, fixes, and improvements. This website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License. Version 0.0.1.9999 "],
["introduction.html", "Chapter 1 Introduction 1.1 What you will learn 1.2 What you won’t learn 1.3 Who we wrote this for 1.4 Why we wrote this 1.5 Acknowledgements", " Chapter 1 Introduction Bioconductor is an open source, open development software project to provide tools for the analysis and comprehension of high-throughput genomic data. It is based primarily on the R programming language. 1.1 What you will learn The goal of this book is to provide a solid foundation in the usage of Bioconductor tools for single-cell RNA-seq analysis by walking through various steps of typical workflows using example datasets. We strive to tackle key concepts covered in the manuscript, “Orchestrating Single-Cell Analysis with Bioconductor”, with each workflow covering these in varying detail, as well as essential preliminaries that are important for following along with the workflows on your own. 1.1.1 Preliminaries For those unfamiliar with R (and those looking to learn more), we recommend reading the Learning R and More chapter, which first and foremost covers how to get started with R. We point to many great online resources for learning R, as well as related tools that are nice to know for bioinformatic analysis. For advanced users, we also point to some extra resources that go beyond the basics. While we provide an extensive list of learning resources for the interested audience in this chapter, we only ask for some familiarity with R before going to the next section. We then briefly cover getting started with Using R and Bioconductor. Bioconductor, being its own repository, has a unique set of tools, documentation, resources, and practices that benefit from some extra explanation. Data Infrastructure merits a separate chapter. The reason for this is that common data containers are an essential part of Bioconductor workflows because they enable interoperability across packages, allowing for “plug and play” usage of cutting-edge tools. Specifically, here we cover the SingleCellExperiment class in depth, as it has become the working standard for Bioconductor based single-cell analysis packages. Finally, before diving into the various workflows, armed with knowledge about the SingleCellExperiment class, we briefly discuss the datasets that will be used throughout the book in About the Data. 1.1.2 Workflows All workflows begin with data import and subsequent quality control and normalization, going from a raw (count) expression matrix to a clean one. This includes adjusting for experimental factors and possibly even latent factors. Using the clean expression matrix, feature selection strategies can be applied to select the features (genes) driving heterogeneity. Furthermore, these features can then be used to perform dimensionality reduction, which enables downstream analysis that would not otherwise be possible and visualization on 2- or 3-dimensions. From there, the workflows largely focus on differing downstream analyses. Clustering details how to segment a scRNA-seq dataset, and differential expression provides a means to determine what drives the differences between different groups of cells. Integrating datasets walks through merging scRNA-seq datasets, an area of need as the number of scRNA-seq datasets continues to grow and comparisons between datasets must be done. Finally, we touch upon how to work with large scale data, specifically where it becomes impractical or impossible to work with data solely in-memory. As an added bonus, we dedicate a chapter to interactive visualization, which focuses on using the iSEE package to enable active exploration of a single cell experiment’s data. 1.2 What you won’t learn The field of bioinformatic analysis is large and filled with many potential trajectories depending on the biological system being studied and technology being deployed. Here, we only briefly survey some of the many tools available for the analysis of scRNA-seq, focusing on Bioconductor packages. It is impossible to thoroughly review the plethora of tools available through R and Bioconductor for biological analysis in one book, but we hope to provide the means for further exploration on your own. Thus, it goes without saying that you may not learn the optimal workflow for your own data from our examples - while we strive to provide high quality templates, they should be treated as just that - a template from which to extend upon for your own analyses. 1.3 Who we wrote this for We’ve written this book with the interested experimental biologist in mind, and do our best to make few assumptions on previous programming or statistical experience. Likewise, we also welcome more seasoned bioinformaticians who are looking for a starting point from which to dive into single-cell RNA-seq analysis. As such, we welcome any and all feedback for improving this book to help increase accessibility and refine technical details. 1.4 Why we wrote this This book was conceived in the fall of 2018, as single-cell RNA-seq analysis continued its rise in prominence in the field of biology. With its rapid growth, and the ongoing developments within Bioconductor tailored specifically for scRNA-seq, it became apparent that an update to the Orchestrating high-throughput genomic analysis with Bioconductor paper was necessary for the age of single-cell studies. We strive to highlight the fantastic software by people who call Bioconductor home for their tools, and in the process hope to showcase the Bioconductor community at large in continually pushing forward the field of biological analysis. 1.5 Acknowledgements We would like to thank all Bioconductor contributors for their efforts in creating the definitive leading-edge repository of software for biological analysis. It is truly extraordinary to chart the growth of Bioconductor over the years. We are thankful for the wonderful community of scientists and developers alike that together make the Bioconductor community special. We would first and foremost like to thank the Bioconductor core team and the emerging targets subcommittee for commissioning this work, Stephanie Hicks and Raphael Gottardo for their continuous mentorship, and all our contributors to the companion manuscript of this book. We’d also like to thank Garret Grolemund and Hadley Wickham for their book, R for Data Science, from which we drew stylistic and teaching inspiration. We also thank Levi Waldron and Aaron Lun for advice on the code-related aspects of managing the online version of this book. "],
["learning-r-and-more.html", "Chapter 2 Learning R and Bioconductor 2.1 The Benefits of R and Bioconductor 2.2 Learning R Online 2.3 Running R Locally 2.4 Getting Help In (and Out) of R 2.5 Bioconductor Help", " Chapter 2 Learning R and Bioconductor In this chapter, we outline various resources for learning R and Bioconductor. We provide a brief set of instructions for installing R on your own machine, and then cover how to get help for functions, packages, and Bioconductor-specific resources for learning more. 2.1 The Benefits of R and Bioconductor R is a high-level programming language that was initially designed for statistical applications. While there is much to be said about R as a programming language, one of the key advantages of using R is that it is highly extensible through packages. Packages are collections of functions, data, and documentation that extend the capabilities of base R. The ease of development and distribution of packages for R has made it a rich environment for many fields of study and application. One of the primary ways in which packages are distributed is through centralized repositories. The first R repository a user typically runs into is the Comprehensive R Archive Network (CRAN), which hosts over 13,000 packages to date, and is home to many of the most popular R packages. Similar to CRAN, Bioconductor is a repository of R packages as well. However, whereas CRAN is a general purpose repository, Bioconductor focuses on software tailored for genomic analysis. Furthermore, Bioconductor has stricter requirements for a package to be accepted into the repository. Of particular interest to us is the inclusion of high quality documentation and the use of common data infrastructure to promote package interoperability. In order to use these packages from CRAN and Bioconductor, and start programming with R to follow along in these workflows, some knowledge of R is helpful. Here we outline resources to guide you through learning the basics. 2.2 Learning R Online To learn more about programming with R, we highly recommend checking out the online courses offered by Datacamp, which includes both introductory and advanced courses within the R track. Datacamp is all online with many free courses, with videos and a code editor/console that promotes an interactive learning experience. What we like about Datacamp is that it is more focused on topics and programming paradigms that center around data science, which is especially helpful for getting started with R. Beyond just Datacamp, a mainstay resource for learning R is the R for Data Science book. This book illustrates R programming through the exploration of various data science concepts - transformation, visualization, exploration, and more. 2.3 Running R Locally While learning R through online resources is a great way to start with R, as it requires minimal knowledge to start up, at some point, it will be desirable to have a local installation - on your own hardware - of R. This will allow you to install and maintain your own software and code, and furthermore allow you to create a personalized workspace. 2.3.1 Installing R Prior to getting started with this book, some prior programming experience with R is helpful. Check out the Learning R and More chapter for a list of resources to get started with R and other useful tools for bioinformatic analysis. To follow along with the analysis workflows in this book on your personal computer, it is first necessary to install the R programming language. Additionally, we recommend a graphical user interface such as RStudio for programming in R and visualization. RStudio features many helpful tools, such as code completion and an interactive data viewer to name but two. For more details, please see the online book R for Data Science prerequisites section for more information about installing R and using RStudio. 2.3.1.1 For MacOS/Linux Users A special note for MacOS/Linux users: we highly recommend using a package manager to manage your R installation. This differs across different Linux distributions, but for MacOS we highly recommend the Homebrew package manager. Follow the website directions to install homebrew, and install R via the commandline with brew install R, and it will automatically configure your installation for you. Upgrading to new R versions can be done by running brew upgrade. 2.3.2 Installing R &amp; Bioconductor Packages After installing R, the next step is to install R packages. In the R console, you can install packages from CRAN via the install.packages() function. In order to install Bioconductor packages, we will first need the BiocManager package which is hosted on CRAN. This can be done by running: install.packages(&quot;BiocManager&quot;) The BiocManager package makes it easy to install packages from the Bioconductor repository. For example, to install the SingleCellExperiment package, we run: ## the command below is a one-line shortcut for: ## library(BiocManager) ## install(&quot;SingleCellExperiment&quot;) BiocManager::install(&quot;SingleCellExperiment&quot;) Throughout the book, we can load packages via the library() function, which by convention usually comes at the top of scripts to alert readers as to what packages are required. For example, to load the SingleCellExperiment package, we run: library(SingleCellExperiment) Many packages will be referenced throughout the book within the workflows, and similar to the above, can be installed using the BiocManager::install() function. 2.4 Getting Help In (and Out) of R One of the most helpful parts of R is being able to get help inside of R. For example, to get the manual associated with a function, class, dataset, or package, you can prepend the code of interest with a ? to retrieve the relevant help page. For example, to get information about the data.frame() function, the SingleCellExperiment class, the in-built iris dataset, or for the BiocManager package, you can type: ?data.frame ?SingleCellExperiment ?iris ?BiocManager Beyond the R console, there are myriad online resources to get help. The R for Data Science book has a great section dedicated to looking for help outside of R. In particular, Stackoverflow’s R tag is a helpful resource for asking and exploring general R programming questions. 2.5 Bioconductor Help One of the key tenets of Bioconductor software that makes it stand out from CRAN is the required documentation of packages and workflows. In addition, Bioconductor hosts a Bioconductor-specific support site that has grown into a valuable resource of its own, thanks to the work of dedicated volunteers. 2.5.1 Bioconductor Packages Each package hosted on Bioconductor has a dedicated page with various resources. For an example, looking at the scater package page on Bioconductor, we see that it contains: a brief description of the package at the top, in addition to the authors, maintainer, and an associated citation installation instructions that can be cut and paste into your R console documentation - vignettes, reference manual, news Here, the most important information comes from the documentation section. Every package in Bioconductor is required to be submitted with a vignette - a document showcasing basic functionality of the package. Typically, these vignettes have a descriptive title that summarizes the main objective of the vignette. These vignettes are a great resource for learning how to operate the essential functionality of the package. The reference manual contains a comprehensive listing of all the functions available in the package. This is a compilation of each function’s manual, aka help pages, which can be accessed programmatically in the R console via ?&lt;function&gt;. Finally, the NEWS file contains notes from the authors which highlight changes across different versions of the package. This is a great way of tracking changes, especially functions that are added, removed, or deprecated, in order to keep your scripts current with new versions of dependent packages. Below this, the Details section covers finer nuances of the package, mostly relating to its relationship to other packages: upstream dependencies (Depends, Imports, Suggests fields): packages that are imported upon loading the given package downstream dependencies (Depends On Me, Imports Me, Suggests Me): packages that import the given package when loaded For example, we can see that an entry called simpleSingle in the Depends On Me field on the scater page takes us to a step-by-step workflow for low-level analysis of single-cell RNA-seq data. One additional Details entry, the biocViews, is helpful for looking at how the authors annotate their package. For example, for the scater package, we see that it is associated with DataImport, DimensionReduction, GeneExpression, RNASeq, and SingleCell, to name but some of its many annotations. We cover biocViews in more detail. 2.5.2 biocViews To find packages via the Bioconductor website, one useful resource is the BiocViews page, which provides a hierarchically organized view of annotations associated with Bioconductor packages. Under the “Software” label for example (which is comprised of most of the Bioconductor packages), there exist many different views to explore packages. For example, we can inspect based on the associated “Technology”, and explore “Sequencing” associated packages, and furthermore subset based on “RNASeq”. Another area of particular interest is the “Workflow” view, which provides Bioconductor packages that illustrate an analytical workflow. For example, the “SingleCellWorkflow” contains the aforementioned tutorial, encapsulated in the simpleSingleCell package. 2.5.3 Bioconductor Forums The Bioconductor support site contains a Stackoverflow-style question and answer support site that is actively contributed to from both users and package developers. Thanks to the work of dedicated volunteers, there are ample questions to explore to learn more about Bioconductor specific workflows. Another way to connect with the Bioconductor community is through Slack, which hosts various channels dedicated to packages and workflows. The Bioc-community Slack is a great way to stay in the loop on the latest developments happening across Bioconductor, and we recommend exploring the “Channels” section to find topics of interest. "],
["beyond-r-basics.html", "Chapter 3 Beyond R Basics 3.1 Becoming an R Expert 3.2 Becoming an R/Bioconductor Developer 3.3 Nice Companions for R", " Chapter 3 Beyond R Basics Here we briefly outline resources for taking your R programming to the next level, including resources for learning about package development. We also outline some companions to R that are good to know not only for package development, but also for running your own bioinformatic pipelines, enabling you to use a broader array of tools to go from raw data to preprocessed data before working in R. 3.1 Becoming an R Expert For a deeper dive into the finer details of the R programming language, the Advanced R. While targeted at more experienced R users and programmers, this book represents a comprehensive compendium of more advanced concepts, and touches on some of the paradigms used extensively by developers throughout Bioconductor, specifically programming with S4. Eventually, you’ll reach the point where you have your own collection of functions, datasets, and reach the point where you will be writing your own packages. Luckily, there’s a guide for just that, with the book R Packages. Packages are great even if just for personal use, and of course, with some polishing, can eventually become a package available on CRAN or Bioconductor. Furthermore, they are also a great way of putting together code associated with a manuscript, promoting reproducible, accessible computing practices, something we all strive for in our work. For many of the little details that are oft forgotten learning about R, the aptly named What They Forgot to Teach You About R is a great read for learning about the little things such as file naming, maintaining an R installation, and reproducible analysis habits. Finally, we save the most intriguing resource for last - another book for those on the road to becoming an R expert is R Inferno, which dives into many of the unique quirks of R. Warning: this book goes very deep into the painstaking details of R. 3.2 Becoming an R/Bioconductor Developer While learning to use Bioconductor tools is a very welcoming experience, unfortunately there is no central resource for navigating the plethora of gotchas and paradigms associated with developing for Bioconductor. Based on conversations with folks involved in developing for Bioconductor, much of this knowledge is hard won and fairly spread out. This however is beginning to change with more recent efforts led by the Bioconductor team, and while this book represents an earnest effort towards addressing the user perspective, it is currently out of scope to include a deep dive about the developer side. For those looking to get started with developing packages for Bioconductor, it is important to first become acquainted with developing standalone R packages. To this end, the R Packages book provides a deep dive into the details of constructing your own package, as well as details regarding submission of a package to CRAN. For programming practices, With that, some resources that are worth looking into to get started are the BiocWorkshops repository under the Bioconductor Github provides a book composed of workshops that have been hosted by Bioconductor team members and contributors. These workshops center around learning, using, and developing for Bioconductor. A host of topics are also available via the Learn module on the Bioconductor website as well. Finally, the Bioconductor developers portal contains a bevy of individual resources and guides for experienced R developers. 3.3 Nice Companions for R While not essential for our purposes, many bioinformatic tools for processing raw sequencing data require knowledge beyond just R to install, run, and import their results into R for further analysis. The most important of which are basic knowledge of the Shell/Bash utilities, for working with bioinformatic pipelines and troubleshooting (R package) installation issues. Additionally, for working with packages or software that are still in development and not hosted on an official repository like CRAN or Bioconductor, knowledge of Git - a version control system - and the popular Github repository is helpful. This enables you to not only work with other people’s code, but also better manage your own code to keep track of changes. 3.3.1 Shell/Bash Datacamp and other interactive online resources such as Codecademy are great places to learn some of these extra skills. We highly recommend learning Shell/Bash, as it is the starting point for most bioinformatic processing pipelines. 3.3.2 Git We would recommend learning Git next, a system for code versioning control which underlies the popular Github repository, where many of the most popular open source tools are hosted. Learning Git is essential for not only keeping track of your own code, but also for using, managing, and contributing to open source software projects. For a more R centric look at using Git (and Github), we highly recommend checking out Happy Git and Github for the useR. 3.3.3 Other Languages A frequent question that comes up is “What else should I learn besides R?” Firstly, we believe that honing your R skills is first and foremost, and beyond just R, learning Shell/Bash and Git covered in the Nice Companions for R section are already a great start. For those just getting started, these skills should become comfortable in practice before moving on. However, there are indeed benefits to going beyond just R. At a basic level, learning other programming languages helps broaden one’s perspective - similar to learning multiple spoken or written languages, learning about other programming languages (even if only in a cursory manner) helps one identify broader patterns that may be applicable across languages. At an applied level, work within and outside of R has made it ever more friendly now than ever before with multi-lingual setups and teams, enabling the use of the best tool for the job at hand. For example, Python is another popular language used in both data science and a broader array of applications as well. R now supports a native Python interface via the reticulate package, enabling access to tools developed originally in Python such as the popular TensorFlow framework for machine learning applications. C++ is frequently used natively in R as well via Rcpp in packages to massively accelerate computations. Finally, multiple langauges are supported in code documents and reports through R Markdown. "],
["data-infrastructure.html", "Chapter 4 Data Infrastructure 4.1 Prerequisites 4.2 The SingleCellExperiment Class 4.3 A Brief Recap: From se to sce 4.4 The reducedDims Slot 4.5 One More Thing: metadata Slot 4.6 About Spike-Ins 4.7 Working with SingleCellExperiment 4.8 The Centrality of SingleCellExperiment 4.9 Multimodal Data: MultiAssayExperiment", " Chapter 4 Data Infrastructure One of the advantages of using Bioconductor packages is that they utilize common data infrastructures which makes analyses interoperable across various packages. Furthermore, much engineering effort is put into making this infrastructure robust and scalable. Here, we describe the SingleCellExperiment object (or sce in shorthand) in detail to describe how it is constructed, utilized in downstream analysis, and how it stores various types of primary data and metadata. 4.1 Prerequisites The Bioconductor package SingleCellExperiment provides the SingleCellExperiment class for usage. While the package is implicitly installed and loaded when using any package that depends on the SingleCellExperiment class, it can be explicitly installed (and loaded) as follows: BiocManager::install(&#39;SingleCellExperiment&#39;) Additionally, we use some functions from the scater and scran packages, as well as the CRAN package uwot (which conveniently can also be installed through BiocManager). These functions will be accessed through the &lt;package&gt;::&lt;function&gt; convention as needed. BiocManager::install(c(&#39;scater&#39;, &#39;scran&#39;, &#39;uwot&#39;)) For this session, all we will need loaded is the SingleCellExperiment package: library(SingleCellExperiment) 4.2 The SingleCellExperiment Class Overview of the SingleCellExperiment class object 4.2.1 Primary Data: The assays Slot The SingleCellExperiment (sce) object is the basis of single-cell analytical applications based in Bioconductor. The sce object is an S4 object, which in essence provides a more formalized approach towards construction and accession of data compared to other methods available in R. The utility of S4 comes from validity checks that ensure that safe data manipulation, and most important for our discussion, from its extensibility through slots. If we imagine the sce object to be a ship, the slots of sce can be thought of as individual cargo boxes - each exists as a separate entity within the sce object. Furthermore, each slot contains data that arrives in its own format. To extend the metaphor, we can imagine that different variations of cargo boxes are required for fruits versus bricks. In the case of sce, certain slots expect numeric matrices, whereas others may expect data frames. To construct a rudimentary sce object, all we need is a single slot: assays slot: contains primary data such as counts in list, where each entry of the list is in a matrix format, where rows correspond to features (genes) and columns correspond to samples (cells) (Figure 1A, blue box) Let’s start simple by generating three cells worth of count data across ten genes. counts_matrix &lt;- data.frame(cell_1 = rpois(10, 10), cell_2 = rpois(10, 10), cell_3 = rpois(10, 30)) rownames(counts_matrix) &lt;- paste0(&quot;gene_&quot;, 1:10) counts_matrix &lt;- as.matrix(counts_matrix) # must be a matrix object! From this, we can now construct our first SingleCellExperiment object, using the defined constructor, SingleCellExperiment(). Note that we provide our data as a named list, and each entry of the list is a matrix. Here, we name the counts_matrix entry as simply counts within the list. sce &lt;- SingleCellExperiment(assays = list(counts = counts_matrix)) To inspect the object, we can simply type sce into the console to see some pertinent information, which will display an overview of the various slots available to us (which may or may not have any data). sce ## class: SingleCellExperiment ## dim: 10 3 ## metadata(0): ## assays(1): counts ## rownames(10): gene_1 gene_2 ... gene_9 gene_10 ## rowData names(0): ## colnames(3): cell_1 cell_2 cell_3 ## colData names(0): ## reducedDimNames(0): ## spikeNames(0): To access the count data we just supplied, we can do any one of the following: assay(sce, &quot;counts&quot;) - this is the most general method, where we can supply the name of the assay as the second argument. counts(sce) - this is the same as the above, but only works for assays with the special name &quot;counts&quot;. counts(sce) ## cell_1 cell_2 cell_3 ## gene_1 6 10 27 ## gene_2 11 6 41 ## gene_3 6 8 28 ## gene_4 8 12 29 ## gene_5 8 11 32 ## gene_6 7 7 36 ## gene_7 6 18 33 ## gene_8 7 13 26 ## gene_9 15 12 41 ## gene_10 9 13 46 ## assay(sce, &quot;counts&quot;) ## same as above in this special case 4.2.2 Extending the assays Slot What makes the assay slot especially powerful is that it can hold multiple representations of the primary data. This is especially useful for storing a normalized version of the data. We can do just that as shown below, using the scran and scater packages to compute a log-count normalized representation of the initial primary data. Note that here, we overwrite our previous sce upon reassigning the results to sce - this is because these functions return a SingleCellExperiment object. Some functions - especially those outside of single-cell oriented Bioconductor packages - do not, in which case you will need to append your results to the sce object (see below). sce &lt;- scran::computeSumFactors(sce) sce &lt;- scater::normalize(sce) Viewing the object again, we see that these functions added some new entries: sce ## class: SingleCellExperiment ## dim: 10 3 ## metadata(1): log.exprs.offset ## assays(2): counts logcounts ## rownames(10): gene_1 gene_2 ... gene_9 gene_10 ## rowData names(0): ## colnames(3): cell_1 cell_2 cell_3 ## colData names(0): ## reducedDimNames(0): ## spikeNames(0): Specifically, we see that the assays slot has grown to be comprised of two entries: counts (our initial data) and logcounts (the normalized data). Similar to counts, the logcounts name is a special name which lets us access it simply by typing logcounts(sce), although the longhand version works just as well. logcounts(sce) ## cell_1 cell_2 cell_3 ## gene_1 3.79 4.10 3.92 ## gene_2 4.61 3.42 4.49 ## gene_3 3.79 3.80 3.97 ## gene_4 4.18 4.35 4.02 ## gene_5 4.18 4.23 4.15 ## gene_6 4.00 3.62 4.31 ## gene_7 3.79 4.91 4.19 ## gene_8 4.00 4.46 3.87 ## gene_9 5.05 4.35 4.49 ## gene_10 4.34 4.46 4.65 ## assay(sce, &quot;logcounts&quot;) ## same as above Notice that the data before had a severe discrepancy in counts between cells 1/2 versus 3, and that normalization has ameliorated this difference. To look at all the available assays within sce, we can type: assays(sce) ## List of length 2 ## names(2): counts logcounts While the functions above demonstrate automatic addition of assays to our sce object, there may be cases where we want to perform our own calculations and save the result into the assays slot. In particular, this is important for using functions that do not return your SingleCellExperiment object. Let’s append a new version of the data that has been offset by +100. counts_100 &lt;- assay(sce, &quot;counts&quot;) + 100 assay(sce, &quot;counts_100&quot;) &lt;- counts_100 # assign a new entry to assays slot Then we can use the accessor assays() (notice this is plural!) to see all our entries into the assay slot that we have made so far. Note that to see the names of all the assays, we use the plural assays() accessor, and to retrieve a single assay entry (as a matrix) we use the singular assay() accessor, providing the name of the assay we wish to retrieve as above. assays(sce) ## List of length 3 ## names(3): counts logcounts counts_100 These entries are also seen on the default view of sce: sce ## class: SingleCellExperiment ## dim: 10 3 ## metadata(1): log.exprs.offset ## assays(3): counts logcounts counts_100 ## rownames(10): gene_1 gene_2 ... gene_9 gene_10 ## rowData names(0): ## colnames(3): cell_1 cell_2 cell_3 ## colData names(0): ## reducedDimNames(0): ## spikeNames(0): This sort of extension of the assays slot is represented graphically in Figure 1B (dark blue box), showing the addition of the logcounts matrix into the assays slot. In a similar manner, many of the slots of sce are extendable through assignment as shown above, thus allowing for myriad custom functionality as needed for interoperability with functions outside of single-cell oriented Bioconductor packages. 4.2.3 Column (Meta)Data: colData Slot To further annotate our sce object, one of the first and most useful pieces of information is adding on metadata that describes the columns of our primary data, e.g. describing the samples or cells of our experiment. This data is entered into the colData slot: colData slot: metadata that describes that samples (cells) provided as a data.frame or (DataFrame if appending), where rows correspond to cells, and columns correspond to the sample (cells) metadata features (e.g. id, batch, author, etc.) (Figure 1A, orange box). So, let’s come up with some metadata for the cells, starting with a batch variable, where cells 1 and 2 are in batch 1, and cell 3 is from batch 2. cell_metadata &lt;- data.frame(batch = c(1, 1, 2)) rownames(cell_metadata) &lt;- paste0(&quot;cell_&quot;, 1:3) Now, we can take two approaches - either append the cell_metadata to our existing sce, or start from scratch via the SingleCellExperiment() constructor and provide it from the get go. We’ll start from scratch for now, but will also show how to append the data as well: ## From scratch: sce &lt;- SingleCellExperiment(assays = list(counts = counts_matrix), colData = cell_metadata) ## Appending to existing object (requires DataFrame() coercion) ## colData(sce) &lt;- DataFrame(cell_metadata) Similar to assays, we can see our colData is now populated from the default view of sce: sce ## class: SingleCellExperiment ## dim: 10 3 ## metadata(0): ## assays(1): counts ## rownames(10): gene_1 gene_2 ... gene_9 gene_10 ## rowData names(0): ## colnames(3): cell_1 cell_2 cell_3 ## colData names(1): batch ## reducedDimNames(0): ## spikeNames(0): And furthermore access our column (meta)data with the accessor, colData(): colData(sce) ## DataFrame with 3 rows and 1 column ## batch ## &lt;numeric&gt; ## cell_1 1 ## cell_2 1 ## cell_3 2 Finally, some packages automatically add to the colData slot, for example, the scater package features a function, calculateQCMetrics(), which appends a lot of quality control data. Here we show the first five columns of colData(sce) with the quality control metrics appended to it. sce &lt;- scater::calculateQCMetrics(sce) colData(sce)[, 1:5] ## DataFrame with 3 rows and 5 columns ## batch is_cell_control total_features_by_counts ## &lt;numeric&gt; &lt;logical&gt; &lt;integer&gt; ## cell_1 1 FALSE 10 ## cell_2 1 FALSE 10 ## cell_3 2 FALSE 10 ## log10_total_features_by_counts total_counts ## &lt;numeric&gt; &lt;integer&gt; ## cell_1 1.04139268515822 83 ## cell_2 1.04139268515822 110 ## cell_3 1.04139268515822 339 4.2.3.1 Using colData for Subsetting A common operation with colData is its use in subsetting. One simple way to access colData is through the use of the $ operator, which is a shortcut for accessing a variable within the colData slot: sce$batch ## [1] 1 1 2 ## colData(sce)$batch # same as above If we only wanted cells within batch 1, we could subset our sce object as follows (remember, we subset on the columns in this case because we are filtering by cells/samples here). sce[, sce$batch == 1] ## class: SingleCellExperiment ## dim: 10 2 ## metadata(0): ## assays(1): counts ## rownames(10): gene_1 gene_2 ... gene_9 gene_10 ## rowData names(7): is_feature_control mean_counts ... total_counts ## log10_total_counts ## colnames(2): cell_1 cell_2 ## colData names(10): batch is_cell_control ... ## pct_counts_in_top_200_features pct_counts_in_top_500_features ## reducedDimNames(0): ## spikeNames(0): 4.2.4 Feature Metadata: rowData/rowRanges Lastly, the rows also have their own metadata slot to store information that pertains to the features of the sce object: rowData slot: contains data in a data.frame (DataFrame) format that describes aspects of the data corresponding to the rows of the primary data (Figure 1A, green box). Furthermore, there is a special slot which pertains to features with genomic coordinates: rowRanges slot: contains data in a GRangesList (where each entry is a GenomicRanges format) that describes the chromosome, start, and end coordinates of the features (genes, genomic regions). Both of these can be accessed via their respective accessors, rowRanges() and rowData(). In our case, rowRanges(sce) produces an empty list: rowRanges(sce) # empty ## GRangesList object of length 10: ## $gene_1 ## GRanges object with 0 ranges and 0 metadata columns: ## seqnames ranges strand ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; ## ------- ## seqinfo: no sequences ## ## $gene_2 ## GRanges object with 0 ranges and 0 metadata columns: ## seqnames ranges strand ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; ## ------- ## seqinfo: no sequences ## ## $gene_3 ## GRanges object with 0 ranges and 0 metadata columns: ## seqnames ranges strand ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; ## ------- ## seqinfo: no sequences ## ## ... ## &lt;7 more elements&gt; However, our call to calculateQCMetrics(sce) in the prior section filled in the rowData slot of our sce object, as we can see below (only the first three columns are shown for brevity): rowData(sce)[, 1:3] ## DataFrame with 10 rows and 3 columns ## is_feature_control mean_counts log10_mean_counts ## &lt;logical&gt; &lt;numeric&gt; &lt;numeric&gt; ## gene_1 FALSE 14.3333333333333 1.18563657696191 ## gene_2 FALSE 19.3333333333333 1.3082085802911 ## gene_3 FALSE 14 1.17609125905568 ## gene_4 FALSE 16.3333333333333 1.23888208891514 ## gene_5 FALSE 17 1.25527250510331 ## gene_6 FALSE 16.6666666666667 1.24715461488113 ## gene_7 FALSE 19 1.30102999566398 ## gene_8 FALSE 15.3333333333333 1.21307482530885 ## gene_9 FALSE 22.6666666666667 1.37413709399941 ## gene_10 FALSE 22.6666666666667 1.37413709399941 In a similar fashion to the colData slot, such feature metadata could be provided at the onset when creating the SingleCellExperiment object, which we leave up to the reader as an exercise. 4.2.4.1 Subsetting with on Rows To subset an sce object down at the feature/gene level, we can do a row subsetting operation similar to other R objects, by supplying either numeric indices or a vector of names: sce[c(&quot;gene_1&quot;, &quot;gene_4&quot;), ] ## class: SingleCellExperiment ## dim: 2 3 ## metadata(0): ## assays(1): counts ## rownames(2): gene_1 gene_4 ## rowData names(7): is_feature_control mean_counts ... total_counts ## log10_total_counts ## colnames(3): cell_1 cell_2 cell_3 ## colData names(10): batch is_cell_control ... ## pct_counts_in_top_200_features pct_counts_in_top_500_features ## reducedDimNames(0): ## spikeNames(0): ## sce[c(1, 4), ] # same as above in this case 4.2.5 Size Factors Slot: sizeFactors Briefly, we already encountered this via the scran::computeSumFactors(sce) call, which adds a sizeFactors slot: sizeFactors slot: contains information in a numeric vector regarding the sample/cell normalization factors used to produce a normalize data representation (Figure 1B, brown box) sce &lt;- scran::computeSumFactors(sce) sce &lt;- scater::normalize(sce) sizeFactors(sce) ## [1] 0.468 0.620 1.912 4.3 A Brief Recap: From se to sce So far, we have covered the assays (primary data), colData (sample metadata), rowData/rowRanges (feature metadata), and sizeFactors slots of SingleCellExperiment. What is important to note is that the SingleCellExperiment class derives from the SummarizedExperiment (se) class, its predecessor, and in particular inherits the aforementioned slots. As such, much of the SummarizedExperiment functionality is retained in SingleCellExperiment. This allows existing methods that work with SummarizedExperiment to work similarly on SingleCellExperiment objects. So what’s new about the SingleCellExperiment class then? For our discussion, the most important change is the addition of a new slot called reducedDims. 4.4 The reducedDims Slot The reducedDims slot is a new addition which is specially designed to store the reduced dimensionality representations of primary data, such as PCA, tSNE, UMAP, and others. reducedDims slot: contains a list of numeric matrix entries which describe dimensionality reduced representations of the primary data, such that rows represent the columns of the primary data (aka the samples/cells), and columns represent the dimensions Most importantly, just like the assays slot, the reducedDims slot can hold a list of many entries. So, it can hold a PCA, TSNE, and UMAP representation of a given dataset all within the reducedDims slot. In our example, we can calculate a PCA representation of our data as follows using the scater package function runPCA(). We see that the sce now shows a new reducedDim and that the accessor reducedDim() produces the results of running PCA on the normalized data from logcounts(sce). sce &lt;- scater::runPCA(sce) reducedDim(sce, &quot;PCA&quot;) ## PC1 PC2 PC3 ## cell_1 -1.05 -1.923 9.84e-15 ## cell_2 2.98 0.343 9.84e-15 ## cell_3 -1.93 1.580 9.84e-15 ## attr(,&quot;percentVar&quot;) ## [1] 6.84e-01 3.16e-01 1.45e-29 From this, we can also calculate a tSNE representation using the scater package function runTSNE(), and see that it can be seen both in the default view of sce and via accession: sce &lt;- scater::runTSNE(sce, perplexity = 0.1) ## Perplexity should be lower than K! reducedDim(sce, &quot;TSNE&quot;) ## [,1] [,2] ## cell_1 2083 -5289 ## cell_2 -5626 821 ## cell_3 3542 4468 We can view the names of all our entries in the reducedDims slot via the accessor, reducedDims() (notice that this is plural, and thus not the same as reducedDim(): reducedDims(sce) ## List of length 2 ## names(2): PCA TSNE Now, say we have a different dimensionality reduction approach which has not yet been implemented with SingleCellExperiment objects in mind. For example, let’s say we want to try the umap() function as implemented in the uwot package (which is a much faster version of the default umap implementation currently in scater). Similar to how we extended the assays slot with our own custom entry of counts_100, we can do similarly for the reducedDims slot: u &lt;- uwot::umap(t(logcounts(sce)), n_neighbors = 2) reducedDim(sce, &quot;UMAP_uwot&quot;) &lt;- u reducedDim(sce, &quot;UMAP_uwot&quot;) ## [,1] [,2] ## cell_1 -0.472 -0.476 ## cell_2 0.636 0.194 ## cell_3 -0.164 0.282 ## attr(,&quot;scaled:center&quot;) ## [1] -6.74 -5.45 And we can also see its entry when we look at the reducedDims() accessor output: reducedDims(sce) ## List of length 3 ## names(3): PCA TSNE UMAP_uwot 4.5 One More Thing: metadata Slot Some analyses produce results that do not fit into the aforementioned slots. Thankfully, there is a slot just for this type of messy data, and in fact, can accommodate any type of data, so long as it is in a named list: metadata slot: a named list of entries, where each entry in the list can be anything you want it to be For example, say we have some favorite genes, such as highly variable genes, we want to save inside of sce for use in our analysis at a later point. We can do this simply by appending to the metadata slot as follows: my_genes &lt;- c(&quot;gene_1&quot;, &quot;gene_5&quot;) metadata(sce) &lt;- list(favorite_genes = my_genes) metadata(sce) ## $favorite_genes ## [1] &quot;gene_1&quot; &quot;gene_5&quot; Similarly, we can append more information via the $ operator: your_genes &lt;- c(&quot;gene_4&quot;, &quot;gene_8&quot;) metadata(sce)$your_genes &lt;- your_genes metadata(sce) ## $favorite_genes ## [1] &quot;gene_1&quot; &quot;gene_5&quot; ## ## $your_genes ## [1] &quot;gene_4&quot; &quot;gene_8&quot; 4.6 About Spike-Ins You might have noticed that the sce default view produces an entry with spikeNames. The SingleCellExperiment object contains some special considerations for experiments with spike-in (ERCC) controls. We leave this to the interested reader to learn more about in the SingleCellExperiment introductory vignette. 4.7 Working with SingleCellExperiment In subsequent sections, we will show an example workflow that uses the SingleCellExperiment object as its base, and similar to our walkthrough of the SingleCellExperiment class above, continually appends new entries to save the results of the analysis. The SingleCellExperiment thus can serve as a record of analysis in this manner. This makes it especially useful for collaboration, as the object can be transferred and then visualized via graphical user interfaces such as iSEE. 4.8 The Centrality of SingleCellExperiment Graph network of package dependencies linking to the SingleCellExperiment package (class). Packages are filtered by biocView “singleCell”. To emphasize its importance, here we show the centrality of the SingleCellExperiment class to the Bioc-verse single-cell related packages. It is this connection to SingleCellExperiment that makes many of these packages easily interoperable and modular over the span of an scRNA-seq analysis. 4.9 Multimodal Data: MultiAssayExperiment Recent advances in technology and protocols allow the simultaneous collection of DNA and RNA from the same cells, enabling single-cell multi-modal analysis. These data present new challenges in the complexity of statistical analyses, which are addressed in Bioconductor through the MultiAssayExperiment container. The MultiAssayExperiment class integrates all major Bioconductor experimental data containers, and any containers derived from those, including SingleCellExperiment. It provides harmonized data management for heterogeneous assays, including subsetting by genomic identifiers, genomic coordinates, or sample/cell attributes such as cell type. The user interface mimics that of SingleCellExperiment, with comparable actions working across all assays. Multi-modal profiling is an emergent area of single-cell biology with many exciting technologies coming online, such as gene expression profiling in tandem with protein via CITE-seq/REAP-seq and adaptive repertoire sequencing. While we won’t cover multimodal data analysis further in this online book as of this writing, we anticipate infrastructure and statistical methodology advances in this area in the near future. "],
["analytical-workflow-overview.html", "Chapter 5 Analytical Workflow Overview 5.1 Experimental Design 5.2 Preprocessing 5.3 Import to R 5.4 Data Processing 5.5 Downstream Statistical Analyses 5.6 Accessible &amp; Reproducible Analysis", " Chapter 5 Analytical Workflow Overview Figure 1. Overview of a typical scRNA-seq workflow. In this chapter, we will orient you to the framework of scRNA-seq, shown in the figure above. This chapter will primarily be textual to give a brief conceptual basis to each of the steps presented below. In the subsequent chapter we will walk through a minimal analysis of a simple scRNA-seq dataset to put code to the concepts presented here, before breaking down individual steps and highlighting alternate or more advanced ways of performing specific tasks. 5.1 Experimental Design Before getting started with actual code, a brief word on the importance of experimental design is warranted. This step is done prior to any sequencing, and involves the proper annotation of associated experimental metadata. This metadata is essential, if not for downstream analyses then at minimum for publication of the dataset and submission to public repositories such as NCBI GEO. Some key metadata to record includes, but is not limited to: Experimental aspects: batch, operator/author, date collected, date processed, date submitted for sequencing Biological traits: organism, sex, age, tissue, isolate, disease Perturbations: genotype, treatment Sequencing strategy: molecule, source, strategy, target, instrument model, read length, single vs paired-end sequencing, barcodes And finally, long form descriptions should be provided alongside informative sample names that encompass the key variables of the experiment. This metadata will often be read in and supplied as colData into the constructed SingleCellExperiment class object. 5.2 Preprocessing Once an experiment has been processed and sequenced, scRNA-seq experiments must be aligned to the transcriptome, and subsequently the reads must be quantified into a counts matrix of expression values consisting of cells versus the features of interest (genes or transcripts). While the specific bioinformatic choices defining this preprocessing pipeline are often technology- or platform-dependent, they are worth discussing briefly. Many popular preprocessing methods are available as command line software that is run outside of R: For the 10X Genomics platform, 10X provides Cell Ranger, which executes a custom pipeline compatible with gene expression as well as feature barcoding techniques such as CITE-seq. Under the hood, it uses the STAR aligner. For droplet-based scRNA-seq as well as 10X, Salmon - a tool originally designed for bulk RNA-seq processing - has a submodule specifically designed for scRNA-seq called Alevin. For other scRNA-seq platforms - most often plate-based methods that demultiplex into a directory per well - methods first created for bulk RNA-seq such as Salmon, Kallisto, and STAR will work. In addition to the above, there also exist Bioconductor packages tailored for scRNA-seq processing within R. The scPipe package uses the Rsubread package under the hood to process droplet and plate-based protocols. In addition, the scruff Bioconductor package is especially designed for the processing of CEL-Seq/CEL-Seq2. One important note: some software such as Cell Ranger automatically filters cells silently based on quality control metrics such as via the “knee plot” method (aka barcode ranks, log-counts by log-rank). Such algorithmic filtering methods may not always work. Thus, we recommend forcing an expected number of cells to be output. In Cell Ranger, this takes the form of supplying the additional argument --force-cells. That way, you can determine your own filtering criteria and assess its efficacy, as well as perform analyses on ambient RNA using empty droplets present in your raw data. 5.3 Import to R In all the cases of above, the end result is a counts matrix of expression values. For the command line preprocessing methods, this matrix will need to be imported into R. This can be done with the help of DropletUtils (which directly creates a SingleCellExperiment object from 10X data via the read10xCounts() function), or tximeta/tximport. In cases where the counts matrix is not directly instantiated into a SingleCellExperiment class object, a basic one can be created manually. 5.4 Data Processing Once the scRNA-seq data has been imported into R and a SingleCellExperiment class object constructed, the next step is to create a clean expression matrix and fundamental dimensionality reduced representations of the data that can together be used in subsequent downstream analyses. 5.4.1 Quality Control Metrics Quality control metrics are utilized to assess not only the overall success of the experiment, but also to determine individual cell level reaction successes and failures. This step ultimately leads to the calculation of various quality control metrics, which can be used either to completely exclude poor-quality droplets/cells or be accounted for in downstream analyses. Some example metrics include total UMI counts, doublet identification, number of mitochondrial reads as a surrogate for cell damage, and complexity (number of genes assigned at least one read). 5.4.2 Normalizing Data Transforming the counts data into a normalized representation allows for cell and gene-specific biases to be eliminated prior to downstream analyses that depend on explicit gene expression value comparisons. This transformation is important for visualizing gene expression data across clusters (where cells may be of different sizes, e.g. have different library sizes) and for tasks such as differential expression analyses. 5.4.3 Feature Selection In most experiments, only a subset of genes drive observed heterogeneity across the population of cells profiled in an scRNA-seq experiment. The aim of performing feature selection is to both reduce computational overhead and increase statistical power in downstream analyses. While it is possible to employ supervised learning approaches in experiments with labeled cells (via an input of sorted populations or other markers), most scRNA-seq experiments do not have a priori knowledge on the identity of the cells. Thus, this necessitates unsupervised learning approaches to identify informative features. Metrics such as variance, deviance, and dropout are often used and fitted against gene expression to select for highly informative genes relative to their expression level. 5.4.4 Imputation Imputation methods have been proposed to address the challenge of the large amount of zeros observed in data from scRNA-seq. In general, these methods rely on inherent structure in the dataset. However, extra care should be taken in applying these methods, as imputation has been shown to generative false signals and decrease the reproducibility of cell-type specific markers. 5.4.5 Dimensionality Reduction While feature selection can ameliorate the complexity of scRNA-seq to some extent, it is often insufficient to make many analyses tractable. Dimensionality reduction can thus be applied to create low-dimensional representations that nonetheless preserve meaningful structure. Principal components analysis (PCA) often serves as the first step, calculating a large number of components which are then trimmed down to those that explain a high amount of variance. Following this, the PCA results serve as the basis for other dimensionality reduction approaches that are often used to visualize the data in 2 or 3-dimensions, including t-SNE, UMAP, and diffusion maps. Alternative, more statistically oriented approaches such as the zero-inflated negative binomial (ZiNB) transform may also be used to produce dimensionality reduced representations that account for confounding factors. 5.4.6 Integrating Datasets In cases where its necessary to bring together multiple distinct scRNA-seq experiments, a new avenue of approaches specifically designed to solve this problem by leveraging the richness inherent to this high-dimensional data have emerged. These new approaches specifically bypass the assumption required by traditional statistical models that the composition of the populations is either known or identical across batches, thus improving the end result of integration. Integration approaches provide a new (potentially dimensionality reduced) representation of the data that allows for the identification of biologically similar cells between batches. This improves the performance of clustering, annotation, and the consequently the interpretability of 2- or 3-dimensional visualizations. 5.5 Downstream Statistical Analyses 5.5.1 Clustering 5.5.2 Differential Expression 5.5.3 Trajectory Analysis 5.5.4 Annotation 5.6 Accessible &amp; Reproducible Analysis 5.6.1 Interactive Data Visualization 5.6.2 Report Generation "],
["quick-start.html", "Chapter 6 Quick Start 6.1 Code 6.2 Visualizations 6.3 Session Info", " Chapter 6 Quick Start .aaron-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .aaron-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } To make it as easy as possible to get started fast, here we simply provide a script that walks through a typical, basic scRNA-seq analysis in code, with prose as comments (#), and all visualization held until the end of the script. The next chapter - “A Basic Analysis” - will provide more commentary on the various steps throughout, as well as relevant intermediate plotting results. Here, we use an example dataset from the Human Cell Atlas immune cell profiling project on bone marrow. This dataset is loaded via the HCAData package, which provides a ready to use SingleCellExperiment object. Note that the HCAData bone marrow dataset is comprised of 8 donors, so we have added an integration step to ameliorate batch effects caused by different donors. However, for use cases where integration is not necessary (e.g. no expected batch effects), we note in the code what to skip and relevant arguments to replace. Lastly, note that some arguments are added for the sake of reducing computational runtime and can be modified or removed. These include parallelization via BPPARAM, and different algorithms for SVD and nearest-neighbor via BSPARAM and BNPARAM. See the “Adaptations for Large-scale Data” chapter for more information on these arguments. 6.1 Code ## Setup --------------------------------------------------- ## not run - uncomment these lines to install necessary pkgs ## install.packages(&#39;BiocManager&#39;) ## BiocManager::install(version = &#39;devel&#39;) # devel=3.10 ## BiocManager::install(c( ## &#39;HCAData&#39;, # dataset ## &#39;scater&#39;, &#39;scran&#39;, &#39;batchelor&#39;, # processing ## &#39;igraph&#39;, # clustering ## &#39;MAST&#39;, &#39;slingshot&#39;, # DE + Trajectory ## &#39;iSEE&#39; # interactive viz ## )) ## Import data into R -------------------------------------- ## For reading in data directly from CellRanger output ## use the lines below and replace with proper paths to data ## append any cell metadata as needed to colData() ## library(DropletUtils) ## sce &lt;- read10xCounts(&#39;/path/to/cellranger/outs/&#39;) ## For this quick-start: Human Cell Atlas (HCA) data library(HCAData) sce &lt;- HCAData(&#39;ica_bone_marrow&#39;) ## subsample for better brevity of compilation set.seed(1234) sce &lt;- sce[, sample(ncol(sce), 10000)] ## Split out donor based on barcode Donor &lt;- lapply(sce$Barcode, strsplit, &#39;_&#39;) Donor &lt;- unlist(lapply(Donor, function(x) { x[[1]][1] })) sce$Donor &lt;- Donor ## Convert DelayedArray to regular matrix counts(sce) &lt;- as.matrix(counts(sce)) ## Quality Control ----------------------------------------- library(scater) sce &lt;- calculateQCMetrics(sce, BPPARAM = BiocParallel::MulticoreParam()) ## remove &quot;bad&quot; cells by total counts/features per cell filt &lt;- sce$total_counts &gt; 500 &amp; sce$total_features_by_counts &gt; 100 sce &lt;- sce[, filt] ## to ease computation, remove low frequency genes from `sce` num_reads &lt;- 1 # minimum 1 read num_cells &lt;- 0.025 * ncol(sce) # in at least 2.5% of all cells keep &lt;- rowSums(counts(sce) &gt;= num_reads) &gt;= num_cells sce &lt;- sce[keep, ] ## for readability, use Symbols in lieu of IDs as rownames uniq_feats &lt;- uniquifyFeatureNames(ID = rowData(sce)$ID, names = rowData(sce)$Symbol) rownames(sce) &lt;- uniq_feats ## Normalization ------------------------------------------- sce &lt;- normalize(sce) ## Feature Selection --------------------------------------- library(scran) fit &lt;- trendVar(sce, use.spikes = FALSE) dec &lt;- decomposeVar(sce, fit) hvg &lt;- rownames(dec)[dec$bio &gt; 0] # save gene names ## Integration --------------------------------------------- ## only perform this section if there is a batch effect library(batchelor) set.seed(1234) mnn &lt;- fastMNN(sce, batch = sce$Donor, subset.row = hvg, BSPARAM = BiocSingular::IrlbaParam(deferred = TRUE), BNPARAM = BiocNeighbors::AnnoyParam(), BPPARAM = BiocParallel::MulticoreParam()) reducedDim(sce, &#39;MNN&#39;) &lt;- reducedDim(mnn, &#39;corrected&#39;) ## Dimensionality Reduction -------------------------------- ## note on `use_dimred` arg: specifies which precomputed ## dimension reduction to use in `sce`; if there is none, ## it will first calculate and save PCA to `sce` then UMAP set.seed(1234) sce &lt;- runUMAP(sce, use_dimred = &#39;MNN&#39;, # omit if `fastMNN()` not run BNPARAM = BiocNeighbors::AnnoyParam(), BPPARAM = BiocParallel::MulticoreParam(), ## unnecessary options, only used to make a pretty graph min_dist = 0.5, repulsion_strength = 0.25, spread = 0.7, n_neighbors = 15) ## Clustering ---------------------------------------------- library(igraph) ## replace `use.dimred` with &#39;PCA&#39; if no integration was performed ## this will be automatically added via `runUMAP` above set.seed(1234) g &lt;- buildSNNGraph(sce, use.dimred = &#39;MNN&#39;, k = 30, # higher = bigger clusters BNPARAM = BiocNeighbors::AnnoyParam(), BPPARAM = BiocParallel::MulticoreParam()) clusters &lt;- as.factor(igraph::cluster_louvain(g)$membership) sce$clusters &lt;- clusters ## Differential Expression --------------------------------- ## pval.type = &#39;all&#39; : only get globally unique markers markers &lt;- findMarkers(sce, clusters = sce$clusters, block = sce$Donor, # use to get within-donor DE direction = &#39;up&#39;, lfc = 1.5, pval.type = &quot;all&quot;, # get cluster-unique markers subset.row = hvg, BPPARAM = BiocParallel::MulticoreParam()) ## pval.type = &#39;any&#39; : get all potential markers of any direction/comparison markers_any &lt;- findMarkers(sce, clusters = sce$clusters, block = sce$Donor, # use to get within-donor DE direction = &#39;any&#39;, lfc = 0, pval.type = &quot;any&quot;, # get all potential markers subset.row = hvg, BPPARAM = BiocParallel::MulticoreParam()) ## Annotation ---------------------------------------------- ## Get mappings of ENTREZID to Symbol library(org.Hs.eg.db) keys_entrez &lt;- keys(org.Hs.eg.db, &#39;ENTREZID&#39;) mapping_es &lt;- AnnotationDbi::select(org.Hs.eg.db, keys = keys_entrez, columns = c(&#39;ENTREZID&#39;, &#39;SYMBOL&#39;), keytype = &#39;ENTREZID&#39;) mapping_es$ENTREZID &lt;- as.integer(mapping_es$ENTREZID) ## Get pathways of interest - convert to list with symbol ## devtools::install_github(&#39;stephenturner/msigdf&#39;) library(msigdf) library(dplyr) mdb &lt;- dplyr::inner_join(msigdf.human, mapping_es, by = c(&#39;entrez&#39; = &#39;ENTREZID&#39;)) %&gt;% dplyr::filter(collection == &#39;c7&#39;) %&gt;% dplyr::select(-collection, -entrez) %&gt;% dplyr::group_nest(geneset) pathways &lt;- purrr::map(mdb$data, function(x) { as.character(x$SYMBOL) }) names(pathways) &lt;- mdb$geneset ## Get stats based on markers search - compare clusters 3 vs 2 stats &lt;- markers_any[[3]]$logFC.2 names(stats) &lt;- rownames(markers_any[[3]]) ## Run fast gene set enrichment analysis (see plot at bottom) library(fgsea) fgseaRes &lt;- fgsea(pathways = pathways, stats = stats, minSize = 15, maxSize = 500, nperm = 10000) ## Trajectory Analysis ------------------------------------- library(slingshot) slc &lt;- slingshot(sce[, sce$clusters %in% c(3, 5, 9)], clusterLabels = &#39;clusters&#39;, reducedDim = &#39;UMAP&#39;) ## Interactive Exploration --------------------------------- ## library(iSEE) ## iSEE(sce) ## not run; opens a web browser GUI 6.2 Visualizations ## Visualizations ------------------------------------------ ## todo: quality control: histogram of total counts ## todo: normalization: ??? ## todo: feature selection: variance v mean expression? ## ## PCA/UMAP: before and after batch correction ## UMAP (no colours) ## UMAP (with clusters colours) ## UMAP (with cell type labels) ## heatmap (top marker genes) ## trajectory plot (all or on subset of cells/within a clust) ## DE (volcano plot) ## Gene set enrichment plot ## Trajectory analysis ------------------------------------- ## Slingshot trajectory plot library(RColorBrewer) colors &lt;- colorRampPalette(brewer.pal(11, &#39;Spectral&#39;)[-6])(100) plotcol &lt;- colors[cut(slc$slingPseudotime_1, breaks = 100)] plot(reducedDims(slc)$UMAP, col = plotcol, pch=16, asp = 1) lines(SlingshotDataSet(slc), lwd = 2, col = &#39;black&#39;) ## UMAP based plots ---------------------------------------- ## UMAP (no colours) plotUMAP(sce) ## Pre vs post batch correction tmp &lt;- runPCA(sce, BSPARAM = BiocSingular::IrlbaParam(), BPPARAM = BiocParallel::MulticoreParam()) tmp &lt;- runUMAP(tmp, BNPARAM = BiocNeighbors::AnnoyParam(), BPPARAM = BiocParallel::MulticoreParam(), ## unnecessary options, only used to make a pretty graph min_dist = 0.5, repulsion_strength = 0.25, spread = 0.7, n_neighbors = 15) p0 &lt;- plotUMAP(tmp, colour_by = &#39;Donor&#39;) p1 &lt;- plotUMAP(sce, colour_by = &#39;Donor&#39;) patchwork::wrap_plots(p0, p1, nrow = 2) ## Gene expression on UMAP plots p2 &lt;- plotUMAP(sce, colour_by = &#39;CD3E&#39;) p3 &lt;- plotUMAP(sce, colour_by = &#39;CD79A&#39;) p4 &lt;- plotUMAP(sce, colour_by = &#39;LYZ&#39;) p5 &lt;- plotUMAP(sce, colour_by = &#39;NKG7&#39;) patchwork::wrap_plots(p2, p3, p4, p5, nrow = 2) ## Clusters on UMAP plotUMAP(sce, colour_by = &#39;clusters&#39;, text_by = &#39;clusters&#39;) ## Gene expression plots ----------------------------------- ## Heatmap: Top global markers per cluster top_markers &lt;- lapply(markers, function(x) { rownames(x)[1:20] }) top_markers &lt;- sort(unique(unlist(top_markers))) top_markers &lt;- top_markers[!grepl(&#39;MT-|^RP&#39;, top_markers)] set.seed(1234) plotHeatmap(sce[, sample(ncol(sce), 5000)], features = top_markers, color = viridis::viridis(101, option = &#39;A&#39;), ## symmetric = TRUE, zlim = c(-5, 5), colour_columns_by = &#39;clusters&#39;, clustering_method = &#39;ward.D2&#39;, show_colnames = FALSE, fontsize_row = 6 ) ## Volcano plot marker_tbl_3 &lt;- as.data.frame(markers_any[[3]]) marker_tbl_3 %&gt;% ggplot(aes(x = logFC.2, y = -log10(FDR))) + geom_point() + geom_vline(xintercept = c(-log(1.5), log(1.5)), linetype = &#39;dashed&#39;) + theme_classic() + coord_cartesian(xlim = c(-2, 2), expand = FALSE) ## Gene set enrichment ------------------------------------- ## Plot multiple pathways enrichment plots topPathwaysUp &lt;- fgseaRes[ES &gt; 0][head(order(pval), n=10), pathway] topPathwaysDown &lt;- fgseaRes[ES &lt; 0][head(order(pval), n=10), pathway] topPathways &lt;- c(topPathwaysUp, rev(topPathwaysDown)) plotGseaTable(pathways[topPathways], stats, fgseaRes, gseaParam = 0.5) ## Traditional GSEA plot plotEnrichment(pathways[[&quot;GSE29618_MONOCYTE_VS_PDC_UP&quot;]], stats) 6.3 Session Info ## &lt;button class=&quot;aaron-collapse&quot;&gt;View session info&lt;/button&gt; ## &lt;div class=&quot;aaron-content&quot;&gt; ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.6 LTS ## ## Matrix products: default ## BLAS/LAPACK: /app/easybuild/software/OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1/lib/libopenblas_prescottp-r0.2.18.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats4 parallel stats graphics grDevices utils datasets ## [8] methods base ## ## other attached packages: ## [1] RColorBrewer_1.1-2 slingshot_1.3.1 ## [3] princurve_2.1.4 fgsea_1.11.0 ## [5] Rcpp_1.0.1 dplyr_0.8.3 ## [7] msigdf_5.2 org.Hs.eg.db_3.8.2 ## [9] AnnotationDbi_1.47.0 igraph_1.2.4.1 ## [11] batchelor_1.1.4 scran_1.13.9 ## [13] scater_1.13.9 ggplot2_3.2.0 ## [15] rhdf5_2.29.0 HCAData_1.1.1 ## [17] SingleCellExperiment_1.5.3 SummarizedExperiment_1.15.5 ## [19] DelayedArray_0.11.4 BiocParallel_1.19.0 ## [21] matrixStats_0.54.0 Biobase_2.45.0 ## [23] GenomicRanges_1.37.14 GenomeInfoDb_1.21.1 ## [25] IRanges_2.19.10 S4Vectors_0.23.17 ## [27] BiocGenerics_0.31.5 BiocStyle_2.13.2 ## [29] Cairo_1.5-10 ## ## loaded via a namespace (and not attached): ## [1] ggbeeswarm_0.6.0 colorspace_1.4-1 ## [3] dynamicTreeCut_1.63-1 XVector_0.25.0 ## [5] BiocNeighbors_1.3.2 bit64_0.9-7 ## [7] interactiveDisplayBase_1.23.0 knitr_1.23 ## [9] zeallot_0.1.0 jsonlite_1.6 ## [11] dbplyr_1.4.2 shiny_1.3.2 ## [13] HDF5Array_1.13.4 BiocManager_1.30.4 ## [15] compiler_3.6.0 httr_1.4.0 ## [17] dqrng_0.2.1 backports_1.1.4 ## [19] assertthat_0.2.1 Matrix_1.2-17 ## [21] lazyeval_0.2.2 limma_3.41.6 ## [23] later_0.8.0 BiocSingular_1.1.5 ## [25] htmltools_0.3.6 tools_3.6.0 ## [27] rsvd_1.0.1 gtable_0.3.0 ## [29] glue_1.3.1 GenomeInfoDbData_1.2.1 ## [31] rappdirs_0.3.1 fastmatch_1.1-0 ## [33] vctrs_0.2.0 ape_5.3 ## [35] nlme_3.1-140 ExperimentHub_1.11.1 ## [37] crosstalk_1.0.0 DelayedMatrixStats_1.7.1 ## [39] xfun_0.8 stringr_1.4.0 ## [41] miniUI_0.1.1.1 mime_0.7 ## [43] irlba_2.3.3 statmod_1.4.32 ## [45] AnnotationHub_2.17.3 edgeR_3.27.6 ## [47] zlibbioc_1.31.0 scales_1.0.0 ## [49] promises_1.0.1 yaml_2.2.0 ## [51] curl_4.0 memoise_1.1.0 ## [53] gridExtra_2.3 stringi_1.4.3 ## [55] RSQLite_2.1.1 manipulateWidget_0.10.0 ## [57] rlang_0.4.0 pkgconfig_2.0.2 ## [59] bitops_1.0-6 rgl_0.100.26 ## [61] evaluate_0.14 lattice_0.20-38 ## [63] purrr_0.3.2 Rhdf5lib_1.7.2 ## [65] htmlwidgets_1.3 bit_1.1-14 ## [67] tidyselect_0.2.5 magrittr_1.5 ## [69] bookdown_0.12 R6_2.4.0 ## [71] DBI_1.0.0 pillar_1.4.2 ## [73] withr_2.1.2 RCurl_1.95-4.12 ## [75] tibble_2.1.3 crayon_1.3.4 ## [77] BiocFileCache_1.9.1 rmarkdown_1.14 ## [79] viridis_0.5.1 locfit_1.5-9.1 ## [81] grid_3.6.0 data.table_1.12.2 ## [83] blob_1.2.0 webshot_0.5.1 ## [85] digest_0.6.20 xtable_1.8-4 ## [87] httpuv_1.5.1 munsell_0.5.0 ## [89] beeswarm_0.2.3 viridisLite_0.3.0 ## [91] vipor_0.4.5 ## &lt;/div&gt; "],
["a-basic-analysis.html", "Chapter 7 A Basic Analysis 7.1 Preprocessing &amp; Import to R 7.2 Constructing the SingleCellExperiment 7.3 Data Processing 7.4 Downstream Statistical Analyses 7.5 Accessible &amp; Reproducible Analysis 7.6 Session Info", " Chapter 7 A Basic Analysis .aaron-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .aaron-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } In this chapter, we will walk through a minimal analysis of a simple scRNA-seq dataset in order to acquaint you with the overall framework of scRNA-seq in code terms. Where relevant within each part of this basic workflow, we will refer the interested reader to the associated chapter to learn advanced or alternate ways of performing a given task. Put another way, the workflow demonstrated in this chapter is written with the aim of simplicity, and thus will likely require nontrivial tweaking of parameters or alternate methods in real-world analyses. One note: in this workflow, we will be loading libraries as they become necessary to clearly link libraries to their respective functions, which usually runs counter to the norm of loading libraries first, at the top of the analysis script. 7.1 Preprocessing &amp; Import to R We will assume here that sequencing alignment and quantification of the data into a counts matrix, as well as the subsequent import to R has already been performed since this is highly platform- or technology-dependent. Note that for 10X Genomics data (which is used in this example workflow), the counts matrix and associated metadata (cell barcodes, data path, etc.) can be imported via the DropletUtils package’s read10xCounts() function. For data processed through Salmon/Alevin/Kallisto, we recommend checking out the tximport/tximeta Bioconductor packages. These are either imported as SingleCellExperiment or as a counts matrix which can be then coerced into a SingleCellExperiment object as demonstrated below. 7.2 Constructing the SingleCellExperiment 7.2.1 From Scratch Below we show an example of creating a SingleCellExperiment class object from a counts matrix and associated experimental metadata. library(SingleCellExperiment) ## More realistic: read in your experimental design metadata ## If its per cell metadata, make sure it lines up with your ## counts matrix row IDs correctly ## my_metadata &lt;- read.csv(&quot;my_metadata.csv&quot;) ## Example data ncells &lt;- 100 my_counts_matrix &lt;- matrix(rpois(20000, 5), ncol = ncells) my_metadata &lt;- data.frame(genotype = rep(c(&#39;A&#39;, &#39;B&#39;), each = 50), experiment_id = &#39;Experiment1&#39;) ## Construct the sce object manually sce &lt;- SingleCellExperiment(assays = list(counts = my_counts_matrix), colData = my_metadata) ## Manually adding a variable that is the same across all cells colData(sce) &lt;- cbind(colData(sce), date = &#39;2020-01-01&#39;) sce ## class: SingleCellExperiment ## dim: 200 100 ## metadata(0): ## assays(1): counts ## rownames: NULL ## rowData names(0): ## colnames: NULL ## colData names(3): genotype experiment_id date ## reducedDimNames(0): ## spikeNames(0): 7.2.2 From Publicly Available Data From here on out, we will be working with a small example dataset from the TENxPBMCData Bioconductor package which has already been packaged into a SingleCellExperiment class object: library(TENxPBMCData) sce &lt;- TENxPBMCData(&#39;pbmc3k&#39;) sce ## class: SingleCellExperiment ## dim: 32738 2700 ## metadata(0): ## assays(1): counts ## rownames(32738): ENSG00000243485 ENSG00000237613 ... ## ENSG00000215616 ENSG00000215611 ## rowData names(3): ENSEMBL_ID Symbol_TENx Symbol ## colnames: NULL ## colData names(11): Sample Barcode ... Individual Date_published ## reducedDimNames(0): ## spikeNames(0): One decision that should be made early on in the analysis is what row identifier to identify genes. Depending on how the data is imported, the rowData component may already have additional annotation information, such as multiple row mappings. For our new sce object from the pbmc3k dataset, we can take a look at rowData to see our options: rowData(sce) ## DataFrame with 32738 rows and 3 columns ## ENSEMBL_ID Symbol_TENx Symbol ## &lt;character&gt; &lt;character&gt; &lt;character&gt; ## ENSG00000243485 ENSG00000243485 MIR1302-10 NA ## ENSG00000237613 ENSG00000237613 FAM138A FAM138A ## ENSG00000186092 ENSG00000186092 OR4F5 OR4F5 ## ENSG00000238009 ENSG00000238009 RP11-34P13.7 LOC100996442 ## ENSG00000239945 ENSG00000239945 RP11-34P13.8 NA ## ... ... ... ... ## ENSG00000215635 ENSG00000215635 AC145205.1 NA ## ENSG00000268590 ENSG00000268590 BAGE5 NA ## ENSG00000251180 ENSG00000251180 CU459201.1 NA ## ENSG00000215616 ENSG00000215616 AC002321.2 NA ## ENSG00000215611 ENSG00000215611 AC002321.1 NA We see that we could choose between ENSEMBL_ID (the default), Symbol_TENx, and Symbol. For ease of readability and subsetting, we will utilize the Symbol_TENx identifier as our object’s rownames, making it possible to subset the sce with gene symbols as in sce[&quot;CD8A&quot;, ]. ## reassign rownames rownames(sce) &lt;- rowData(sce)[, &quot;Symbol_TENx&quot;] Now, while this seems to work just fine, eventually we may run into an issue because we actually have duplicated row names here. Depending on how a downstream function is coded, this may cause an esoteric error to pop-up. In fact, here we have about 100 duplicates. We can avoid future errors (and many headaches) by removing duplicates before any analysis: ## counts dupes from top to bottom to make a logical vector dupes &lt;- duplicated(rownames(sce)) sce &lt;- sce[!dupes, ] Keep in mind, the above is likely the most inelegant solution to the problem. Other methods could include, from the duplicated set of genes, choosing the one with the highest expression, aggregating the counts per cell, or keeping them all by adding an additional suffix to make the row names unique. Each has its own tradeoffs, so we leave this choice up to the diligent reader. And one more bit of preprocessing to prevent a potential downstream error is to assign our columns proper names. We can grab the barcodes of each cell from colData and assign them as column names as follows: colnames(sce) &lt;- sce$Barcode 7.3 Data Processing The aim of this section is to form the basis for more interesting downstream analyses. Thus, the objective here is to transform the data into a “clean” expression matrix that has been normalized and freed of technical artifacts, as well as a dimensionality reduction representation that can be used in subsequent analyses and visualization. 7.3.1 Quality Control Metrics The first step is to ensure that our dataset only contains viable cells, e.g. droplets that contain proper mRNA libraries. One way to do that is to use the popular “knee plot”, which shows the relationship between the log rank vs the log total counts, and then calculate where the “knee” of the plot is. We use the DropletUtils package to demonstrate this in our example PBMC dataset. library(DropletUtils) ## Calculate the rank vs total counts per cell br &lt;- barcodeRanks(counts(sce)) ## Create the knee plot plot(log10(br$rank), log10(br$total)) abline(h = log10(metadata(br)$knee)) Figure 7.1: Barcode rank (aka knee) plot showing log10-rank by log10-total counts relationship and calculated knee (horizontal line). ## Save the calculated knee from `barcodeRanks()` knee &lt;- log10(metadata(br)$knee) We see that the knee calculated via this method (horizontal line) is at 1740, or on the log scale, 3.2405. This can be used as a filter to remove cells that are likely to be empty droplets. Before we do that, we will finish calculating other quality control (QC) metrics via the scater package and show the results from the first three cells. library(scater) sce &lt;- calculateQCMetrics(sce) We can display some of the calculated QC metrics appended to the colData component - there are a number of other columns present, but for brevity will only show two pertinent ones. colData(sce)[1:3, c(&quot;log10_total_features_by_counts&quot;, &quot;log10_total_counts&quot;)] ## DataFrame with 3 rows and 2 columns ## log10_total_features_by_counts log10_total_counts ## &lt;numeric&gt; &lt;numeric&gt; ## AAACATACAACCAC-1 2.89 3.38 ## AAACATTGAGCTAC-1 3.13 3.69 ## AAACATTGATCAGC-1 3.05 3.5 We can further inspect these cells based on their total counts as well as vs the total features detected by counts (e.g. the number of genes that have nonzero counts). hist(sce$log10_total_counts, breaks = 100) abline(v = knee) Figure 7.2: Histogram of the log10 total counts with the calculated knee from above (vertical line). smoothScatter(sce$log10_total_counts, sce$log10_total_features_by_counts, nbin = 250) abline(v = knee) Figure 7.3: Smoothed scatter plot of the log10-total counts vs the log10-total features detected by counts with the calculated knee from above (vertical line). While there are various ways to filter cells, here we actually will not need to perform any filtering, as the data has already undergone a stringent quality control, and thus all the cells can be considered high quality. For the sake of completeness, we will demonstrate here - without evaluating - how to subset based on the previously calculated barcode ranks knee: ## not run sce &lt;- sce[, sce$log10_total_counts &gt; knee] 7.3.2 Normalizing Data Next up we will transform the primary data, the counts, into a (log) normalized version. In this section, we will use the scran package throughout. First however, we will need to calculate scaling factors per cell. This function relies on an initial “quick and dirty” clustering to get roughly similar pools of cells. These are used to generate pool-based estimates, from which the subsequent cell-based size factors are generated. To learn more about the method, see the ?computeSumFactors documentation. For now, we will perform a simpler normalization, using the library sizes per cell to create a log-normalized expression matrix: sce &lt;- scater::normalize(sce) We can see below that we now have two assays, counts and logcounts. assays(sce) ## List of length 2 ## names(2): counts logcounts 7.3.3 Feature Selection This section will use the scran package, as we select for informative genes by selecting for those with high coefficients of biological variation. Since this experiment does not have spike-ins, we will fit the mean-variance trend across the endogenous genes. library(scran) fit &lt;- trendVar(sce, use.spikes = FALSE) plot(fit$mean, fit$var) curve(fit$trend(x), col = &#39;red&#39;, lwd = 2, add = TRUE) Figure 7.4: Mean-variance trend line fit by scran package trendVar() function. We can see that the trend line goes through the central mass of genes, and thus continue on with looking at the decomposed variance. In this method, it is assumed that the total variance is the sum of the technical and biological variance, where the technical variance can be determined by interpolating the fitted trend at the mean log-count for that gene. Thus the biological variance is the total variance minus this interpolated (technical) variance. We can then rank and choose genes which have a biological coefficient of variance greater than zero. dec &lt;- decomposeVar(sce, fit) dec &lt;- dec[order(dec$bio, decreasing = TRUE), ] # order by bio var dec[1:5, ] ## mean total bio tech p.value FDR ## LYZ 1.629 3.925 3.116 0.809 0 0 ## S100A9 1.068 3.352 2.628 0.725 0 0 ## HLA-DRA 1.543 3.071 2.278 0.793 0 0 ## FTL 3.637 2.941 2.203 0.738 0 0 ## CD74 2.209 2.696 1.900 0.796 0 0 The total number of genes with biological variance greater than zero as 3770. Alternatively, we could use the p-value/FDR as a way to rank our genes, but do note the following (from the simpleSingleCell vignette: “Ranking based on p-value tends to prioritize HVGs that are more likely to be true positives but, at the same time, less likely to be interesting. This is because the ratio can be very large for HVGs that have very low total variance and do not contribute much to the cell-cell heterogeneity.” However we choose, we can save these highly variable genes and use them for subsequent analyses: hvg_genes &lt;- rownames(dec)[dec$bio &gt; 0] For the purpose of sharing and saving this list of genes, we can stash the result into the metadata component of our sce object as follows: metadata(sce)$hvg_genes &lt;- hvg_genes metadata(sce)$hvg_genes[1:10] ## [1] &quot;LYZ&quot; &quot;S100A9&quot; &quot;HLA-DRA&quot; &quot;FTL&quot; &quot;CD74&quot; &quot;CST3&quot; &quot;S100A8&quot; ## [8] &quot;TYROBP&quot; &quot;NKG7&quot; &quot;FTH1&quot; The metadata component can hold any object, as it is a list container. Any results that you’d like to keep are safe to store here, and a great way to save or share intermediate results that would otherwise be kept in separate objects. 7.3.4 Dimensionality Reduction We now can perform dimensionality reduction using our highly variable genes (hvg_genes) subset. To do this, we will first calculate the PCA representation via the runPCA() function from the scater package. We will calculate 50 components on our highly variable genes: sce &lt;- runPCA(sce, ncomponents = 50, feature_set = hvg_genes) The results of these calculations will be stored in the reducedDims component. This method saves the percent variance explained per component as an attribute, which can be accessed as follows, and subsequently plot the “elbow plot”: ## access the attribute where percentVar is saved in reducedDim pct_var_explained &lt;- attr(reducedDim(sce, &#39;PCA&#39;), &#39;percentVar&#39;) plot(pct_var_explained) # elbow plot To calculate a 2-dimensional representation of the data, we will use the top 20 components of our PCA result to compute the UMAP representation. sce &lt;- runUMAP(sce, use_dimred = &#39;PCA&#39;, n_dimred = 20) plotUMAP(sce) Figure 7.5: UMAP plot. With that, we have a canvas on which to paint our downstream analyses. 7.4 Downstream Statistical Analyses There are a plethora of potential downstream analyses to run, the choice of which is highly dependent on the biological objective. For this example dataset, our aim will be to identify the key cell types via a combination of clustering and differential expression. 7.4.1 Clustering Based on our earlier UMAP plot, it appears that we have a few distinct clusters. To do this computationally, we can utilize the scran package to: build a shared nearest neighbor (SNN) graph calculate based on the SNN graph the most representative clusters In this first step, we will specify that we will consider k nearest neighbors, and d dimensions from the PCA calculation as follows: set.seed(1234) # to make results reproducible snng &lt;- buildSNNGraph(sce, k = 50, d = 20) Following the graph construction, we can calculate the clusters using a variety of different graph-based methods from the igraph package. Here, we use the louvain method to determine our cell’s cluster memberships. snng_clusters &lt;- igraph::cluster_louvain(snng) We see that we have the following numbers of cells per cluster: table(snng_clusters$membership) ## ## 1 2 3 4 5 ## 687 350 556 528 579 To view this result graphically on the UMAP plot, we first assign the result to the colData component as a new column, and specify this as our color variable in the plotUMAP() function: colData(sce)$clusters &lt;- as.factor(snng_clusters$membership) plotUMAP(sce, colour_by = &#39;clusters&#39;) Figure 7.6: UMAP plot showing calculated clusters. Naturally, this result will change as we tweak the number of k neighbors to consider and with the specific clustering algorithm, but for now we will go onwards to find markers of each of our clusters. 7.4.2 Differential Expression In this section, we will look to identify genes that are unique to each of our clusters. To accomplish this, we will lean on the scran package to perform the analysis, and then the scater package to visualize the results. For this analysis, we will limit ourselves to a top subset of highly variable genes in our hvg_genes set, purely for the sake of computation time. Furthermore, we will limit our consideration to genes with an increased log fold-change of at least 1.5 versus other clusters. We will also use the BiocParallel package to parallelize the computation and speed up our processing via the BPPARAM argument. markers &lt;- findMarkers(sce, clusters = colData(sce)$clusters, subset.row = hvg_genes[1:250], lfc = 1.5, direction = &#39;up&#39;, log.p = TRUE, BPPARAM = BiocParallel::MulticoreParam()) We can view the top 5 markers that are differentially expressed (by our specified metrics): markers[[1]][1:5, ] ## Top log.p.value log.FDR logFC.2 logFC.3 logFC.4 logFC.5 ## CST3 1 -831.4 -825.9 3.446 3.470 3.413 3.480 ## TYROBP 1 -778.7 -773.8 3.325 3.324 2.691 3.301 ## LYZ 2 -526.9 -522.8 4.028 4.011 4.072 4.000 ## FTL 3 -667.3 -662.9 3.110 3.634 3.463 3.534 ## FTH1 4 -474.3 -470.6 2.771 3.118 3.100 2.755 We can see that CD3D, a marker of T cells, is one of our top differentially expressed genes in cluster 1. We can plot the expression of this gene across all our clusters as follows: plotExpression(sce, &#39;CD3D&#39;, x = &#39;clusters&#39;) Figure 7.7: Violin plots of CD3D expression across clusters. This plot highlights that CD3D is more highly expressed in cluster 1 relative to some of the other clusters, but not all. This can also be seen from our raw output above, where the log fold-change is calculated with respect to each cluster. There, we see that the log fold-change for CD3D is very high only relative to clusters 2 and 3 (meeting our cutoff of 1.5). 7.4.3 Annotation 7.4.3.1 A Manual Approach To finish off our the downstream analysis section here, we will look to annotate our clusters with a cell type designation, based on publicly available knowledge. Before we do that, let’s get a broader view of our top differentially expressed genes. To do this, we can iterate over the list-object returned by findMarkers to get the top 10 genes per cluster, and then plot these genes in a heatmap. ## grab the top 10 genes per cluster (e.g. within each list component) genes &lt;- lapply(markers, function(x) { rownames(x)[x$Top &lt;= 10] }) ## uniqify the set of genes returned, after coercing to a vector genes &lt;- unique(unlist(genes)) plotHeatmap(sce, genes, colour_columns_by = &quot;clusters&quot;, show_colnames = FALSE, clustering_method = &#39;ward.D2&#39;, fontsize_row = 6) Figure 7.8: Heatmap showing top differentially expressed genes across the clusters. Based on the heatmap output (and a priori knowledge), we can make some observations: CD79A/CD79B, markers of B cells, are uniquely and highly expressed in cluster 2 HLA genes, present on antigen presenting cells (APCs), are highly expressed across clusters 2 and 3 LYZ, a marker of dendritic cells (an APC), is highly expressed in cluster 3 Granzymes A and B (GZMA/GZMB), and NKG7, markers of cytotoxic cells such as CD8s and NK cells, are highly expressed within (a subset of cluster 4) CD3D/CD3E, markers of T cells, are expressed across clusters 5, 1, and 4 Finally, we can view a selection of the genes mentioned above on our previous UMAP plot: plotUMAP(sce, colour_by = &quot;CD79A&quot;) Figure 7.9: Various UMAP plots showing the expression of select cell-type specific genes. plotUMAP(sce, colour_by = &quot;LYZ&quot;) Figure 7.10: Various UMAP plots showing the expression of select cell-type specific genes. plotUMAP(sce, colour_by = &quot;NKG7&quot;) Figure 7.11: Various UMAP plots showing the expression of select cell-type specific genes. plotUMAP(sce, colour_by = &quot;CD3D&quot;) Figure 7.12: Various UMAP plots showing the expression of select cell-type specific genes. Combining the information derived from our heatmap and viewing these genes on our UMAP, we can come to the following conclusion: Cluster 2 is likely to be B cells Cluster 3 is likely to be dendritic cells Clusters 1, 5, 4 appear to represent a spectrum of cells with cytotoxic capabilities, likely composed of a combination of T cells and NK cells, Cluster 4 exhibits an strong NK cell signature on the basis of NKG7 Now that we’ve manually sorted our dataset on the basis of prior knowledge, let’s try a more automated approach using publicly available markers. 7.4.3.2 An Automated Approach Manually classifying cell types present in an scRNA-seq experiment can be prone to bias in terms of how a label is selected. Thus have emerged automated classification approaches which take a measured approach to the labeling of cell types. One such approach - cellassign - applies labels in a single-cell manner based on a gene by cell type “marker matrix”. Here, we utilize an existing gene by cell type annotation from a publication by Becht et al. (2016) which categorizes genes into cell types based on the specificity of their expression. Let’s first construct a marker matrix loosely inspired by the Seurat PBMC 3k tutorial: anno &lt;- data.frame( SYMBOL = c( &#39;IL7R&#39;, &#39;CCR7&#39;, &#39;CD4&#39;, &#39;CD3D&#39;, &#39;CD3E&#39;, &#39;CD14&#39;, &#39;LYZ&#39;, &#39;MS4A1&#39;, &#39;CD79A&#39;, &#39;CD79B&#39;, &#39;CD8A&#39;, &#39;CD8B&#39;, &#39;CD3D&#39;, &#39;CD3E&#39;, &#39;GNLY&#39;, &#39;NKG7&#39;, &#39;FCER1A&#39;, &#39;CST3&#39;, &#39;ITGAX&#39; ), cell_type = c( rep(&#39;CD4 T cell&#39;, 5), rep(&#39;Monocyte&#39;, 2), rep(&#39;B cell&#39;, 3), rep(&#39;CD8 T cell&#39;, 4), rep(&#39;NK cell&#39;, 2), rep(&#39;Dendritic cell&#39;, 3) ) ) Lastly, we’ll need to reformat this matrix to fit the expectations of cellassign, chiefly to convert the annotation into a binary matrix of genes (rows) by cell types (columns): ## construct rho (binary marker matrix) tmp &lt;- tidyr::spread(anno, cell_type, cell_type) rho &lt;- ifelse(is.na(tmp[, -1]), 0, 1) rownames(rho) &lt;- tmp$SYMBOL ## remove entries that are not present in our dataset rho &lt;- rho[rownames(rho) %in% rownames(sce), ] rho[1:3, ] ## B cell CD4 T cell CD8 T cell Dendritic cell Monocyte NK cell ## CCR7 0 1 0 0 0 0 ## CD14 0 0 0 0 1 0 ## CD3D 0 1 1 0 0 0 We can then run the cellassign method to produce cell type labels on a per cell basis: ## devtools::install_github(&#39;Irrationone/cellassign&#39;) library(cellassign) library(tensorflow) set.seed(1234) reticulate::py_set_seed(1234) fit &lt;- cellassign(sce[rownames(rho), ], marker_gene_info = rho, s = sizeFactors(sce)) ## 160 L old: -97042.4116434613; L new: -46422.4311352146; Difference (%): 0.521627396217512 ## 80 L old: -46422.4311352146; L new: -46072.5841608096; Difference (%): 0.00753616227866197 ## 60 L old: -46072.5841608096; L new: -45509.3585831869; Difference (%): 0.0122247446693432 ## 60 L old: -45509.3585831869; L new: -44726.5864190189; Difference (%): 0.0172002460271371 ## 60 L old: -44726.5864190189; L new: -44358.1679824796; Difference (%): 0.00823712395772292 ## 40 L old: -44358.1679824796; L new: -44281.9051353908; Difference (%): 0.00171925150558442 ## 40 L old: -44281.9051353908; L new: -44230.4961636357; Difference (%): 0.00116094760597745 ## 40 L old: -44230.4961636357; L new: -44181.1130183581; Difference (%): 0.00111649539482703 ## 40 L old: -44181.1130183581; L new: -44135.5164487203; Difference (%): 0.00103203759531462 ## 40 L old: -44135.5164487203; L new: -44101.5814927208; Difference (%): 0.000768880908845695 ## 40 L old: -44101.5814927208; L new: -44080.7744168154; Difference (%): 0.000471798860747732 ## 40 L old: -44080.7744168154; L new: -44067.3190301326; Difference (%): 0.00030524388150761 ## 40 L old: -44067.3190301326; L new: -44056.8289956957; Difference (%): 0.000238045668939741 ## 40 L old: -44056.8289956957; L new: -44047.3356402519; Difference (%): 0.000215479771470688 ## 20 L old: -44047.3356402519; L new: -44040.3851785381; Difference (%): 0.000157795281207311 ## 20 L old: -44040.3851785381; L new: -44032.980873052; Difference (%): 0.000168125357126047 ## 40 L old: -44032.980873052; L new: -44022.7713210755; Difference (%): 0.00023186147687762 ## 40 L old: -44022.7713210755; L new: -44011.9926758065; Difference (%): 0.000244842497314432 ## 40 L old: -44011.9926758065; L new: -43999.8964578935; Difference (%): 0.000274839133099664 ## 40 L old: -43999.8964578935; L new: -43986.1070954304; Difference (%): 0.000313395338926541 ## add cell type info into colData colData(sce)$cellassign_type &lt;- fit$cell_type ## plot the cellassign results on UMAP plotUMAP(sce, colour_by = &#39;cellassign_type&#39;) Figure 7.13: UMAP showing the results of automated label assignment as performed by cellassign. In practice, some combination of the above manual and automated classification schema will likely be necessary to properly annotate an scRNA-seq dataset. 7.5 Accessible &amp; Reproducible Analysis In collaborative settings, it is essential to share data and analyses. Thanks to the SingleCellExperiment class, most of if not all analysis steps performed can be recorded. These outputs are accessible through not only R, but also via graphical user interfaces as well that broaden the potential viewing audience. 7.5.1 Interactive Data Visualization Interactive exploration and visualization is a great way for collaborators to learn more about scRNA-seq data and analyses. In particular the iSEE package has been especially designed for viewing and sharing scRNA-seq. ## not run library(iSEE) iSEE(sce) Based on the example analyses, we task the interested reader to assess the previous section’s automatic annotation relative to the clustering results using iSEE. 7.6 Session Info ## &lt;button class=&quot;aaron-collapse&quot;&gt;View session info&lt;/button&gt; ## &lt;div class=&quot;aaron-content&quot;&gt; ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.6 LTS ## ## Matrix products: default ## BLAS/LAPACK: /app/easybuild/software/OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1/lib/libopenblas_prescottp-r0.2.18.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats4 parallel stats graphics grDevices utils datasets ## [8] methods base ## ## other attached packages: ## [1] tensorflow_1.13.1 cellassign_0.99.2 ## [3] scran_1.13.9 scater_1.13.9 ## [5] ggplot2_3.2.0 DropletUtils_1.5.4 ## [7] TENxPBMCData_1.3.0 HDF5Array_1.13.4 ## [9] rhdf5_2.29.0 SingleCellExperiment_1.5.3 ## [11] SummarizedExperiment_1.15.5 DelayedArray_0.11.4 ## [13] BiocParallel_1.19.0 matrixStats_0.54.0 ## [15] Biobase_2.45.0 GenomicRanges_1.37.14 ## [17] GenomeInfoDb_1.21.1 IRanges_2.19.10 ## [19] S4Vectors_0.23.17 BiocGenerics_0.31.5 ## [21] BiocStyle_2.13.2 Cairo_1.5-10 ## ## loaded via a namespace (and not attached): ## [1] bitops_1.0-6 bit64_0.9-7 ## [3] httr_1.4.0 dynamicTreeCut_1.63-1 ## [5] tools_3.6.0 backports_1.1.4 ## [7] R6_2.4.0 irlba_2.3.3 ## [9] vipor_0.4.5 DBI_1.0.0 ## [11] lazyeval_0.2.2 colorspace_1.4-1 ## [13] withr_2.1.2 gridExtra_2.3 ## [15] tidyselect_0.2.5 bit_1.1-14 ## [17] curl_4.0 compiler_3.6.0 ## [19] BiocNeighbors_1.3.2 bookdown_0.12 ## [21] scales_1.0.0 tfruns_1.4 ## [23] rappdirs_0.3.1 stringr_1.4.0 ## [25] digest_0.6.20 rmarkdown_1.14 ## [27] R.utils_2.9.0 XVector_0.25.0 ## [29] base64enc_0.1-3 pkgconfig_2.0.2 ## [31] htmltools_0.3.6 dbplyr_1.4.2 ## [33] limma_3.41.6 rlang_0.4.0 ## [35] RSQLite_2.1.1 DelayedMatrixStats_1.7.1 ## [37] shiny_1.3.2 jsonlite_1.6 ## [39] dplyr_0.8.3 R.oo_1.22.0 ## [41] RCurl_1.95-4.12 magrittr_1.5 ## [43] BiocSingular_1.1.5 GenomeInfoDbData_1.2.1 ## [45] Matrix_1.2-17 ggbeeswarm_0.6.0 ## [47] Rcpp_1.0.1 munsell_0.5.0 ## [49] Rhdf5lib_1.7.2 reticulate_1.12 ## [51] viridis_0.5.1 R.methodsS3_1.7.1 ## [53] whisker_0.3-2 stringi_1.4.3 ## [55] yaml_2.2.0 edgeR_3.27.6 ## [57] zlibbioc_1.31.0 BiocFileCache_1.9.1 ## [59] AnnotationHub_2.17.3 grid_3.6.0 ## [61] blob_1.2.0 promises_1.0.1 ## [63] dqrng_0.2.1 ExperimentHub_1.11.1 ## [65] crayon_1.3.4 lattice_0.20-38 ## [67] locfit_1.5-9.1 zeallot_0.1.0 ## [69] knitr_1.23 pillar_1.4.2 ## [71] igraph_1.2.4.1 glue_1.3.1 ## [73] evaluate_0.14 BiocManager_1.30.4 ## [75] vctrs_0.2.0 httpuv_1.5.1 ## [77] gtable_0.3.0 purrr_0.3.2 ## [79] assertthat_0.2.1 xfun_0.8 ## [81] rsvd_1.0.1 mime_0.7 ## [83] xtable_1.8-4 later_0.8.0 ## [85] viridisLite_0.3.0 tibble_2.1.3 ## [87] beeswarm_0.2.3 AnnotationDbi_1.47.0 ## [89] memoise_1.1.0 statmod_1.4.32 ## [91] interactiveDisplayBase_1.23.0 ## &lt;/div&gt; "],
["quality-control.html", "Chapter 8 Quality Control 8.1 Motivation 8.2 Choice of QC metrics 8.3 Identifying low-quality cells 8.4 Checking diagnostic plots 8.5 Droplet-specific procedures 8.6 Assumptions of outlier detection 8.7 Session Info", " Chapter 8 Quality Control .aaron-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .aaron-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } 8.1 Motivation Low-quality cells in scRNA-seq data can arise from a variety of sources such as cell damage during dissociation or failure in library preparation (e.g., during reverse transcription or PCR amplification). These usually manifest as “cells” with low total counts, few expressed genes and high mitochondrial or spike-in proportions. These low-quality cells are problematic as they can yield misleading results in downstream analyses. They can forming their own distinct cluster(s), complicating interpretation of the results. This can be most obviously driven by increased mitochondrial proportions or enrichment for nuclear RNAs after cell damage. However, very small libraries can also form their own clusters due to shifts in the mean expression upon log-transformation. They contain genes that appear to be strongly “upregulated”, again complicating interpretation. This is most problematic if some transcripts are present at constant levels in the ambient solution for all libraries. These counts will then be greatly inflated upon normalization for the small library sizes. They containing genes that appear to be strongly “downregulated” due to the loss of RNA upon cell damage. This seems most pronounced with ribosomal protein genes, though other cytoplasmic transcripts are likely to be affected. They distort the characterization of population heterogeneity during variance estimation or principal components analysis. The first few principal components will capture differences in quality rather than biology, reducing the effectiveness of dimensionality reduction. Similarly, genes with the largest variances will be driven by differences between low- and high-quality cells. To avoid - or at least mitigate - these problems, we need to remove these cells at the start of the analysis. This step is commonly referred to as quality control (QC) on the cells. (We will use “library” and “cell” rather interchangeably here, though the distinction will become important when dealing with droplet-based data.) We will demonstrate using a small scRNA-seq dataset from A. T. L. Lun et al. (2017), which contains SMART-seq2 data with spike-ins from two 96-well plates of 416B cells with and without induction of the CBFB-MYH11 oncogene. Importantly, the data is provided with no prior QC, allowing us to apply our own procedures as desired. View history ### loading ### library(scRNAseq) sce.416b &lt;- LunSpikeInData(which=&quot;416b&quot;) sce.416b ## class: SingleCellExperiment ## dim: 46703 192 ## metadata(0): ## assays(1): counts ## rownames(46703): ENSMUSG00000102693 ENSMUSG00000064842 ... SIRV7 ## CBFB-MYH11-mcherry ## rowData names(1): Length ## colnames(192): SLX-9555.N701_S502.C89V9ANXX.s_1.r_1 ## SLX-9555.N701_S503.C89V9ANXX.s_1.r_1 ... ## SLX-11312.N712_S508.H5H5YBBXX.s_8.r_1 ## SLX-11312.N712_S517.H5H5YBBXX.s_8.r_1 ## colData names(9): Source Name cell line ... spike-in addition ## block ## reducedDimNames(0): ## spikeNames(2): ERCC SIRV ## altExpNames(0): 8.2 Choice of QC metrics We use several common QC metrics to identify low-quality cells based on their expression profiles. These metrics are described below in terms of reads for SMART-seq2 data, but the same definitions apply to UMI data generated by other technologies like MARS-seq and droplet-based protocols. The library size is defined as the total sum of counts across all features, i.e., genes and spike-in transcripts. Cells with small library sizes are of low quality as the RNA has not been efficiently captured (i.e., converted into cDNA and amplified) during library preparation. The number of expressed features in each cell is defined as the number of features with non-zero counts for that cell. Any cell with very few expressed genes is likely to be of poor quality as the diverse transcript population has not been successfully captured. The proportion of reads mapped to spike-in transcripts is calculated relative to the library size for each cell. High proportions are indicative of poor-quality cells, where endogenous RNA has been lost during processing (e.g., due to partial cell lysis or RNA degradation during dissociation). The same amount of spike-in RNA to each cell, so an enrichment in spike-in counts is symptomatic of loss of endogenous RNA. In the absence of spike-in transcripts, the proportion of reads mapped to genes in the mitochondrial genome can be used. High proportions are indicative of poor-quality cells (Islam et al. 2014; Ilicic et al. 2016), possibly because of loss of cytoplasmic RNA from perforated cells. The reasoning is that mitochondria are larger than individual transcript molecules and less likely to escape through tears in the cell membrane. For each cell, we calculate these QC metrics using the calculateQCMetrics function from the scater package (McCarthy et al. 2017). These are stored in the column-wise metadata of the SingleCellExperiment for future reference. (Similar per-gene metrics are computed in the row-wise metadata, but we will not use them here.) # Identifying the mitochondrial transcripts: library(TxDb.Mmusculus.UCSC.mm10.ensGene) location &lt;- mapIds(TxDb.Mmusculus.UCSC.mm10.ensGene, keys=rownames(sce.416b), column=&quot;CDSCHROM&quot;, keytype=&quot;GENEID&quot;) is.mito &lt;- which(location==&quot;chrM&quot;) # Computing the QC metrics. library(scater) sce.416b &lt;- calculateQCMetrics(sce.416b, feature_controls=list(Mito=is.mito)) head(colnames(rowData(sce.416b))) ## [1] &quot;Length&quot; &quot;is_feature_control&quot; ## [3] &quot;is_feature_control_Mito&quot; &quot;is_feature_control_ERCC&quot; ## [5] &quot;is_feature_control_SIRV&quot; &quot;mean_counts&quot; head(colnames(colData(sce.416b))) ## [1] &quot;Source Name&quot; &quot;cell line&quot; ## [3] &quot;cell type&quot; &quot;single cell well quality&quot; ## [5] &quot;genotype&quot; &quot;phenotype&quot; 8.3 Identifying low-quality cells 8.3.1 With fixed thresholds The simplest approach to identifying low-quality cells is to apply thresholds on the QC metrics. For example, we might discard all cells that have library sizes below 100,000 reads; express fewer than 5,000 genes; have spike-in proportions above 10%; or have mitochondrial proportions above 5%1. qc.lib &lt;- sce.416b$total_counts &lt; 1e5 qc.nexprs &lt;- sce.416b$total_features_by_counts &lt; 5e3 qc.spike &lt;- sce.416b$pct_counts_ERCC &gt; 10 qc.mito &lt;- sce.416b$pct_counts_Mito &gt; 5 discard &lt;- qc.lib | qc.nexprs | qc.spike | qc.mito # Summarize the number of cells removed for each reason. DataFrame(LibSize=sum(qc.lib), NExprs=sum(qc.nexprs), SpikeProp=sum(qc.spike), MitoProp=sum(qc.mito), Total=sum(discard)) ## DataFrame with 1 row and 5 columns ## LibSize NExprs SpikeProp MitoProp Total ## &lt;integer&gt; &lt;integer&gt; &lt;integer&gt; &lt;integer&gt; &lt;integer&gt; ## 1 3 0 19 7 27 # Retain only high-quality cells in the SingleCellExperiment. filtered &lt;- sce.416b[,!discard] dim(filtered) ## [1] 46703 165 While simple, this strategy generally requires considerable experience to determine appropriate thresholds for each experimental protocol and biological system. For example, thresholds for read count-based data are simply not applicable for UMI-based data, and vice versa. Indeed, even with the same protocol and system, the appropriate threshold can vary from run to run due to the vagaries of cDNA capture efficiency and sequencing depth. 8.3.2 With adaptive thresholds To obtain an adaptive threshold, we assume that most of the dataset consists of high-quality cells. We then identify cells that are outliers for the various QC metrics. Outliers are defined based on the median absolute deviation (MADs) from the median value of each metric across all cells. This is loosely motivated by the fact that such a filter will retain 99% of non-outlier values that follow a Normal distribution. For the 416B data, we remove cells with log-library sizes that are more than 3 MADs below the median. We also remove cells where the log-transformed number of expressed genes is 3 MADs below the median. A log-transformation is used to improve resolution at small values when type=&quot;lower&quot;. It ensures that the threshold is not a negative value, which would be meaningless for quality control on these metrics. qc.lib2 &lt;- isOutlier(log(sce.416b$total_counts), nmads=3, type=&quot;lower&quot;) qc.nexprs2 &lt;- isOutlier(log(sce.416b$total_features_by_counts), nmads=3, type=&quot;lower&quot;) isOutlier() will also return the exact filter thresholds for each metric. These may be useful for checking whether the automatically selected thresholds are appropriate. attr(qc.lib2, &quot;thresholds&quot;) ## lower higher ## 13.08 Inf attr(qc.nexprs2, &quot;thresholds&quot;) ## lower higher ## 8.57 Inf We identify outliers for the proportion-based metrics in a similar manner. Here, no transformation is required as we are identifying large outliers that should already be clearly distinguishable from zero. qc.spike2 &lt;- isOutlier(sce.416b$pct_counts_ERCC, nmads=3, type=&quot;higher&quot;) qc.mito2 &lt;- isOutlier(sce.416b$pct_counts_ERCC, nmads=3, type=&quot;higher&quot;) We use all of these filters to identify and remove low-quality cells. discard2 &lt;- qc.lib2 | qc.nexprs2 | qc.spike2 | qc.mito2 # Summarize the number of cells removed for each reason. DataFrame(LibSize=sum(qc.lib2), NExprs=sum(qc.nexprs2), SpikeProp=sum(qc.spike2), MitoProp=sum(qc.mito2), Total=sum(discard2)) ## DataFrame with 1 row and 5 columns ## LibSize NExprs SpikeProp MitoProp Total ## &lt;integer&gt; &lt;integer&gt; &lt;integer&gt; &lt;integer&gt; &lt;integer&gt; ## 1 4 0 1 1 4 # Retain only high-quality cells in the SingleCellExperiment. filtered2 &lt;- sce.416b[,!discard2] With this strategy, the thresholds adapt to both the location and spread of the distribution of values for a given metric. This allows the QC procedure to adjust to changes in sequencing depth, cDNA capture efficiency, mitochondrial content, etc. without requiring any user intervention or prior experience. However, it does require some implicit assumptions that are discussed below in more detail. 8.3.3 Considering experimental factors More complex studies may involve batches of cells generated at different times or in different systems. In such cases, the adaptive strategy should be applied to each batch separately. It makes little sense to compute medians and MADs from a mixture distribution involving observations from multiple batches. For example, if the sequencing coverage is lower in one batch compared to the others, it will drag down the median and inflate the MAD. This will reduce the suitability of the adaptive threshold for the other batches. If each batch is represented by its own SingleCellExperiment, the isOutlier() function can be directly applied as shown above. However, if cells from all batches have been merged into a single SingleCellExperiment, the batch= argument should be used to ensure that outliers are identified within each batch. This allows isOutlier() to accommodate systematic differences in the QC metrics across batches. We will again illustrate using the 416B dataset, which actually contains two experimental factors - plate of origin and oncogene induction status. (We had been ignoring this in the previous section for simplicity, but no longer.) We combine these factors together and supply this to the batch= argument. This results in the removal of more cells as the MAD is no longer inflated by (i) systematic differences in sequencing depth between batches and (ii) differences in number of genes expressed upon oncogene induction. batch &lt;- paste0(sce.416b$phenotype, &quot;-&quot;, sce.416b$Plate) qc.lib3 &lt;- isOutlier(log(sce.416b$total_counts), nmads=3, type=&quot;lower&quot;, batch=batch) qc.nexprs3 &lt;- isOutlier(log(sce.416b$total_features_by_counts), nmads=3, type=&quot;lower&quot;, batch=batch) qc.spike3 &lt;- isOutlier(sce.416b$pct_counts_ERCC, nmads=3, type=&quot;higher&quot;, batch=batch) qc.mito3 &lt;- isOutlier(sce.416b$pct_counts_ERCC, nmads=3, type=&quot;higher&quot;, batch=batch) discard3 &lt;- qc.lib3 | qc.nexprs3 | qc.spike3 | qc.mito3 # Summarize the number of cells removed for each reason. DataFrame(LibSize=sum(qc.lib3), NExprs=sum(qc.nexprs3), SpikeProp=sum(qc.spike3), MitoProp=sum(qc.mito3), Total=sum(discard3)) ## DataFrame with 1 row and 5 columns ## LibSize NExprs SpikeProp MitoProp Total ## &lt;integer&gt; &lt;integer&gt; &lt;integer&gt; &lt;integer&gt; &lt;integer&gt; ## 1 4 2 4 4 7 # Retain only high-quality cells in the SingleCellExperiment. filtered3 &lt;- sce.416b[,!discard3] 8.4 Checking diagnostic plots TODO. 8.5 Droplet-specific procedures 8.5.1 Background An interesting aspect of droplet-based data is that we have no prior knowledge about whether a particular library (i.e., cell barcode) corresponds to cell-containing or empty droplets. Thus, we need to call cells from empty droplets based on the observed expression profiles. This is not entirely straightforward as empty droplets can contain ambient (i.e., extracellular) RNA that can be captured and sequenced, resulting in non-zero counts for libraries that do not contain any cell. To demonstrate, we obtain the unfiltered count matrix for the PBMC dataset from 10X Genomics. This has not been subjected to the cell filtering algorithm method in the CellRanger software suite, allowing us to apply our own algorithms for identifying the libraries that correspond to cells. # Hoping to shovel this code into TENxPBMCData. # Just bear with it for now. library(BiocFileCache) bfc &lt;- BiocFileCache(&quot;raw_data&quot;, ask = FALSE) raw.path &lt;- bfcrpath(bfc, file.path(&quot;http://cf.10xgenomics.com/samples&quot;, &quot;cell-exp/2.1.0/pbmc4k/pbmc4k_raw_gene_bc_matrices.tar.gz&quot;)) untar(raw.path, exdir=file.path(tempdir(), &quot;pbmc4k&quot;)) library(DropletUtils) library(Matrix) fname &lt;- file.path(tempdir(), &quot;pbmc4k/raw_gene_bc_matrices/GRCh38&quot;) sce.pbmc &lt;- read10xCounts(fname, col.names=TRUE) sce.pbmc ## class: SingleCellExperiment ## dim: 33694 737280 ## metadata(0): ## assays(1): counts ## rownames(33694): ENSG00000243485 ENSG00000237613 ... ## ENSG00000277475 ENSG00000268674 ## rowData names(2): ID Symbol ## colnames(737280): AAACCTGAGAAACCAT-1 AAACCTGAGAAACCGC-1 ... ## TTTGTCATCTTTAGTC-1 TTTGTCATCTTTCCTC-1 ## colData names(2): Sample Barcode ## reducedDimNames(0): ## spikeNames(0): ## altExpNames(0): The distribution of total counts exhibits a sharp transition between barcodes with large and small total counts (Figure 8.1), probably corresponding to cell-containing and empty droplets respectively. bcrank &lt;- barcodeRanks(counts(sce.pbmc)) # Only showing unique points for plotting speed. uniq &lt;- !duplicated(bcrank$rank) plot(bcrank$rank[uniq], bcrank$total[uniq], log=&quot;xy&quot;, xlab=&quot;Rank&quot;, ylab=&quot;Total UMI count&quot;, cex.lab=1.2) abline(h=metadata(bcrank)$inflection, col=&quot;darkgreen&quot;, lty=2) abline(h=metadata(bcrank)$knee, col=&quot;dodgerblue&quot;, lty=2) legend(&quot;bottomleft&quot;, legend=c(&quot;Inflection&quot;, &quot;Knee&quot;), col=c(&quot;darkgreen&quot;, &quot;dodgerblue&quot;), lty=2, cex=1.2) Figure 8.1: Total UMI count for each barcode in the PBMC dataset, plotted against its rank (in decreasing order of total counts). The inferred locations of the inflection and knee points are also shown. 8.5.2 Testing for empty droplets We use the emptyDrops() function to test whether the expression profile for each cell barcode is significantly different from the ambient RNA pool (Lun et al. 2018). Any significant deviation indicates that the barcode corresponds to a cell-containing droplet. We call cells at a false discovery rate (FDR) of 0.1%, meaning that no more than 0.1% of our called barcodes should be empty droplets on average. set.seed(100) e.out &lt;- emptyDrops(counts(sce.pbmc)) sum(e.out$FDR &lt;= 0.001, na.rm=TRUE) ## [1] 4233 emptyDrops() computes Monte Carlo \\(p\\)-values based on a Dirichlet-multinomial model of sampling molecules into droplets. These \\(p\\)-values are stochastic so it is important to set the random seed to obtain reproducible results. The stability of the Monte Carlo \\(p\\)-values depends on the number of iterations used to compute them. This is controlled using the niters= argument in emptyDrops(), set to a default of 10000 for speed. Larger values improve stability with the only cost being that of time, so users should set niters= to the largest value they are willing to wait for. emptyDrops() assumes that libraries with total UMI counts below a certain threshold (lower=100 by default) correspond to empty droplets. These are used to estimate the ambient expression profile against which the remaining libraries are tested. Under this definition, these low-count libraries cannot be cell-containing droplets and are excluded from the hypothesis testing - hence the NAs in the output of the function. To retain only the detected cells, we subset our SingleCellExperiment object. Discerning readers will notice the use of which(), which conveniently removes the NAs prior to the subsetting. # using which() to automatically remove NAs. sce.pbmc &lt;- sce.pbmc[,which(e.out$FDR &lt;= 0.001)] Users wanting to use the cell calling algorithm from the CellRanger pipeline (version 2) can call defaultDrops() instead. This tends to be quite conservative as it often discards genuine cells with low RNA content (and thus low total counts). It also requires an estimate of the expected number of cells in the experiment. 8.5.3 Examining cell-calling diagnostics The number of Monte Carlo iterations determines the lower bound for the \\(p\\)-values (Phipson and Smyth 2010). The Limited field in the output indicates whether or not the computed \\(p\\)-value for a particular barcode is bounded by the number of iterations. If any non-significant barcodes are TRUE for Limited, we may need to increase the number of iterations. A larger number of iterations will result in a lower \\(p\\)-value for these barcodes, which may allow them to be detected after correcting for multiple testing. table(Sig=e.out$FDR &lt;= 0.001, Limited=e.out$Limited) ## Limited ## Sig FALSE TRUE ## FALSE 1056 0 ## TRUE 1661 2572 As mentioned above, emptyDrops() assumes that barcodes with low total UMI counts are empty droplets. Thus, the null hypothesis should be true for all of these barcodes. We can check whether the hypothesis testing procedure holds its size by examining the distribution of \\(p\\)-values for low-total barcodes with test.ambient=TRUE. Ideally, the distribution should be close to uniform. full.data &lt;- read10xCounts(fname, col.names=TRUE) set.seed(100) limit &lt;- 100 all.out &lt;- emptyDrops(counts(full.data), lower=limit, test.ambient=TRUE) hist(all.out$PValue[all.out$Total &lt;= limit &amp; all.out$Total &gt; 0], xlab=&quot;P-value&quot;, main=&quot;&quot;, col=&quot;grey80&quot;) Figure 8.2: Distribution of \\(p\\)-values for the assumed empty droplets. Large peaks near zero indicate that barcodes with total counts below lower are not all ambient in origin. This can be resolved by decreasing lower further to ensure that barcodes corresponding to droplets with very small cells are not used to estimate the ambient profile. While emptyDrops() will distinguish cells from empty droplets, it makes no statement about the quality of the cells. It is entirely possible for droplets to contain damaged or dying cells, which need to be removed prior to downstream analysis. This can be done using the outlier-based strategy to set data-adaptive thresholds on the mitochondrial proportion. (emptyDrops() will already remove cells with low library sizes or number of expressed genes, so further filtering on these metrics is generally not necessary.) 8.6 Assumptions of outlier detection 8.6.1 Overview The use of isOutlier() is a general and effective strategy for identifying and removing low-quality cells. However, it involves a number of assumptions about the experiment and biological system, and while these assumptions are mostly reasonable, users should still keep them in mind when interpreting the results2. An outlier-based definition for low-quality cells assumes that most cells are of acceptable quality. This is usually a reasonable expectation and can be experimentally supported in some situations by visually checking that the cells are intact, e.g., on the microwell plate. If most cells are of (unacceptably) low quality, the adaptive thresholds will fail as they cannot remove the majority of cells. Of course, what is acceptable or not is in the eye of the beholder - neurons, for example, are notoriously difficult to dissociate, and we would often retain cells in a neuronal scRNA-seq dataset with QC metrics that would be unacceptable in a more amenable system like embryonic stem cells. Another assumption is that the QC metrics are independent of the biological state of each cell. This implies that any outlier values for these metrics are driven by technical factors rather than biological processes. Thus, removing cells based on the metrics will not misrepresent the biology in downstream analyses. This assumption is most likely to be violated in highly heterogeneous cell populations. For example, some cell types may naturally have less RNA or express fewer genes than other cell types. These cell types are more likely to be considered outliers and removed, even if they are of high quality. QC filtering based on outliers may not be appropriate in such cases. The use of the MAD mitigates this problem by accounting for biological variability in the QC metrics. A heterogeneous population should have higher variability in the metrics among high-quality cells, increasing the MAD and reducing the chance of incorrectly removing particular cell types (at the cost of reducing power to remove low-quality cells). Systematic differences in the QC metrics can be handled to some extent using the batch= argument in the isOutlier() function. However, this is not applicable in experiments where the relevant factors (e.g., cell type) are not known in advance. 8.6.2 Checking for discarded cell types The biggest practical concern with the assumptions is whether a cell type is lost during the QC procedure. We can diagnose this by looking for differences in gene expression between the discarded and retained cells. To demonstrate, we compute the average count across the discarded and retained pools in the 416B data set, and we compute the log-fold change between the averages. lost &lt;- calcAverage(counts(sce.416b)[,!discard]) kept &lt;- calcAverage(counts(sce.416b)[,discard]) library(edgeR) logged &lt;- cpm(cbind(lost, kept), log=TRUE, prior.count=2) logFC &lt;- logged[,1] - logged[,2] abundance &lt;- rowMeans(logged) If the discarded pool is enriched for a certain cell type, we should observe increased expression of the corresponding marker genes. No systematic upregulation of genes is apparent in the discarded pool in Figure 8.3, suggesting that the QC step did not inadvertently filter out a cell type in the 416B dataset. plot(abundance, logFC, xlab=&quot;Average count&quot;, ylab=&quot;Log-FC (lost/kept)&quot;, pch=16) is.spike &lt;- isSpike(sce.416b) points(abundance[is.spike], logFC[is.spike], col=&quot;red&quot;, pch=16) points(abundance[is.mito], logFC[is.mito], col=&quot;dodgerblue&quot;, pch=16) Figure 8.3: Log-fold change in expression in the discarded cells compared to the retained cells in the 416B dataset. Each point represents a gene, with spike-in and mitochondrial transcripts in red and blue respectively. For comparison, let’s pretend that we applied a fixed threshold on the library size to filter cells in the PBMC data set. Specifically, we remove all libraries with a library size below 500. alt.discard &lt;- colSums(counts(sce.pbmc)) &lt; 500 lost &lt;- calcAverage(counts(sce.pbmc)[,alt.discard]) kept &lt;- calcAverage(counts(sce.pbmc)[,!alt.discard]) logged &lt;- edgeR::cpm(cbind(lost, kept), log=TRUE, prior.count=2) logFC &lt;- logged[,1] - logged[,2] abundance &lt;- rowMeans(logged) The presence of a distinct population in the discarded pool manifests in Figure 8.4 as a a number of genes that are This includes PF4, PPBP and CAVIN2, which (spoiler alert!) indicates that there is a platelet population that has been discarded by alt.discard. plot(abundance, logFC, xlab=&quot;Average count&quot;, ylab=&quot;Log-FC (lost/kept)&quot;, pch=16) platelet &lt;- c(&quot;PF4&quot;, &quot;PPBP&quot;, &quot;CAVIN2&quot;) library(org.Hs.eg.db) ids &lt;- mapIds(org.Hs.eg.db, keys=platelet, column=&quot;ENSEMBL&quot;, keytype=&quot;SYMBOL&quot;) points(abundance[ids], logFC[ids], col=&quot;orange&quot;, pch=16) Figure 8.4: Average counts across all discarded and retained cells in the PBMC dataset, after using a more stringent filter on the total UMI count. Each point represents a gene, with platelet-related genes highlighted in orange. If we suspect that cell types have been incorrectly discarded by our QC procedure, the most direct solution is to relax the QC filters by increasing nmads= in the isOutlier() calls. We can also avoid filtering on metrics that are associated with genuine biological differences between cell types. As an aside, it is worth mentioning that the true technical quality of a cell may also be correlated with its type. (This differs from a correlation between the cell type and the QC metrics, as the latter are our imperfect proxies for quality.) This can arise if some cell types are not amenable to dissociation or microfluidics handling during the scRNA-seq protocol. In such cases, it is possible to “correctly” discard an entire cell type during QC if all of its cells are damaged. Indeed, concerns over the computational removal of cell types during QC are probably minor compared to losses in the experimental protocol. 8.7 Session Info ## &lt;button class=&quot;aaron-collapse&quot;&gt;View session info&lt;/button&gt; ## &lt;div class=&quot;aaron-content&quot;&gt; ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.6 LTS ## ## Matrix products: default ## BLAS/LAPACK: /app/easybuild/software/OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1/lib/libopenblas_prescottp-r0.2.18.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats4 parallel stats graphics grDevices utils datasets ## [8] methods base ## ## other attached packages: ## [1] org.Hs.eg.db_3.8.2 ## [2] edgeR_3.27.6 ## [3] limma_3.41.6 ## [4] Matrix_1.2-17 ## [5] DropletUtils_1.5.4 ## [6] BiocFileCache_1.9.1 ## [7] dbplyr_1.4.2 ## [8] scater_1.13.9 ## [9] ggplot2_3.2.0 ## [10] TxDb.Mmusculus.UCSC.mm10.ensGene_3.4.0 ## [11] GenomicFeatures_1.37.4 ## [12] AnnotationDbi_1.47.0 ## [13] SingleCellExperiment_1.5.3 ## [14] SummarizedExperiment_1.15.5 ## [15] DelayedArray_0.11.4 ## [16] BiocParallel_1.19.0 ## [17] matrixStats_0.54.0 ## [18] Biobase_2.45.0 ## [19] GenomicRanges_1.37.14 ## [20] GenomeInfoDb_1.21.1 ## [21] IRanges_2.19.10 ## [22] S4Vectors_0.23.17 ## [23] BiocGenerics_0.31.5 ## [24] BiocStyle_2.13.2 ## [25] Cairo_1.5-10 ## ## loaded via a namespace (and not attached): ## [1] bitops_1.0-6 bit64_0.9-7 ## [3] progress_1.2.2 httr_1.4.0 ## [5] tools_3.6.0 backports_1.1.4 ## [7] R6_2.4.0 irlba_2.3.3 ## [9] HDF5Array_1.13.4 vipor_0.4.5 ## [11] DBI_1.0.0 lazyeval_0.2.2 ## [13] colorspace_1.4-1 withr_2.1.2 ## [15] tidyselect_0.2.5 gridExtra_2.3 ## [17] prettyunits_1.0.2 bit_1.1-14 ## [19] curl_4.0 compiler_3.6.0 ## [21] BiocNeighbors_1.3.2 rtracklayer_1.45.1 ## [23] bookdown_0.12 scales_1.0.0 ## [25] askpass_1.1 rappdirs_0.3.1 ## [27] stringr_1.4.0 digest_0.6.20 ## [29] Rsamtools_2.1.3 R.utils_2.9.0 ## [31] rmarkdown_1.14 XVector_0.25.0 ## [33] pkgconfig_2.0.2 htmltools_0.3.6 ## [35] rlang_0.4.0 RSQLite_2.1.1 ## [37] DelayedMatrixStats_1.7.1 R.oo_1.22.0 ## [39] dplyr_0.8.3 RCurl_1.95-4.12 ## [41] magrittr_1.5 BiocSingular_1.1.5 ## [43] GenomeInfoDbData_1.2.1 Rhdf5lib_1.7.2 ## [45] Rcpp_1.0.1 ggbeeswarm_0.6.0 ## [47] munsell_0.5.0 viridis_0.5.1 ## [49] R.methodsS3_1.7.1 stringi_1.4.3 ## [51] yaml_2.2.0 zlibbioc_1.31.0 ## [53] rhdf5_2.29.0 grid_3.6.0 ## [55] blob_1.2.0 dqrng_0.2.1 ## [57] crayon_1.3.4 lattice_0.20-38 ## [59] Biostrings_2.53.1 hms_0.5.0 ## [61] locfit_1.5-9.1 zeallot_0.1.0 ## [63] knitr_1.23 pillar_1.4.2 ## [65] biomaRt_2.41.7 XML_3.98-1.20 ## [67] glue_1.3.1 evaluate_0.14 ## [69] BiocManager_1.30.4 vctrs_0.2.0 ## [71] gtable_0.3.0 openssl_1.4.1 ## [73] purrr_0.3.2 assertthat_0.2.1 ## [75] xfun_0.8 rsvd_1.0.1 ## [77] viridisLite_0.3.0 tibble_2.1.3 ## [79] GenomicAlignments_1.21.4 beeswarm_0.2.3 ## [81] memoise_1.1.0 ## &lt;/div&gt; References "],
["normalization.html", "Chapter 9 Normalization 9.1 Motivation 9.2 Setting up the data 9.3 Library size normalization 9.4 Normalization by deconvolution 9.5 Normalization by spike-ins 9.6 Putting it all together 9.7 Why (log-)transform? 9.8 Session Info", " Chapter 9 Normalization .aaron-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .aaron-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } 9.1 Motivation Systematic differences in coverage between libraries are often observed in single-cell RNA sequencing data. This typically arises from differences in cDNA capture or PCR amplification efficiency across cells, attributable to the difficulty of achieving consistent library preparation with minimal starting material3. Normalization aims to remove these systematic differences such that they do not interfere with comparisons of the expression profiles between cells, e.g., during clustering or differential expression analyses. At this point, it’s worth being clear on what we mean by “systematic differences”. For the purposes of this chaper, systematic differences refer to biases that affect all genes in a predictable manner. This includes, for example, a change in sequencing depth that scales the expected coverage of all genes by a certain factor. One can also consider more complex scaling effects, e.g., with respect to gene abundance, which would require non-linear normalization methods reminiscent of microarray4 analyses. In contrast, general batch correction methods aim to remove gene-specific differences between batches that may not follow any predictable pattern across genes, and thus will not be considered in this chapter. 9.2 Setting up the data To demonstrate a range of normalization strategies in this section, we will be using the Zeisel et al. (2015) dataset from the scRNAseq package. This dataset was generated using the STRT/C1 protocol and contains UMI count data for 3005 cells from the mouse brain. ERCC spike-in transcripts were also added to each cell. For simplicity, we will trust that sufficient quality control on the cells has already been performed by the original authors. library(scRNAseq) sce.zeisel &lt;- ZeiselBrainData() sce.zeisel ## class: SingleCellExperiment ## dim: 21135 3005 ## metadata(0): ## assays(1): counts ## rownames(21135): Tspan12 Tshz1 ... r_U4 r_tRNA-Ser-TCG ## rowData names(1): featureType ## colnames(3005): 1772071015_C02 1772071017_G12 ... 1772066098_A12 ## 1772058148_F03 ## colData names(10): tissue group # ... level1class level2class ## reducedDimNames(0): ## spikeNames(1): ERCC ## altExpNames(0): 9.3 Library size normalization Scaling normalization is the simplest and most commonly used class of normalization strategies. This involves dividing all counts for each cell by a cell-specific scaling factor, often called a “size factor”5. The assumption here is that any cell-specific bias (e.g., in capture or amplification efficiency) affects all genes equally via scaling of the expected mean count for that cell. The size factor represents the relative bias in each cell, so division of the counts by the size factor should remove that bias. The resulting “normalized expression values” can then be used for downstream analyses such as clustering and dimensionality reduction. Library size normalization is the simplest strategy for performing scaling normalization. We define the library size as the total sum of counts across all genes for each cell. The “library size factor” for each cell is then directly proportional to its library size. The proportionality constant is defined such that the mean size factor across all cells is equal to 1. This ensures that the normalized expression values are on the same scale as the original counts, which is useful for interpretation - especially when dealing with transformed data (see below). library(scater) lib.sf.zeisel &lt;- librarySizeFactors(sce.zeisel) summary(lib.sf.zeisel) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.146 0.646 0.899 1.000 1.260 3.738 In the Zeisel brain data, the library size factors differ by up to 10-fold across cells (Figure 9.1). This is typical of the variability in coverage in scRNA-seq data. hist(log10(lib.sf.zeisel), xlab=&quot;Log10[Size factor]&quot;, col=&#39;grey80&#39;) Figure 9.1: Distribution of size factors derived from the library size in the Zeisel brain dataset. Strictly speaking, the use of library size factors assumes that there is no “imbalance” in the differentially expressed (DE) genes between any pair of cells. That is, any upregulation for a subset of genes is cancelled out by the same magnitude of downregulation in a different subset of genes. This ensures that the library size is an unbiased estimate of the relative cell-specific bias6. (Otherwise, the estimate would be compromised by composition biases, as discussed in Robinson and Oshlack (2010).) This may not be true in scRNA-seq applications, which means that library size normalization may not yield accurate normalized expression values for downstream analyses. In practice, normalization accuracy is not a major consideration for exploratory scRNA-seq data analyses. Composition biases do not usually affect the separation of clusters, only the magnitude - and to a lesser extent, direction - of the log-fold changes between clusters or cell types. As such, library size normalization is usually sufficient in many applications where the aim is to identify clusters and the top markers that define each cluster. 9.4 Normalization by deconvolution As previously mentioned, composition biases will be present when any unbalanced differential expression exists between samples. Consider the simple example of two cells where a single gene \\(X\\) is upregulated in one cell \\(A\\) compared to the other cell \\(B\\). This upregulation means that either (i) more sequencing resources are devoted to \\(X\\) in \\(A\\), thus decreasing coverage of all other non-DE genes when the total library size of each cell is experimentally fixed (e.g., due to library quantification); or (ii) the library size of \\(A\\), increasing its library size factor and yielding smaller normalized expression values for all non-DE genes. In both cases, the net effect is that non-DE genes in \\(A\\) will incorrectly appear to be downregulated compared to \\(B\\). The removal of composition biases is a well-studied problem for bulk RNA sequencing data analysis. Normalization can be performed with the estimateSizeFactorsFromMatrix function in the DESeq2 package (Anders and Huber 2010; Love, Huber, and Anders 2014) or with the calcNormFactors function (Robinson and Oshlack 2010) in the edgeR package. These assume that most genes are not DE between cells. Any systematic difference in count size across the non-DE majority of genes between two cells is assumed to represent bias that is used to compute an appropriate size factor for its removal. However, single-cell data can be problematic for these bulk normalization methods due to the dominance of low and zero counts. To overcome this, we pool counts from many cells to increase the size of the counts for accurate size factor estimation (Lun, Bach, and Marioni 2016). Pool-based size factors are then “deconvolved” into cell-based factors for normalization of each cell’s expression profile. This is performed using the computeSumFactors() function from scran, as shown below. library(scran) set.seed(100) clust.zeisel &lt;- quickCluster(sce.zeisel) table(clust.zeisel) ## clust.zeisel ## 1 2 3 4 5 6 7 8 9 10 11 12 ## 380 162 172 116 426 194 423 252 267 253 166 194 deconv.sf.zeisel &lt;- computeSumFactors(sce.zeisel, cluster=clust.zeisel, sf.out=TRUE, min.mean=0.1) summary(deconv.sf.zeisel) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.126 0.461 0.817 1.000 1.335 4.677 The above code contains several points worth elaborating on: We use a pre-clustering step with quickCluster() where cells in each cluster are normalized separately and the size factors are rescaled to be comparable across clusters. This avoids the assumption that most genes are non-DE across the entire population - only a non-DE majority is required between pairs of clusters, which is a weaker assumption for highly heterogeneous populations. By default, quickCluster() will use an approximate algorithm for PCA based on methods from the irlba package. The approximation relies on stochastic initialization so we need to set the random seed (via set.seed()) for reproducibility. sf.out=TRUE simply instructs the function to return the size factors directly, rather than a SingleCellExperiment containing the size factors. In the latter case, the size factors can be easily extracted with the sizeFactors() function. The deconvolution approach will eventually fail if too many counts are zero, manifesting as nonsensical negative size factors. To avoid this, computeSumFactors() will automatically remove low-abundance genes with average counts below min.mean. For read count data, the default value of 1 is usually satisfactory, but UMI counts are naturally lower so we set min.mean=0.1. We see that the deconvolution size factors exhibit cell type-specific deviations from the library size factors in Figure ??. This is consistent with the presence of composition biases that are introduced by strong differential expression between cell types. Use of the deconvolution size factors adjusts for these biases to improve normalization accuracy for downstream applications. plot(lib.sf.zeisel, deconv.sf.zeisel, xlab=&quot;Library size factor&quot;, ylab=&quot;Deconvolution size factor&quot;, log=&#39;xy&#39;, pch=16, col=as.integer(factor(sce.zeisel$level1class))) abline(a=0, b=1, col=&quot;red&quot;) Figure 9.2: Deconvolution size factor for each cell in the Zeisel brain dataset, compared to the equivalent size factor derived from the library size. The red line corresponds to identity between the two size factors. Accurate normalization is most important for procedures that involve estimation and interpretation of per-gene statistics. For example, composition biases can compromise DE analyses by systematically shifting the log-fold changes in one direction or another. However, it tends to provide less benefit over simple library size normalization for cell-based analyses such as clustering. The presence of composition biases already implies strong differences in expression profiles, so changing the normalization strategy is unlikely to affect the outcome of a clustering procedure. 9.5 Normalization by spike-ins Spike-in normalization is based on the assumption that the same amount of spike-in RNA was added to each cell (A. T. L. Lun et al. 2017). Systematic differences in the coverage of the spike-in transcripts can only be due to cell-specific biases, e.g., in capture efficiency or sequencing depth. Scaling normalization is then applied to equalize spike-in coverage across cells. We refer to the corresponding scaling factors as “spike-in size factors”. Compared to the previous methods, spike-in normalization swaps one assumption for another; it requires no assumption about the biology of DE genes, but assumes that the spike-in transcripts were (i) added at a constant level to each cell, and (ii) respond to biases in the same relative manner as endogenous genes. Practically, spike-in normalization should be used if differences in the total RNA content of individual cells are of interest and must be preserved in downstream analyses. In any particular cell, an increase in the amount of endogenous RNA will not increase spike-in coverage. Thus, the former will not be represented as part of the bias in the latter, which means that the effects of total RNA content on expression will not be removed upon scaling. By comparison, the other normalization methods described above will simply interpret any change in total RNA content as part of the bias and remove it. We demonstrate the use of spike-in normalization on a different dataset involving mouse embryonic stem cells (mESCs) and mouse embryonic fibroblasts (MEFs) (Islam et al. 2011). (We will discuss the application of spike-in normalization to the Zeisel brain data shortly.) library(BiocFileCache) bfc &lt;- BiocFileCache(&quot;raw_data&quot;, ask=FALSE) islam.fname &lt;- bfcrpath(bfc, file.path(&quot;ftp://ftp.ncbi.nlm.nih.gov/geo/series&quot;, &quot;GSE29nnn/GSE29087/suppl/GSE29087_L139_expression_tab.txt.gz&quot;)) counts &lt;- read.table(islam.fname, colClasses=c(list(&quot;character&quot;, NULL, NULL, NULL, NULL, NULL, NULL), rep(&quot;integer&quot;, 96)), skip=6, sep=&#39;\\t&#39;, row.names=1) is.spike &lt;- grep(&quot;SPIKE&quot;, rownames(counts)) sce.islam &lt;- SingleCellExperiment(list(counts=as.matrix(counts))) isSpike(sce.islam, &quot;spike&quot;) &lt;- is.spike sce.islam$grouping &lt;- rep(c(&quot;mESC&quot;, &quot;MEF&quot;, &quot;Neg&quot;), c(48, 44, 4)) sce.islam ## class: SingleCellExperiment ## dim: 22936 96 ## metadata(0): ## assays(1): counts ## rownames(22936): RNA_SPIKE_1 RNA_SPIKE_2 ... r_U14 r_(CGTAG)n ## rowData names(0): ## colnames(96): V8 V9 ... V102 V103 ## colData names(1): grouping ## reducedDimNames(0): ## spikeNames(1): spike ## altExpNames(0): We apply the computeSpikeFactors() method to estimate size factors for all cells. This method computes the total count over all spike-in transcripts in each cell, and calculates size factors to equalize the total spike-in count across cells. It assumes that the relevant rows of the SingleCellExperiment have been marked as spike-ins with the isSpike()&lt;- function. Again, we set sf.out=TRUE to return the spike-in size factors directly for simplicity. spike.sf.islam &lt;- computeSpikeFactors(sce.islam, sf.out=TRUE) We observe a negative correlation between the two sets of size factors (Figure 9.3). This is because MEFs contain more endogenous RNA, which reduces the relative spike-in coverage in each library (thereby decreasing the spike-in size factors) but increases the coverage of endogenous genes (thus increasing the library size factors). If the spike-in size factors were applied to the counts, the expression values in MEFs would be scaled up while expression in mESCs would be scaled down. However, the opposite would occur if library size factors were used. lib.sf.islam &lt;- librarySizeFactors(sce.islam) colours &lt;- c(mESC=&quot;red&quot;, MEF=&quot;grey&quot;) plot(lib.sf.islam, spike.sf.islam, col=colours[sce.islam$grouping], pch=16, log=&quot;xy&quot;, xlab=&quot;Library size factor&quot;, ylab=&quot;Spike-in size factor&quot;) legend(&quot;bottomleft&quot;, col=colours, legend=names(colours), pch=16) Figure 9.3: Size factors from spike-in normalization, plotted against the library size factors for all cells in the mESC/MEF dataset. Each point is a cells, coloured according to its type. Whether or not total RNA content is relevant – and thus, the choice of normalization strategy – depends on the biological hypothesis. In most cases, changes in total RNA content are not interesting and can be normalized out by applying the library size or deconvolution factors. However, this may not always be appropriate if differences in total RNA are associated with a biological process of interest, e.g., cell cycle activity or T cell activation (Richard et al. 2018). Spike-in normalization will preserve these differences such that any changes in expression between biological groups have the correct sign. However! Regardless of whether we care about total RNA content, it is critical that the spike-in transcripts are normalized using the spike-in size factors. Size factors computed from the counts for endogenous genes should not be applied to the spike-in transcripts, precisely because the former captures differences in total RNA content that are not experienced by the latter. Attempting to normalize the spike-in counts with the gene-based size factors will lead to over-normalization and incorrect quantification. Thus, whenever spike-in data is present, we must compute a separate set of size factors for the spike-in set. This is discussed below for the Zeisel dataset. 9.6 Putting it all together We calculate all necessary size factors as shown below. Each of the compute*Factors() functions now returns a SingleCellExperiment containing the computed size factors. We set general.use=FALSE in computeSpikeFactors() to indicate that the spike-in size factors are only to be used for the spike-in transcripts and not for all genes in sce.zeisel. set.seed(100) clust.zeisel &lt;- quickCluster(sce.zeisel) sce.zeisel &lt;- computeSumFactors(sce.zeisel, cluster=clust.zeisel, min.mean=0.1) sce.zeisel &lt;- computeSpikeFactors(sce.zeisel, general.use=FALSE) Inspection of the SingleCellExperiment indicates that, indeed, two different sets of spike-ins have been added. summary(sizeFactors(sce.zeisel)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.126 0.461 0.817 1.000 1.335 4.677 summary(sizeFactors(sce.zeisel, &quot;ERCC&quot;)) # for spike-ins. ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.157 0.770 0.984 1.000 1.181 3.417 We then use the normalize() function from scater to compute normalized expression values for each cell. This is done by dividing the count for each gene/spike-in transcript with the appropriate size factor for that cell. The function also log-transforms the normalized values, creating a new assay called &quot;logcounts&quot;7. These log-values will be the basis of our downstream analyses in the following chapters. sce.zeisel &lt;- normalize(sce.zeisel) assayNames(sce.zeisel) ## [1] &quot;counts&quot; &quot;logcounts&quot; 9.7 Why (log-)transform? The log-transformation is useful as differences in the log-values represent log-fold changes in expression. This is important in downstream procedures based on Euclidean distances, which includes many forms of clustering of dimensionality reduction. By operating on log-transformed data, we ensure that these procedures are measuring distances between cells based on log-fold changes in expression. Or in other words, which is more interesting - a gene that is expressed at an average count of 50 in cell type \\(A\\) and 10 in cell type \\(B\\), or a gene that is expressed at an average count of 1100 in \\(A\\) and 1000 in \\(B\\)? Log-transformation focuses on the former by promoting contributions from genes with strong relative differences. When log-transforming, we need to consider the pseudo-count that is added to avoid undefined values at zero. Larger pseudo-counts will effectively shrink the log-fold changes between cells towards zero for low-abundance genes, meaning that downstream analyses will be driven more by differences in expression for high-abundance genes. Conversely, smaller pseudo-counts will increase the contribution of low-abundance genes. Common practice is to use a pseudo-count of 1, for the simple pragmatic reason8 that it preserves sparsity in the original matrix (i.e., zeroes in the input remain zeroes after transformation). This works well in all but the most pathological scenarios (???). (Incidentally, the addition of the pseudo-count is the motivation for the centering of the size factors at unity. This ensures that both the pseudo-count and the normalized expression values are on the same scale. A pseudo-count of 1 can then be interpreted as an extra read or UMI for each gene. In practical terms, this means that the shrinkage effect of the pseudo-count diminishes as sequencing depth improves. This is the correct behavior as it allows log-fold change estimates to become increasingly accurate with deeper coverage. In contrast, if we had simply divided each cell’s counts by its library size before log-transformation, accuracy of the log-fold changes would never improve regardless of how much additional sequencing we performed.) Of course, log-transformation is not the only possible transformation. More sophisticated approaches can be used such as dedicated variance stabilizing transformations (e.g., from the DESeq2 or sctransform packages), which out-perform the log-transformation for removal of the mean-variance trend. In practice, though, the log-transformation is a good default choice due its simplicity (a.k.a., reliability9, predictability and computational efficiency) and interpretability. 9.8 Session Info ## &lt;button class=&quot;aaron-collapse&quot;&gt;View session info&lt;/button&gt; ## &lt;div class=&quot;aaron-content&quot;&gt; ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.6 LTS ## ## Matrix products: default ## BLAS/LAPACK: /app/easybuild/software/OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1/lib/libopenblas_prescottp-r0.2.18.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats4 parallel stats graphics grDevices utils datasets ## [8] methods base ## ## other attached packages: ## [1] BiocFileCache_1.9.1 dbplyr_1.4.2 ## [3] scran_1.13.9 scater_1.13.9 ## [5] ggplot2_3.2.0 scRNAseq_1.11.8 ## [7] SingleCellExperiment_1.5.3 SummarizedExperiment_1.15.5 ## [9] DelayedArray_0.11.4 BiocParallel_1.19.0 ## [11] matrixStats_0.54.0 Biobase_2.45.0 ## [13] GenomicRanges_1.37.14 GenomeInfoDb_1.21.1 ## [15] IRanges_2.19.10 S4Vectors_0.23.17 ## [17] BiocGenerics_0.31.5 BiocStyle_2.13.2 ## [19] Cairo_1.5-10 ## ## loaded via a namespace (and not attached): ## [1] bitops_1.0-6 bit64_0.9-7 ## [3] httr_1.4.0 dynamicTreeCut_1.63-1 ## [5] tools_3.6.0 backports_1.1.4 ## [7] R6_2.4.0 irlba_2.3.3 ## [9] vipor_0.4.5 DBI_1.0.0 ## [11] lazyeval_0.2.2 colorspace_1.4-1 ## [13] withr_2.1.2 tidyselect_0.2.5 ## [15] gridExtra_2.3 bit_1.1-14 ## [17] curl_4.0 compiler_3.6.0 ## [19] BiocNeighbors_1.3.2 bookdown_0.12 ## [21] scales_1.0.0 rappdirs_0.3.1 ## [23] stringr_1.4.0 digest_0.6.20 ## [25] rmarkdown_1.14 XVector_0.25.0 ## [27] pkgconfig_2.0.2 htmltools_0.3.6 ## [29] limma_3.41.6 rlang_0.4.0 ## [31] RSQLite_2.1.1 shiny_1.3.2 ## [33] DelayedMatrixStats_1.7.1 dplyr_0.8.3 ## [35] RCurl_1.95-4.12 magrittr_1.5 ## [37] BiocSingular_1.1.5 GenomeInfoDbData_1.2.1 ## [39] Matrix_1.2-17 Rcpp_1.0.1 ## [41] ggbeeswarm_0.6.0 munsell_0.5.0 ## [43] viridis_0.5.1 stringi_1.4.3 ## [45] yaml_2.2.0 edgeR_3.27.6 ## [47] zlibbioc_1.31.0 AnnotationHub_2.17.3 ## [49] grid_3.6.0 blob_1.2.0 ## [51] promises_1.0.1 dqrng_0.2.1 ## [53] ExperimentHub_1.11.1 crayon_1.3.4 ## [55] lattice_0.20-38 locfit_1.5-9.1 ## [57] zeallot_0.1.0 knitr_1.23 ## [59] pillar_1.4.2 igraph_1.2.4.1 ## [61] glue_1.3.1 evaluate_0.14 ## [63] BiocManager_1.30.4 vctrs_0.2.0 ## [65] httpuv_1.5.1 gtable_0.3.0 ## [67] purrr_0.3.2 assertthat_0.2.1 ## [69] xfun_0.8 rsvd_1.0.1 ## [71] mime_0.7 xtable_1.8-4 ## [73] later_0.8.0 viridisLite_0.3.0 ## [75] tibble_2.1.3 AnnotationDbi_1.47.0 ## [77] beeswarm_0.2.3 memoise_1.1.0 ## [79] statmod_1.4.32 interactiveDisplayBase_1.23.0 ## &lt;/div&gt; References "],
["clustering-2.html", "Chapter 10 Clustering 10.1 Motivation 10.2 Comments on Truth 10.3 Graph-based clustering 10.4 \\(k\\)-means clustering 10.5 Hierarchical clustering 10.6 Session Info", " Chapter 10 Clustering .aaron-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .aaron-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } 10.1 Motivation Clustering is an unsupervised learning procedure that is used in scRNA-seq data analysis to empirically define groups of cells with similar expression profiles. Its primary purpose is to summarize the data in a digestible format for human interpretation. This allows us to describe population heterogeneity in terms of discrete labels that are easily understood, rather than attempting to comprehend the high-dimensional manifold on which the cells truly reside. After annotation based on marker genes, the clusters can be treated as proxies for more abstract biological concepts such as cell types or states. Clustering is thus a critical step for extracting biological insights from scRNA-seq data. Here, we demonstrate the application of several commonly used methods with the 10X PBMC dataset. View history ### loading ### library(BiocFileCache) bfc &lt;- BiocFileCache(&quot;raw_data&quot;, ask = FALSE) raw.path &lt;- bfcrpath(bfc, file.path(&quot;http://cf.10xgenomics.com/samples&quot;, &quot;cell-exp/2.1.0/pbmc4k/pbmc4k_raw_gene_bc_matrices.tar.gz&quot;)) untar(raw.path, exdir=file.path(tempdir(), &quot;pbmc4k&quot;)) library(DropletUtils) fname &lt;- file.path(tempdir(), &quot;pbmc4k/raw_gene_bc_matrices/GRCh38&quot;) sce.pbmc &lt;- read10xCounts(fname, col.names=TRUE) ### gene-annotation ### library(scater) rownames(sce.pbmc) &lt;- uniquifyFeatureNames(rowData(sce.pbmc)$ID, rowData(sce.pbmc)$Symbol) library(EnsDb.Hsapiens.v86) location &lt;- mapIds(EnsDb.Hsapiens.v86, keys=rowData(sce.pbmc)$ID, column=&quot;SEQNAME&quot;, keytype=&quot;GENEID&quot;) ### cell-detection ### set.seed(100) e.out &lt;- emptyDrops(counts(sce.pbmc)) sce.pbmc &lt;- sce.pbmc[,which(e.out$FDR &lt;= 0.001)] ### quality-control ### sce.pbmc &lt;- calculateQCMetrics(sce.pbmc, feature_controls=list(Mito=which(location==&quot;MT&quot;))) high.mito &lt;- isOutlier(sce.pbmc$pct_counts_Mito, nmads=3, type=&quot;higher&quot;) sce.pbmc &lt;- sce.pbmc[,!high.mito] ### normalization ### library(scran) set.seed(1000) clusters &lt;- quickCluster(sce.pbmc) sce.pbmc &lt;- computeSumFactors(sce.pbmc, min.mean=0.1, cluster=clusters) sce.pbmc &lt;- normalize(sce.pbmc) ### feature-selection ### fit.pbmc &lt;- trendVar(sce.pbmc, use.spikes=FALSE) dec.pbmc &lt;- decomposeVar(fit=fit.pbmc) o &lt;- order(dec.pbmc$bio, decreasing=TRUE) chosen.hvgs &lt;- rownames(dec.pbmc)[head(o, 2000)] ### dimensionality-reduction ### set.seed(10000) sce.pbmc &lt;- runPCA(sce.pbmc, feature_set=chosen.hvgs, ncomponents=25, BSPARAM=BiocSingular::IrlbaParam()) set.seed(100000) sce.pbmc &lt;- runTSNE(sce.pbmc, use_dimred=&quot;PCA&quot;) set.seed(1000000) sce.pbmc &lt;- runUMAP(sce.pbmc, use_dimred=&quot;PCA&quot;) sce.pbmc ## class: SingleCellExperiment ## dim: 33694 3922 ## metadata(1): log.exprs.offset ## assays(2): counts logcounts ## rownames(33694): RP11-34P13.3 FAM138A ... AC213203.1 FAM231B ## rowData names(10): ID Symbol ... total_counts log10_total_counts ## colnames(3922): AAACCTGAGAAGGCCT-1 AAACCTGAGACAGACC-1 ... ## TTTGTCACAGGTCCAC-1 TTTGTCATCCCAAGAT-1 ## colData names(38): Sample Barcode ... ## pct_counts_in_top_200_features_Mito ## pct_counts_in_top_500_features_Mito ## reducedDimNames(3): PCA TSNE UMAP ## spikeNames(0): ## altExpNames(0): 10.2 Comments on Truth At this point10, it is worth stressing the distinction between clusters and cell types. The former is an empirical construct while the latter is a biological truth (albeit a vaguely defined one). For this reason, questions like “what is the true number of clusters?” are usually meaningless. We can define as many clusters as we like, with whatever algorithm we like - each clustering will represent its own partitioning of the high-dimensional expression space, and is as “real” as any other clustering. A more relevant question is “how well do the clusters approximate the cell types?” Unfortunately, this is difficult to answer given the context-dependent interpretation of biological truth. Some analysts will be satisfied with resolution of the major cell types; other analysts may want resolution of subtypes; and others still may require resolution of different states (e.g., metabolic activity, stress) within those subtypes. Two clusterings can also be highly inconsistent yet both valid, simply partitioning the cells based on different aspects of biology. Indeed, asking for an unqualified “best” clustering is akin to asking for the best magnification on a microscope without any context. It is helpful to realize that clustering, like a microscope, is simply a tool to explore the data. We can zoom in and out by changing the resolution of the clustering parameters, and we can experiment with different clustering algorithms to obtain alternative perspectives of the data. This iterative approach is entirely permissible for data exploration, which constitutes the majority of all scRNA-seq data analysis. Now, if we were operating in a formal hypothesis testing framework, this parameter fiddling would constitute data dredging and be penalized by the multiple testing correction - but we can afford to be more relaxed for exploratory data analysis, given that we did not have a firm quantitative hypothesis in the first place11! 10.3 Graph-based clustering 10.3.1 Background Popularized by its use in Seurat, graph-based clustering is a flexible and scalable technique for clustering large scRNA-seq datasets. We first build a graph where each node is a cell that is connected to its nearest neighbours in the high-dimensional space. Edges are weighted based on the similarity between the cells involved, with higher weight given to cells that are more closely related. We then apply algorithms to identify “communities” of cells that are more connected to cells in the same community than they are to cells of different communities. Each community represents a cluster that we can use for downstream interpretation. The major advantage of graph-based clustering lies in its scalability. It only requires a \\(k\\)-nearest neighbor search that can be done in log-linear time (on average), in contrast to hierachical clustering methods that are quadratic with respect to the number of cells. Graph construction avoids making strong assumptions about the shape of the clusters or the distribution of cells within each cluster, compared to other methods like \\(k\\)-means (that favor spherical clusters) or Gaussian mixture models (that require normality). From a practical perspective, each cell is forcibly connected to a minimum number of neighboring cells, which reduces the risk of generating many uninformative clusters consisting of one or two outlier cells. The main drawback of graph-based methods is that, after graph construction, no information is retained about relationships beyond the neighbouring cells12. This has some practical consequences in datasets that exhibit differences in cell density, as more steps through the graph are required to move the same distance through a region of higher cell density. From the perspective of community detection algorithms, this effect “inflates” the high-density regions such that any internal substructure or noise is more likely to cause formation of subclusters. The resolution of clustering thus becomes dependent on the density of cells, which can occasionally be misleading if overstates the heterogeneity in the data. 10.3.2 Implementation There are several considerations in the practical execution of a graph-based clustering method: How many neighbors are considered when constructing the graph. What scheme is used to weight the edges. Which community detection algorithm is used to define the clusters. For example, the following code uses the 10 nearest neighbors of each cell to construct a shared nearest neighbor graph. Two cells are connected by an edge if any of their nearest neighbors are shared, with the edge weight defined from the highest average rank of the shared neighbors (Xu and Su 2015). The Walktrap method from the igraph package is then used to identify communities. All calculations are performed using the top PCs to take advantage of data compression and denoising. library(scran) g &lt;- buildSNNGraph(sce.pbmc, k=10, use.dimred = &#39;PCA&#39;) clust &lt;- igraph::cluster_walktrap(g)$membership table(clust) ## clust ## 1 2 3 4 5 6 7 8 9 10 11 12 ## 792 519 568 549 117 202 89 943 55 35 17 36 We assign the cluster assignments back into our SingleCellExperiment object as a factor in the column metadata. This allows us to conveniently visualize the distribution of clusters in a \\(t\\)-SNE plot (Figure 10.1). library(scater) sce.pbmc$cluster &lt;- factor(clust) plotReducedDim(sce.pbmc, &quot;TSNE&quot;, colour_by=&quot;cluster&quot;) Figure 10.1: \\(t\\)-SNE plot of the 10X PBMC dataset, where each point represents a cell and is coloured according to the identity of the assigned cluster from graph-based clustering. The most important parameter is k, i.e., the number of nearest neighbors used to construct the graph. This controls the resolution of the clustering, with higher k yielding a more inter-connected graph and larger clusters. Users can exploit this by experimenting with different values of k to obtain a satisfactory resolution. # More resolved. g.5 &lt;- buildSNNGraph(sce.pbmc, k=5, use.dimred = &#39;PCA&#39;) clust.5 &lt;- igraph::cluster_walktrap(g.5)$membership table(clust.5) ## clust.5 ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## 123 798 26 559 505 200 91 505 986 27 50 9 10 9 7 17 # Less resolved. g.50 &lt;- buildSNNGraph(sce.pbmc, k=50, use.dimred = &#39;PCA&#39;) clust.50 &lt;- igraph::cluster_walktrap(g.50)$membership table(clust.50) ## clust.50 ## 1 2 3 4 5 6 7 8 ## 181 839 572 116 173 518 488 1035 10.3.3 Other parameters Further tweaking can be done by changing the weighting scheme. Setting type=&quot;number&quot; will weight edges based on the number of nearest neighbors that are shared between two cells. Similarly, type=&quot;jaccard&quot; will weight edges according to the Jaccard index of the two sets of neighbors. We can also disable weighting altogether by using buildKNNGraph(), which is occasionally useful for downstream graph operations that do not support weights. g.num &lt;- buildSNNGraph(sce.pbmc, use.dimred=&quot;PCA&quot;, type=&quot;number&quot;) g.jaccard &lt;- buildSNNGraph(sce.pbmc, use.dimred=&quot;PCA&quot;, type=&quot;jaccard&quot;) g.none &lt;- buildKNNGraph(sce.pbmc, use.dimred=&quot;PCA&quot;) All of these g variables are graph objects from the igraph package. They can be used with any of the community detection algorithms provided by igraph. We have already mentioned the Walktrap approach, but many others are available to choose from: clust.louvain &lt;- igraph::cluster_louvain(g)$membership clust.infomap &lt;- igraph::cluster_infomap(g)$membership clust.fast &lt;- igraph::cluster_fast_greedy(g)$membership clust.labprop &lt;- igraph::cluster_label_prop(g)$membership clust.eigen &lt;- igraph::cluster_leading_eigen(g)$membership It is then straightforward to compare two clustering strategies to see how they differ. For example, the results below suggest that Louvain is similar to Walktrap; fast-greedy yields coarser clusters; and Infomap provides higher resolution. table(Louvain=clust.louvain, Walktrap=clust) ## Walktrap ## Louvain 1 2 3 4 5 6 7 8 9 10 11 12 ## 1 0 0 0 0 0 0 0 0 0 0 0 36 ## 2 0 0 0 0 0 187 0 0 0 0 0 0 ## 3 0 0 546 2 0 15 0 0 0 0 0 0 ## 4 0 0 8 23 0 0 0 943 0 0 0 0 ## 5 32 0 0 1 0 0 89 0 0 35 0 0 ## 6 0 519 0 0 0 0 0 0 0 0 0 0 ## 7 760 0 0 0 4 0 0 0 1 0 0 0 ## 8 0 0 0 0 113 0 0 0 0 0 0 0 ## 9 0 0 0 0 0 0 0 0 0 0 17 0 ## 10 0 0 13 8 0 0 0 0 54 0 0 0 ## 11 0 0 1 515 0 0 0 0 0 0 0 0 table(Infomap=clust.infomap, Walktrap=clust) ## Walktrap ## Infomap 1 2 3 4 5 6 7 8 9 10 11 12 ## 1 0 0 2 24 0 0 0 943 0 0 0 0 ## 2 483 0 0 0 0 0 0 0 0 0 0 0 ## 3 0 0 1 375 0 0 0 0 0 0 0 0 ## 4 0 295 0 0 0 0 0 0 0 0 0 0 ## 5 0 0 254 0 0 2 0 0 0 0 0 0 ## 6 246 0 0 0 0 0 0 0 0 0 0 0 ## 7 0 211 0 0 0 0 0 0 0 0 0 0 ## 8 0 0 0 0 0 187 0 0 0 0 0 0 ## 9 0 0 168 4 0 0 0 0 0 0 0 0 ## 10 0 0 120 0 0 13 0 0 0 0 0 0 ## 11 0 0 0 137 0 0 0 0 0 0 0 0 ## 12 0 0 0 0 94 0 0 0 0 0 0 0 ## 13 0 0 0 0 0 0 89 0 0 0 0 0 ## 14 0 0 0 0 0 0 0 0 55 0 0 0 ## 15 0 0 0 0 0 0 0 0 0 0 0 36 ## 16 20 0 0 0 23 0 0 0 0 0 0 0 ## [ reached getOption(&quot;max.print&quot;) -- omitted 6 rows ] table(Fast=clust.fast, Walktrap=clust) ## Walktrap ## Fast 1 2 3 4 5 6 7 8 9 10 11 12 ## 1 0 0 0 4 0 0 0 880 0 0 0 0 ## 2 0 0 0 0 88 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 0 17 0 ## 4 3 3 568 545 0 202 0 63 55 1 0 0 ## 5 789 0 0 0 29 0 89 0 0 34 0 0 ## 6 0 0 0 0 0 0 0 0 0 0 0 36 ## 7 0 516 0 0 0 0 0 0 0 0 0 0 Pipelines involving scran default to rank-based weights followed by Walktrap clustering. In contrast, Seurat uses Jaccard-based weights followed by Louvain clustering. Both of these strategies work well, and it is likely that the same could be said for many other combinations of weighting schemes and community detection algorithms13. 10.3.4 Assessing cluster separation When dealing with graphs, the modularity is a natural metric for evaluating the separation between communities/clusters. This is defined as the (scaled) difference between the observed total weight of edges between nodes in the same cluster and the expected total weight if edge weights were randomly distributed across all pairs of nodes. Larger modularity values indicate that there most edges occur within clusters, suggesting that the clusters are sufficiently well separated to avoid edges forming between neighboring cells in different clusters. The standard approach is to report a single modularity value for a clustering on a given graph. This is useful for comparing different clusterings on the same graph - and indeed, some community detection algorithms are designed with the aim of maximizing the modularity - but it is less helpful for interpreting a given clustering. Rather, we use the clusterModularity() function, which returns modularity scores for each cluster and cluster pair. More precisely, when get.values=TRUE, it returns the observed and expected sum of weights that are used to compute the standard modularity score. mod &lt;- clusterModularity(g, clust, get.values=TRUE) names(mod) ## [1] &quot;observed&quot; &quot;expected&quot; In each matrix, each row/column corresponds to a cluster, and each entry of the matrix contains the total weight of edges between cells in the respective clusters. A dataset containing well-separated clusters should contain most of the observed total weight on the diagonal entries, i.e., most edges occur between cells in the same cluster. We visualize this by computing the log-ratio of observed to expected weights (Figure 10.2). (We use the log-ratio instead of the difference as the latter’s scale depends on the number of cells in each cluster, meaning that any visualization would be dominated by large differences for large clusters.) ratio &lt;- mod$observed/mod$expected library(pheatmap) pheatmap(log2(ratio+1), cluster_rows=FALSE, cluster_cols=FALSE, color=colorRampPalette(c(&quot;white&quot;, &quot;blue&quot;))(100)) Figure 10.2: Heatmap of the log2-ratio of the total weight between nodes in the same cluster or in different clusters, relative to the total weight expected under a null model of random links. Figure 10.2 shows that the weight is mostly concentrated on the diagonal. The few off-diagonal contributions represent closely related clusters with (relatively) many inter-connecting edges. This is not a problem, as poorer separation is to be expected when resolution increases. In fact, this can assist interpretation by knowing which clusters are more closely related to each other. One useful approach is to use the ratio matrix to form another graph where the nodes are clusters rather than cells. Edges between nodes are weighted according to the ratio of observed to expected edge weights between cells in those clusters. We can then repeat our graph operations on this new cluster-level graph. For example, we could obtain clusters of clusters, or we could simply create a new cluster-based layout for visualization (Figure 10.3). This is analogous to the “graph abstraction” approach described by Wolf et al. (2017). cluster.gr &lt;- igraph::graph_from_adjacency_matrix(ratio, mode=&quot;undirected&quot;, weighted=TRUE, diag=FALSE) plot(cluster.gr, edge.width=igraph::E(cluster.gr)$weight*10) Figure 10.3: Force-directed layout showing the relationships between clusters based on the ratio of observed to expected total weights between nodes in different clusters. The thickness of the edge between a pair of clusters is proportional to the corresponding ratio. Incidentally, some readers may have noticed that all igraph commands were prefixed with igraph::. We have done this deliberately to avoid bringing igraph::normalize into the global namespace. Rather unfortunately, this normalize function accepts any argument and returns NULL, which causes difficult-to-diagnose bugs when it overwrites our intended normalize from scater. 10.4 \\(k\\)-means clustering 10.4.1 Background \\(k\\)-means clustering is a classic technique that aims to partition cells into \\(k\\) clusters. Each cell is assigned to the cluster with the closest centroid, which is done by minimizing the within-cluster sum of squares using a random starting configuration for the \\(k\\) centroids. The main advantage of this approach lies in its speed, given the simplicity and ease of implementation of the algorithm. However, it has a number of serious shortcomings: It implicitly favours spherical clusters of equal radius. This can result in inappropriate partitionings on real datasets that contain groupings with irregular sizes and shapes. The number of clusters \\(k\\) must be specified beforehand. This strongly affects the algorithm such that an inappropriate choice will inevitably lead to a poor clustering. (For example, setting \\(k\\) to be below the number of cell types will always lead to co-clustering of two cell types, regardless of how well separated they are. In contrast, other methods like graph-based clustering will respect strong separation even if the resolution is set to a low value.) It is dependent on the randomly chosen initial coordinates. This requires multiple runs to verify that the clustering is stable. These issues reduce the appeal of \\(k\\)-means clustering for scRNA-seq data analysis. Despite this, it is still used widely as it is fast and - frankly - often good enough. Even if we were to forgo \\(k\\)-means as our clustering method of choice, it is still one of the best approaches for sample-based data compression. In this application, we set \\(k\\) to a large value such as the square root of the number of cells to obtain fine-grained clusters. These are not meant to be interpreted directly, but rather, the centroids are treated as “samples” for further analyses. The idea here is to obtain a single representative of each region of the expression space, reducing the number of samples and computational work in later steps like, e.g., trajectory reconstruction. This approach will also eliminate differences in cell density across the expression space, ensuring that the most abundant cell type does not dominate downstream results. 10.4.2 Base implementation Base R provides the kmeans() function that does as its name suggests. We call this on our top PCs to obtain a clustering for a specified number of clusters in the centers= argument. This requires a random seed to ensure that the results are reproducible. In general, the \\(k\\)-means clusters correspond to the visual clusters on the \\(t\\)-SNE plot in Figure 10.4, though there are some divergences that are not apparent in, e.g., Figure 10.1. (This is at least partially due to the fact that \\(t\\)-SNE is itself graph-based and so will naturally agree more with a graph-based clustering strategy.) set.seed(100) clust.kmeans &lt;- kmeans(reducedDim(sce.pbmc, &quot;PCA&quot;), centers=10) table(clust.kmeans$cluster) ## ## 1 2 3 4 5 6 7 8 9 10 ## 458 207 23 517 1 568 36 1033 1057 22 sce.pbmc$cluster &lt;- factor(clust.kmeans$cluster) plotReducedDim(sce.pbmc, &quot;TSNE&quot;, colour_by=&quot;cluster&quot;) Figure 10.4: \\(t\\)-SNE plot of the 10X PBMC dataset, where each point represents a cell and is coloured according to the identity of the assigned cluster from \\(k\\)-means clustering. We obtain a “reasonable” choice of \\(k\\) by computing the gap statistic using methods from the cluster package. This is the log-ratio of the expected to observed within-cluster sum of squares, where the expected value is computed by randomly distributingn cells throughout high-dimensional space using the bounding box of the original data. A larger gap statistic represents a lower observed sum of squares - and thus better clustering - compared to a population with no structure. Ideally, we would choose the \\(k\\) that maximizes the gap statistic, but this is often unhelpful as the tendency of \\(k\\)-means to favour spherical clusters results in a large choice \\(k\\) to capture different cluster shapes. Instead, we choose the most parsimonious \\(k\\) beyond which the increases in the gap statistic are considered insignificant (Figure ??). library(cluster) gaps &lt;- clusGap(reducedDim(sce.pbmc, &quot;PCA&quot;), kmeans, K.max=20) best.k &lt;- maxSE(gaps$Tab[,&quot;gap&quot;], gaps$Tab[,&quot;SE.sim&quot;]) best.k ## [1] 9 plot(gaps$Tab[,&quot;gap&quot;], xlab=&quot;Number of clusters&quot;, ylab=&quot;Gap statistic&quot;) abline(v=best.k, col=&quot;red&quot;) Figure 10.5: Gap statistic with respect to increasing number of \\(k\\)-means clusters in the 10X PBMC dataset. The red line represents the chosen \\(k\\). It is then straightforwad to repeat the clustering with the best.k (Figure 10.6. set.seed(100) clust.kmeans2 &lt;- kmeans(reducedDim(sce.pbmc, &quot;PCA&quot;), centers=best.k) table(clust.kmeans2$cluster) ## ## 1 2 3 4 5 6 7 8 9 ## 458 207 24 517 22 568 36 1033 1057 sce.pbmc$cluster &lt;- factor(clust.kmeans2$cluster) plotReducedDim(sce.pbmc, &quot;TSNE&quot;, colour_by=&quot;cluster&quot;) Figure 10.6: \\(t\\)-SNE plot of the 10X PBMC dataset, where each point represents a cell and is coloured according to the identity of the assigned cluster from \\(k\\)-means clustering. Here, \\(k\\) is chosen based on the gap statistic. 10.4.3 Assessing cluster separation It is informative to examine the within-cluster sum of squares for each cluster. Clusters are more likely to have low sum of squares if they have no internal structure and are separated from other clusters (such that there are not many cells on the boundaries between clusters, which results in a higher sum of squares from the centroid). As an aside, clusters with the largest sum of squares do not appear to be the most dispersed in Figure ??, which highlights the risks of attempting to quantitatively interpret \\(t\\)-SNE plots. tab &lt;- data.frame(WithinSS=clust.kmeans2$withinss, NCells=tabulate(clust.kmeans2$cluster)) tab ## WithinSS NCells ## 1 11113 458 ## 2 5715 207 ## 3 291364 24 ## 4 15365 517 ## 5 276270 22 ## 6 18199 568 ## 7 1533 36 ## 8 15074 1033 ## 9 73975 1057 To explore the relationships between \\(k\\)-means clusters, a natural approach is to compute distances between their centroids. This directly lends itself to visualization as a tree after hierarchical clustering (Figure 10.7). cent.tree &lt;- hclust(dist(clust.kmeans2$centers), &quot;ward.D2&quot;) plot(cent.tree) Figure 10.7: Hierarchy of \\(k\\)-means cluster centroids, using Ward’s minimum variance method. 10.5 Hierarchical clustering 10.5.1 Background Hierarchical clustering is an ancient technique that aims to generate a dendrogram containing a hierarchy of samples. This is most commonly done by greedily agglomerating samples into clusters, then agglomerating those clusters into larger clusters, and so on until all samples belong to a single cluster. Variants of hierarchical clustering methods primarily differ in how they choose to perform the agglomerations. For example, complete linkage aims to merge clusters with the smallest maximum distance between their elements, while Ward’s method aims to minimize the increase in within-cluster variance. In the context of scRNA-seq, the main advantage of hierarchical clustering lies in the production of the dendrogram. This is a rich summary that describes not only the relationships between cells but also the relationships between clusters at varying resolution. Users can easily “cut” the tree at different heights to define clusters with different granularity, where clusters defined at high resolution are guaranteed to be nested within those defined at a lower resolution. The dendrogram is also a natural representation of the data in situations where cells have descended from a relatively recent common ancestor. In practice, hierachical clustering is too slow to be used for anything but the smallest scRNA-seq datasets. Most variants require a cell-cell distance matrix that is prohibitively expensive to compute for many cells. Greedy agglomeration is also likely to result in a quantitatively suboptimal partitioning (as defined by the agglomeration measure) at higher levels of the dendrogram when the number of cells and merge steps is high. Nonetheless, we will still demonstrate the application of hierarchical clustering here, as it is useful for squeezing more information out of datasets with very few cells14. 10.5.2 Implementation As the PBMC dataset is too large, we will demonstrate on the 416B dataset instead. View history ### loading ### library(scRNAseq) sce.416b &lt;- LunSpikeInData(which=&quot;416b&quot;) ### gene-annotation ### library(org.Mm.eg.db) symb &lt;- mapIds(org.Mm.eg.db, keys=rownames(sce.416b), keytype=&quot;ENSEMBL&quot;, column=&quot;SYMBOL&quot;) rowData(sce.416b)$ENSEMBL &lt;- rownames(sce.416b) rowData(sce.416b)$SYMBOL &lt;- symb library(scater) rownames(sce.416b) &lt;- uniquifyFeatureNames(rowData(sce.416b)$ENSEMBL, rowData(sce.416b)$SYMBOL) library(TxDb.Mmusculus.UCSC.mm10.ensGene) location &lt;- mapIds(TxDb.Mmusculus.UCSC.mm10.ensGene, keys=rowData(sce.416b)$ENSEMBL, column=&quot;CDSCHROM&quot;, keytype=&quot;GENEID&quot;) rowData(sce.416b)$CHR &lt;- location ### quality-control ### mito &lt;- which(rowData(sce.416b)$CHR==&quot;chrM&quot;) sce.416b &lt;- calculateQCMetrics(sce.416b, feature_controls=list(Mt=mito)) libsize.drop &lt;- isOutlier(sce.416b$total_counts, nmads=3, type=&quot;lower&quot;, log=TRUE, batch=sce.416b$PlateOnco) feature.drop &lt;- isOutlier(sce.416b$total_features_by_counts, nmads=3, type=&quot;lower&quot;, log=TRUE, batch=sce.416b$PlateOnco) spike.drop &lt;- isOutlier(sce.416b$pct_counts_ERCC, nmads=3, type=&quot;higher&quot;, batch=sce.416b$PlateOnco) keep &lt;- !(libsize.drop | feature.drop | spike.drop) sce.416b &lt;- sce.416b[,keep] ### normalization ### library(scran) sce.416b &lt;- computeSumFactors(sce.416b) sce.416b &lt;- computeSpikeFactors(sce.416b, general.use=FALSE) sce.416b &lt;- normalize(sce.416b) ### variance-modelling ### fit.416b &lt;- trendVar(sce.416b, block=sce.416b$Plate) dec.416b &lt;- decomposeVar(sce.416b, fit.416b) # Taking all of the genes with positive biological components. chosen.hvgs &lt;- rownames(dec.416b)[dec.416b$bio &gt; 0] ### batch-correction ### # Composition of the plates is expected to be the same, # hence the use of a simple &#39;removeBatchEffect&#39;. library(limma) assay(sce.416b, &quot;corrected&quot;) &lt;- removeBatchEffect(logcounts(sce.416b), design=model.matrix(~sce.416b$phenotype), batch=sce.416b$Plate) ### dimensionality-reduction ### sce.416b &lt;- denoisePCA(sce.416b, technical=dec.416b, subset.row=chosen.hvgs, assay.type=&quot;corrected&quot;) set.seed(1010) sce.416b &lt;- runTSNE(sce.416b, use_dimred=&quot;PCA&quot;, perplexity=10) sce.416b ## class: SingleCellExperiment ## dim: 46703 188 ## metadata(1): log.exprs.offset ## assays(3): counts logcounts corrected ## rownames(46703): ENSMUSG00000102693 ENSMUSG00000064842 ... SIRV7 ## CBFB-MYH11-mcherry ## rowData names(14): Length ENSEMBL ... total_counts ## log10_total_counts ## colnames(188): SLX-9555.N701_S502.C89V9ANXX.s_1.r_1 ## SLX-9555.N701_S503.C89V9ANXX.s_1.r_1 ... ## SLX-11312.N712_S507.H5H5YBBXX.s_8.r_1 ## SLX-11312.N712_S517.H5H5YBBXX.s_8.r_1 ## colData names(63): Source Name cell line ... ## pct_counts_in_top_200_features_SIRV ## pct_counts_in_top_500_features_SIRV ## reducedDimNames(2): PCA TSNE ## spikeNames(2): ERCC SIRV ## altExpNames(0): We compute a cell-cell distance matrix using the top PCs and we apply hierarchical clustering with Ward’s method. This aims to minimize the increase in the intra-cluster variance at each merge step. The resulting tree in Figure 10.8 shows a clear split in the population caused by oncogene induction. dist.416b &lt;- dist(reducedDim(sce.416b, &quot;PCA&quot;)) tree.416b &lt;- hclust(dist.416b, &quot;ward.D2&quot;) # Making a prettier plot. library(dendextend) tree.416b$labels &lt;- seq_along(tree.416b$labels) dend &lt;- as.dendrogram(tree.416b, hang=0.1) combined.fac &lt;- paste0(sce.416b$Plate, &quot;.&quot;, sce.416b$Oncogene) labels_colors(dend) &lt;- c( `20160113.control`=&quot;blue&quot;, `20160113.induced`=&quot;red&quot;, `20160325.control`=&quot;dodgerblue&quot;, `20160325.induced`=&quot;salmon&quot; )[combined.fac][order.dendrogram(dend)] plot(dend) Figure 10.8: Hierarchy of cells in the 416B data set after hierarchical clustering, where each leaf node is a cell that is coloured according to its oncogene induction status (red is induced, blue is control) and plate of origin (light or dark). To obtain explicit clusters, we “cut” the tree by removing internal branches such that every subtree represents a distinct cluster. This is most simply done by removing internal branches above a certain height of the tree, as performed by the cutree() function. We generally prefer to use the dynamicTreeCut package, which uses the shape of the tree to choose a suitable partitioning (Figure ??). library(dynamicTreeCut) # minClusterSize needs to be turned down for small datasets. # deepSplit controls the resolution of the partitioning. clust.416b &lt;- cutreeDynamic(tree.416b, distM=as.matrix(dist.416b), minClusterSize=10, deepSplit=1) ## ..cutHeight not given, setting it to 1300 ===&gt; 99% of the (truncated) height range in dendro. ## ..done. table(clust.416b) ## clust.416b ## 1 2 3 4 5 6 ## 45 44 37 36 13 13 labels_colors(dend) &lt;- clust.416b[order.dendrogram(dend)] plot(dend) Figure 10.9: Hierarchy of cells in the 416B data set after hierarchical clustering, where each leaf node is a cell that is coloured according to its assigned cluster identity from a dynamic tree cut. This generally corresponds well to the grouping of cells on a \\(t\\)-SNE plot (Figure 10.10). sce.416b$cluster &lt;- factor(clust.416b) plotReducedDim(sce.416b, &quot;TSNE&quot;, colour_by=&quot;cluster&quot;) Figure 10.10: \\(t\\)-SNE plot of the 416B dataset, where each point represents a cell and is coloured according to the identity of the assigned cluster from hierarchical clustering. 10.5.3 Assessing cluster separation We check the separation of the clusters using the silhouette width (Figure 10.11). For each cell, we compute the average distance to cells in each other cluster. We then compute the minimum of these average distances across all clusters, as well as the average distance to cells in the same cluster. The silhouette width for each cell is defined as the difference between these two values divided by their maximum. Cells with large positive silhouette widths are closer to other cells in the same cluster than to cells in different clusters. Each cluster would ideally contain large positive silhouette widths, indicating that it is well-separated from other clusters. In Figure 10.11, some clusters are well-separated while others have a substantial proportion of negative widths. These can arise from the presence of internal subclusters, which inflates the within-cluster distance; or overclustering, where cells at the boundary of a partition are closer to the neighboring cluster than their own cluster. sil &lt;- silhouette(clust.416b, dist = dist.416b) plot(sil) Figure 10.11: Silhouette widths for cells in each cluster in the 416B dataset. Each bar represents a cell, grouped by the cluster to which it is assigned. For a more detailed examination, we identify the closest neighboring cluster for cells with negative widths. This provides a perspective on the relationships between clusters that is closer to the raw data than the dendrogram in Figure 10.9. neg.widths &lt;- sil[,3] &lt; 0 table(Cluster=sil[neg.widths,1], Neighbor=sil[neg.widths,2]) ## Neighbor ## Cluster 1 2 4 5 6 ## 1 0 0 1 0 0 ## 2 0 0 0 4 0 ## 3 0 32 0 0 5 ## 6 1 0 2 0 0 The average silhouette width across all cells can also be used to choose clustering parameters. The aim is to maximize the average silhouette width in order to obtain well-separated clusters. This can be helpful to automatically obtain a “reasonable” clustering, though in practice, the clustering that yields the strongest separation is usually trivial and not biologically informative. 10.6 Session Info ## &lt;button class=&quot;aaron-collapse&quot;&gt;View session info&lt;/button&gt; ## &lt;div class=&quot;aaron-content&quot;&gt; ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.6 LTS ## ## Matrix products: default ## BLAS/LAPACK: /app/easybuild/software/OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1/lib/libopenblas_prescottp-r0.2.18.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats4 parallel stats graphics grDevices utils datasets ## [8] methods base ## ## other attached packages: ## [1] dynamicTreeCut_1.63-1 dendextend_1.12.0 ## [3] cluster_2.1.0 pheatmap_1.0.12 ## [5] scater_1.13.9 ggplot2_3.2.0 ## [7] scran_1.13.9 SingleCellExperiment_1.5.3 ## [9] SummarizedExperiment_1.15.5 DelayedArray_0.11.4 ## [11] BiocParallel_1.19.0 matrixStats_0.54.0 ## [13] Biobase_2.45.0 GenomicRanges_1.37.14 ## [15] GenomeInfoDb_1.21.1 IRanges_2.19.10 ## [17] S4Vectors_0.23.17 BiocGenerics_0.31.5 ## [19] BiocStyle_2.13.2 Cairo_1.5-10 ## ## loaded via a namespace (and not attached): ## [1] viridis_0.5.1 edgeR_3.27.6 ## [3] BiocSingular_1.1.5 viridisLite_0.3.0 ## [5] DelayedMatrixStats_1.7.1 assertthat_0.2.1 ## [7] statmod_1.4.32 BiocManager_1.30.4 ## [9] dqrng_0.2.1 GenomeInfoDbData_1.2.1 ## [11] vipor_0.4.5 yaml_2.2.0 ## [13] pillar_1.4.2 lattice_0.20-38 ## [15] glue_1.3.1 limma_3.41.6 ## [17] digest_0.6.20 RColorBrewer_1.1-2 ## [19] XVector_0.25.0 colorspace_1.4-1 ## [21] htmltools_0.3.6 Matrix_1.2-17 ## [23] pkgconfig_2.0.2 bookdown_0.12 ## [25] zlibbioc_1.31.0 purrr_0.3.2 ## [27] scales_1.0.0 tibble_2.1.3 ## [29] withr_2.1.2 lazyeval_0.2.2 ## [31] magrittr_1.5 crayon_1.3.4 ## [33] evaluate_0.14 beeswarm_0.2.3 ## [35] tools_3.6.0 stringr_1.4.0 ## [37] munsell_0.5.0 locfit_1.5-9.1 ## [39] irlba_2.3.3 compiler_3.6.0 ## [41] rsvd_1.0.1 rlang_0.4.0 ## [43] grid_3.6.0 RCurl_1.95-4.12 ## [45] BiocNeighbors_1.3.2 igraph_1.2.4.1 ## [47] bitops_1.0-6 rmarkdown_1.14 ## [49] gtable_0.3.0 R6_2.4.0 ## [51] gridExtra_2.3 knitr_1.23 ## [53] dplyr_0.8.3 stringi_1.4.3 ## [55] ggbeeswarm_0.6.0 Rcpp_1.0.1 ## [57] tidyselect_0.2.5 xfun_0.8 ## &lt;/div&gt; References "],
["marker-gene-detection.html", "Chapter 11 Marker gene detection 11.1 Motivation 11.2 Using pairwise \\(t\\)-tests 11.3 Alternative testing regimes 11.4 Handling blocking factors 11.5 Using the block= argument 11.6 Using the design= argument 11.7 Invalidity of \\(p\\)-values 11.8 Session Info", " Chapter 11 Marker gene detection .aaron-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .aaron-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } 11.1 Motivation To interpret our clustering results from Chapter ???, we identify the genes that drive separation between clusters. These marker genes allow us to assign biological meaning to each cluster based on their functional annotation. In the most obvious case, the marker genes for each cluster are a priori associated with particular cell types, allowing us to treat the clustering as a proxy for cell type identity. The same principle can be applied to more subtle differences in activation status or differentiation state. Identification of marker genes is usually based around the retrospective detection of differential expression between clusters. Genes that are more strongly DE are more likely to have driven cluster separation in the first place. Several different statistical tests are available to quantify the differences in expression profiles, and different approaches can be used to consolidate test results into a single ranking of genes for each cluster. These choices parametrize the theroetical differences between the various marker detection strategies presented in this chapter. We will demonstrate using the 10X PBMC dataset: View history ### loading ### library(BiocFileCache) bfc &lt;- BiocFileCache(&quot;raw_data&quot;, ask = FALSE) raw.path &lt;- bfcrpath(bfc, file.path(&quot;http://cf.10xgenomics.com/samples&quot;, &quot;cell-exp/2.1.0/pbmc4k/pbmc4k_raw_gene_bc_matrices.tar.gz&quot;)) untar(raw.path, exdir=file.path(tempdir(), &quot;pbmc4k&quot;)) library(DropletUtils) fname &lt;- file.path(tempdir(), &quot;pbmc4k/raw_gene_bc_matrices/GRCh38&quot;) sce.pbmc &lt;- read10xCounts(fname, col.names=TRUE) ### gene-annotation ### library(scater) rownames(sce.pbmc) &lt;- uniquifyFeatureNames(rowData(sce.pbmc)$ID, rowData(sce.pbmc)$Symbol) library(EnsDb.Hsapiens.v86) location &lt;- mapIds(EnsDb.Hsapiens.v86, keys=rowData(sce.pbmc)$ID, column=&quot;SEQNAME&quot;, keytype=&quot;GENEID&quot;) ### cell-detection ### set.seed(100) e.out &lt;- emptyDrops(counts(sce.pbmc)) sce.pbmc &lt;- sce.pbmc[,which(e.out$FDR &lt;= 0.001)] ### quality-control ### sce.pbmc &lt;- calculateQCMetrics(sce.pbmc, feature_controls=list(Mito=which(location==&quot;MT&quot;))) high.mito &lt;- isOutlier(sce.pbmc$pct_counts_Mito, nmads=3, type=&quot;higher&quot;) sce.pbmc &lt;- sce.pbmc[,!high.mito] ### normalization ### library(scran) set.seed(1000) clusters &lt;- quickCluster(sce.pbmc) sce.pbmc &lt;- computeSumFactors(sce.pbmc, min.mean=0.1, cluster=clusters) sce.pbmc &lt;- normalize(sce.pbmc) ### feature-selection ### fit.pbmc &lt;- trendVar(sce.pbmc, use.spikes=FALSE) dec.pbmc &lt;- decomposeVar(fit=fit.pbmc) o &lt;- order(dec.pbmc$bio, decreasing=TRUE) chosen.hvgs &lt;- rownames(dec.pbmc)[head(o, 2000)] ### dimensionality-reduction ### set.seed(10000) sce.pbmc &lt;- runPCA(sce.pbmc, feature_set=chosen.hvgs, ncomponents=25, BSPARAM=BiocSingular::IrlbaParam()) set.seed(100000) sce.pbmc &lt;- runTSNE(sce.pbmc, use_dimred=&quot;PCA&quot;) set.seed(1000000) sce.pbmc &lt;- runUMAP(sce.pbmc, use_dimred=&quot;PCA&quot;) ### clustering ### g &lt;- buildSNNGraph(sce.pbmc, k=10, use.dimred = &#39;PCA&#39;) clust &lt;- igraph::cluster_walktrap(g)$membership sce.pbmc$cluster &lt;- factor(clust) sce.pbmc ## class: SingleCellExperiment ## dim: 33694 3922 ## metadata(1): log.exprs.offset ## assays(2): counts logcounts ## rownames(33694): RP11-34P13.3 FAM138A ... AC213203.1 FAM231B ## rowData names(10): ID Symbol ... total_counts log10_total_counts ## colnames(3922): AAACCTGAGAAGGCCT-1 AAACCTGAGACAGACC-1 ... ## TTTGTCACAGGTCCAC-1 TTTGTCATCCCAAGAT-1 ## colData names(39): Sample Barcode ... ## pct_counts_in_top_500_features_Mito cluster ## reducedDimNames(3): PCA TSNE UMAP ## spikeNames(0): ## altExpNames(0): 11.2 Using pairwise \\(t\\)-tests 11.2.1 Standard application The Welch \\(t\\)-test is an obvious choice of statistical method to test for differences in expression between clusters. It is quickly computed and has good statistical properties for large numbers of cells [soneson2018bias]. We use the findMarkers() function to perform pairwise comparisons between clusters for each gene. This yields a list of DataFrames contained ranked candidate markers for each cluster. library(scran) markers.pbmc &lt;- findMarkers(sce.pbmc, sce.pbmc$cluster) markers.pbmc ## DataFrameList of length 12 ## names(12): 1 2 3 4 5 6 7 8 9 10 11 12 To demonstrate, we use cluster 9 as our cluster of interest for this section. The relevant DataFrame contains log2-fold changes of expression in cluster 9 over each other cluster, along with several statistics obtained by combining \\(p\\)-values (Simes 1986) across the pairwise comparisons involving 9. chosen &lt;- &quot;9&quot; interesting &lt;- markers.pbmc[[chosen]] colnames(interesting) ## [1] &quot;Top&quot; &quot;p.value&quot; &quot;FDR&quot; &quot;logFC.1&quot; &quot;logFC.2&quot; &quot;logFC.3&quot; ## [7] &quot;logFC.4&quot; &quot;logFC.5&quot; &quot;logFC.6&quot; &quot;logFC.7&quot; &quot;logFC.8&quot; &quot;logFC.10&quot; ## [13] &quot;logFC.11&quot; &quot;logFC.12&quot; Of particular interest is the Top field, which contains the highest15 rank for each gene across all pairwise comparisons involving cluster 9. The set of genes with Top values of 1 contains the gene with the lowest \\(p\\)-value from each comparison. Similarly, the set of genes with Top values less than or equal to 10 contains the top 10 genes from each comparison. Each DataFrame produced by findMarkers() will order genes based on the Top value. interesting[1:10,1:3] ## DataFrame with 10 rows and 3 columns ## Top p.value FDR ## &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt; ## LEF1 1 6.20939421496419e-167 2.09219328679006e-162 ## GZMA 1 2.85174646001885e-163 4.80433726119381e-159 ## ACAP1 1 6.15083691785694e-163 6.9082099703425e-159 ## IGHD 1 7.47621860433727e-135 5.03807419309093e-131 ## TGFBI 1 5.05258071032724e-82 4.25604136134411e-79 ## FCGR3A 1 2.52077860009527e-70 1.4395782059595e-67 ## HLA-DQA1 1 3.97224203642212e-60 1.48711914639119e-57 ## SEC61B 1 6.04861464944905e-38 7.25274099638915e-36 ## PF4 1 2.35117342678427e-28 1.53231020197426e-26 ## LYAR 2 4.83449973651891e-131 2.71489390203784e-127 We use the Top value to identify a set of genes that is guaranteed to distinguish cluster 9 from any other cluster. Here, we examine the top 5 genes from each pairwise comparison (Figure 11.1). Some inspection of the most upregulated genes suggest that cluster 9 contains platelets or their precursors, based on the expression of platelet factor 4 (PF4) and pro-platelet basic protein (PPBP). best.set &lt;- interesting[interesting$Top &lt;= 5,] logFCs &lt;- as.matrix(best.set[,-(1:3)]) colnames(logFCs) &lt;- sub(&quot;logFC.&quot;, &quot;&quot;, colnames(logFCs)) library(pheatmap) pheatmap(logFCs, breaks=seq(-5, 5, length.out=101)) Figure 11.1: Heatmap of log-fold changes for cluster r chosen over all other clusters. Colours are capped at -5 and 5 to preserve dynamic range. 11.2.2 Using the log-fold change Our previous findMarkers() call considers both up- and downregulated genes to be potential markers. However, downregulated genes are less appealing as markers as it is more difficult to interpret and experimentally validate an absence of expression. To focus on up-regulated markers, we can instead perform a one-sided \\(t\\)-test to identify genes that are upregulated in each cluster compared to the others. This is achieved by setting direction=&quot;up&quot; in the findMarkers() call. markers.pbmc.up &lt;- findMarkers(sce.pbmc, sce.pbmc$cluster, direction=&quot;up&quot;) interesting.up &lt;- markers.pbmc.up[[chosen]] interesting.up[1:10,1:3] ## DataFrame with 10 rows and 3 columns ## Top p.value FDR ## &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt; ## PF4 1 1.17558671339212e-28 3.96102187210342e-24 ## TMSB4X 2 1.64674064685164e-25 2.77426396775095e-21 ## SDPR 2 3.55136790784062e-23 3.98865967622607e-19 ## GPX1 3 4.58923373210941e-22 3.50460237357863e-18 ## PPBP 3 5.20063271439818e-22 3.50460237357863e-18 ## NRGN 4 1.28482173823981e-21 7.21513060804201e-18 ## TAGLN2 4 4.3101937823381e-21 2.07468099002999e-17 ## GNG11 5 8.00725077665062e-20 3.37245384585583e-16 ## HIST1H2AC 6 1.02722989599352e-19 3.84572045728949e-16 ## TUBB1 7 1.52910079823675e-19 5.15215222957892e-16 The \\(t\\)-test also allows us to specify a non-zero log-fold change as the null hypothesis. This allows us to consider the magnitude of the log-fold change in our \\(p\\)-value calculations, in a manner that is more rigorous than simply filtering directly on the log-fold changes (McCarthy and Smyth 2009). (Specifically, a simple threshold does not consider the variance and can enrich for genes that have both large log-fold changes and large variances.) We perform this by setting lfc= in our findMarkers() call - when combined with direction=, this tests for genes with log-fold changes that are significantly greater than 1: markers.pbmc.up2 &lt;- findMarkers(sce.pbmc, sce.pbmc$cluster, direction=&quot;up&quot;, lfc=1) interesting.up2 &lt;- markers.pbmc.up2[[chosen]] interesting.up2[1:10,1:3] ## DataFrame with 10 rows and 3 columns ## Top p.value FDR ## &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt; ## PF4 1 6.02669690930859e-25 2.03063525662244e-20 ## TMSB4X 2 7.97170089366702e-19 8.95328299704057e-15 ## SDPR 2 3.73547685699784e-19 6.29315786098425e-15 ## GPX1 3 6.07463154377111e-18 4.09357270471648e-14 ## PPBP 3 1.33107797557639e-18 1.12123353272677e-14 ## NRGN 4 4.05970433364242e-17 2.27979463029579e-13 ## TAGLN2 4 2.25018630384581e-16 1.08311110459687e-12 ## GNG11 5 5.50609782505896e-16 2.31903075146921e-12 ## HIST1H2AC 6 6.53594082728356e-16 2.44691100260547e-12 ## TUBB1 7 2.07146248785668e-15 6.97958570658431e-12 These two settings yield a more focused set of candidate marker genes that are upregulated in cluster 9 (Figure ??). best.set &lt;- interesting.up2[interesting.up2$Top &lt;= 5,] logFCs &lt;- as.matrix(best.set[,-(1:3)]) colnames(logFCs) &lt;- sub(&quot;logFC.&quot;, &quot;&quot;, colnames(logFCs)) library(pheatmap) pheatmap(logFCs, breaks=seq(-5, 5, length.out=101)) Figure 11.2: Heatmap of log-fold changes for cluster 9 over all other clusters. Colours are capped at -5 and 5 to preserve dynamic range. Of course, this increased stringency is not without cost. If only upregulated genes are requested from findMarkers(), any cluster defined by downregulation of a marker gene will not contain that gene among the top set of features in its DataFrame. This is occasionally relevant for subtypes or other states that are distinguished by high versus low expression of particular genes16. Similarly, setting an excessively high log-fold change threshold may discard otherwise useful genes. For example, a gene upregulated in a small proportion of cells of a cluster will have a small log-fold change but can still be an effective marker if the focus is on specificity rather than sensitivity. 11.2.3 Finding cluster-specific markers By default, findMarkers() will give a high ranking to genes that are differentially expressed in any pairwise comparison. This is because a gene only needs a very low \\(p\\)-value in a single pairwise comparison to achieve a low Top value. A more stringent approach would onlly consider genes that are differentially expressed in all pairwise comparisons involving the cluster of interest. To achieve this, we set pval.type=&quot;all&quot; in findMarkers() to use an intersection-union test (Berger and Hsu 1996) where the combined \\(p\\)-value for each gene is the maximum of the \\(p\\)-values from all pairwise comparisons. A gene will only achieve a low combined \\(p\\)-value if it is strongly DE in all comparisons to other clusters. # We can combine this with &#39;direction=&#39;. markers.pbmc.up3 &lt;- findMarkers(sce.pbmc, sce.pbmc$cluster, pval.type=&quot;all&quot;, direction=&quot;up&quot;) interesting.up3 &lt;- markers.pbmc.up3[[chosen]] interesting.up3[1:10,1:2] ## DataFrame with 10 rows and 2 columns ## p.value FDR ## &lt;numeric&gt; &lt;numeric&gt; ## PF4 1.80014139122098e-28 6.06539640357997e-24 ## SDPR 3.55136790784062e-23 5.98298951433908e-19 ## PPBP 6.48169985592535e-22 7.27981316485164e-18 ## NRGN 5.86340715476335e-21 4.93904101681492e-17 ## GNG11 8.95542696697954e-20 6.03488312450816e-16 ## HIST1H2AC 1.39989539303288e-19 7.360217470827e-16 ## TUBB1 1.52910079823675e-19 7.360217470827e-16 ## TMSB4X 1.11117788046074e-16 4.68000343803052e-13 ## ACRBP 7.90465526790601e-15 2.95932727329806e-11 ## TAGLN2 1.13799040801938e-14 3.8343448807805e-11 This strategy will only report genes that are highly specific to the cluster of interest. When it works, it can be highly effective as it generates a small focused set of candidate markers. However, any gene that is expressed at the same level in two or more clusters will simply not be detected. This is likely to discard many interesting genes, especially if the clusters are finely resolved with weak separation. To give a concrete example, consider a mixed population of CD4+-only, CD8+-only, double-positive and double-negative T cells. With pval.type=&quot;all&quot;, neither Cd4 or Cd8 would be detected as subpopulation-specific markers because each gene is expressed in two subpopulations. In comparison, pval.type=&quot;any&quot; will detect both of these genes as they will be DE between at least one pair of subpopulations. This reliability motivates the use of the latter setting as the default in findMarkers(). 11.3 Alternative testing regimes 11.3.1 Using the Wilcoxon rank sum test The Wilcoxon rank sum test (also known as the Wilcoxon-Mann-Whitney test, or WMW test) is another widely used method for pairwise comparisons between groups of observations. Its strength lies in the fact that it directly assesses separation between the expression distributions of different clusters. The WMW test statistic is proportional to the area-under-the-curve (AUC), i.e., the concordance probability, which is the probability of a random cell from one cluster having higher expression than a random cell from another cluster. In a pairwise comparison, AUCs of 1 or 0 indicate that the two clusters have perfectly separated expression distributions. Thus, the WMW test directly addresses the most desirable property of a candidate marker gene, while the \\(t\\) test only does so indirectly via the difference in the means and the intra-group variance. We perform WMW tests using the overlapExprs() function. This returns a list of DataFrames containing ranked candidate markers for each cluster. The direction=, lfc= and pval.type= arguments can be specified and have the same interpretation as described for \\(t\\) tests. We demonstrate below by detecting upregulated genes in each cluster with direction=&quot;up&quot;. # TODO: make findMarkers() accept a method= option that # switches between these things, rather than having a # different function altogether. Also need a lfc= option. markers.pbmc.wmw &lt;- overlapExprs(sce.pbmc, sce.pbmc$cluster, direction=&quot;up&quot;) names(markers.pbmc.wmw) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; To explore the results in more detail, we focus on the DataFrame for cluster 9. The interpretation of Top is the same as described for \\(t\\) tests, and Simes’ method is again used to combine \\(p\\)-values across pairwise comparisons. interesting.wmw &lt;- markers.pbmc.wmw[[chosen]] interesting.wmw[1:10,1:3] ## DataFrame with 10 rows and 3 columns ## Top p.value FDR ## &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt; ## PF4 1 1.0202929049051e-187 3.43777491378731e-183 ## TMSB4X 1 1.01281466445067e-33 3.25007402895248e-31 ## SDPR 2 3.5648749185145e-184 6.00574477522145e-180 ## PPBP 3 4.69409849189378e-163 5.27209848619572e-159 ## TUBB1 3 8.5842665088848e-162 7.23095689375918e-158 ## GNG11 3 3.27662779920279e-159 2.20805394132683e-155 ## NRGN 3 1.42552621019204e-155 8.0052800210352e-152 ## CLU 5 1.29715673377618e-149 5.46329987348186e-146 ## HIST1H2AC 5 2.48168081125816e-106 5.57451688363549e-103 ## TAGLN2 5 2.32628433861945e-34 7.60988587431489e-32 The DataFrame contains the AUCs from comparing cluster 9 to every other cluster (Figure 11.3). A value greater than 0.5 indicates that the gene is upregulated in the current cluster compared to the other cluster, while values less than 0.5 correspond to downregulation. We would typically expect AUCs of 0.7-0.8 for a strongly upregulated candidate marker. best.set &lt;- interesting.wmw[interesting.wmw$Top &lt;= 5,] AUCs &lt;- as.matrix(best.set[,-(1:3)]) colnames(AUCs) &lt;- sub(&quot;AUC.&quot;, &quot;&quot;, colnames(AUCs)) library(pheatmap) pheatmap(AUCs, breaks=seq(0, 1, length.out=21), color=viridis::viridis(21)) Figure 11.3: Heatmap of AUCs for cluster r chosen over all other clusters. The main disadvantage of the WMW test is that the AUCs are much slower to compute compared to \\(t\\)-statistics. This may be inconvenient for interactive analyses involving multiple iterations of marker detection. We can mitigate this to some extent by parallelizing these calculations using the BPPARAM= argument in overlapExprs(). 11.3.2 Using a binomial test The binomial test identifies genes that differ in the proportion of expressing cells between clusters. (For the purposes of this section, a cell is considered to express a gene simply if it has non-zero expression for that gene.) This represents a much more stringent definition of marker genes compared to the other methods, as differences in expression between clusters are effectively ignored if both distributions of expression values are not near zero. The premise is that genes are more likely to contribute to important biological decisions if they were active in one cluster and silent in another, compared to more subtle “tuning” effects from changing the expression of an active gene. From a practical perspective, a binary measure of presence/absence is easier to validate. We perform pairwise binomial tests between clusters using the pairwiseBinom() function. This returns a list of DataFrames containing marker statistics for each cluster, such as the Top rank and its \\(p\\)-value. Here, the effect size is reported as the log-fold change in this proportion between each pair of clusters. Large positive log-fold changes indicate that the gene is more frequently expressed in one cluster compared to the other. Here, we focus on genes that are upregulated in each cluster compared to the others by setting direction=&quot;up&quot;. # TODO: push this into another findMarkers() option. b.pairs &lt;- pairwiseBinom(sce.pbmc, sce.pbmc$cluster, direction=&quot;up&quot;) markers.pbmc.binom &lt;- combineMarkers(b.pairs$statistics, b.pairs$pairs) names(markers.pbmc.binom) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; interesting.binom &lt;- markers.pbmc.binom[[chosen]] colnames(interesting.binom) ## [1] &quot;Top&quot; &quot;p.value&quot; &quot;FDR&quot; &quot;logFC.1&quot; &quot;logFC.2&quot; &quot;logFC.3&quot; ## [7] &quot;logFC.4&quot; &quot;logFC.5&quot; &quot;logFC.6&quot; &quot;logFC.7&quot; &quot;logFC.8&quot; &quot;logFC.10&quot; ## [13] &quot;logFC.11&quot; &quot;logFC.12&quot; Figure (fig:viol-de-binom) confirms that the top genes exhibit strong differences in the proportion of expressing cells in cluster 9 compared to the others. library(scater) top.genes &lt;- head(rownames(interesting.binom)) plotExpression(sce.pbmc, x=&quot;cluster&quot;, features=top.genes) Figure 11.4: Distribution of log-normalized expression values for the top 10 DE genes involving cluster 9 with the binomial test, stratified by cluster assignment and coloured by the plate of origin for each cell. The disadvantage of the binomial test is that its increased stringency can lead to the loss of good candidate markers. For example, GCG is a known marker for pancreatic alpha cells but is expressed in almost every other cell of the Lawlor et al. (2017) pancreas data (Figure 11.5) and would not be highly ranked by the binomial test. View history ### loading ### library(scRNAseq) sce.lawlor &lt;- LawlorPancreasData() ### gene-annotation ### library(AnnotationHub) edb &lt;- AnnotationHub()[[&quot;AH73881&quot;]] anno &lt;- select(edb, keys=rownames(sce.lawlor), keytype=&quot;GENEID&quot;, columns=c(&quot;SYMBOL&quot;, &quot;SEQNAME&quot;)) rowData(sce.lawlor) &lt;- anno[match(rownames(sce.lawlor), anno[,1]),-1] ### quality-control ### library(scater) sce.lawlor &lt;- calculateQCMetrics(sce.lawlor, compact=TRUE, feature_controls=list(Mito=which(rowData(sce.lawlor)$SEQNAME==&quot;MT&quot;))) NFeatures &lt;- isOutlier(sce.lawlor$scater_qc$all$total_features_by_counts, log=TRUE, type=&quot;lower&quot;, nmads=3) LibSize &lt;- isOutlier(sce.lawlor$scater_qc$all$total_counts, log=TRUE, type=&quot;lower&quot;, nmads=3) Mito &lt;- isOutlier(sce.lawlor$scater_qc$feature_control_Mito$pct_counts, type=&quot;higher&quot;, nmads=3) discard &lt;- NFeatures | LibSize | Mito sce.lawlor &lt;- sce.lawlor[,!discard] ### normalization ### library(scran) set.seed(1000) clusters &lt;- quickCluster(sce.lawlor) sce.lawlor &lt;- computeSumFactors(sce.lawlor, clusters=clusters) sce.lawlor &lt;- normalize(sce.lawlor) plotExpression(sce.lawlor, x=&quot;cell type&quot;, features=&quot;ENSG00000115263&quot;) Figure 11.5: Distribution of log-normalized expression values for GCG across different pancreatic cell types in GSE86469. Another property of the binomial test is that it will not respond to scaling normalization. Systematic differences in library size between clusters will not be considered when computing \\(p\\)-values or effect sizes. This is not necessarily problematic for marker gene detection - users can treat this as retaining information about the total RNA content, analogous to spike-in normalization. 11.3.3 Using custom DE methods It is possible to perform marker gene detection based on precomputed DE statistics. This allows us to take advantage of more sophisticated tests in dedicated DE analysis packages. To demonstrate, consider the voom() approach from the limma package (Law et al. 2014). We first process our SingleCellExperiment to obtain a fit object as shown below. library(limma) design &lt;- model.matrix(~0 + cluster, data=colData(sce.pbmc)) colnames(design) ## [1] &quot;cluster1&quot; &quot;cluster2&quot; &quot;cluster3&quot; &quot;cluster4&quot; &quot;cluster5&quot; ## [6] &quot;cluster6&quot; &quot;cluster7&quot; &quot;cluster8&quot; &quot;cluster9&quot; &quot;cluster10&quot; ## [11] &quot;cluster11&quot; &quot;cluster12&quot; # Removing very low-abundance genes. keep &lt;- calculateAverage(sce.pbmc) &gt; 0.1 summary(keep) ## Mode FALSE TRUE ## logical 29387 4307 y &lt;- convertTo(sce.pbmc, subset.row=keep) v &lt;- voom(y, design) fit &lt;- lmFit(v, design) We then perform pairwise comparisons between clusters using the TREAT strategy (McCarthy and Smyth 2009) to test for log-fold changes that are significantly greater than 0.5. For each comparison, we store the corresponding data frame of statistics in all.results, along with the identities of the clusters involved in all.pairs. nclust &lt;- length(unique(sce.pbmc$cluster)) all.results &lt;- all.pairs &lt;- list() counter &lt;- 1L # Iterating across the first &#39;nclust&#39; coefficients in design, # and comparing them to each other in a pairwise manner. for (x in seq_len(nclust)) { for (y in seq_len(x-1L)) { con &lt;- integer(ncol(design)) con[x] &lt;- 1 con[y] &lt;- -1 fit2 &lt;- contrasts.fit(fit, con) fit2 &lt;- treat(fit2, robust=TRUE, lfc=0.5) res &lt;- topTreat(fit2, n=Inf, sort.by=&quot;none&quot;) all.results[[counter]] &lt;- res all.pairs[[counter]] &lt;- colnames(design)[c(x, y)] counter &lt;- counter+1L # Also filling the reverse comparison. res$logFC &lt;- -res$logFC all.results[[counter]] &lt;- res all.pairs[[counter]] &lt;- colnames(design)[c(y, x)] counter &lt;- counter+1L } } These custom results are consolidated into a single marker list for each cluster with the combineMarkers() function. This combines test statistics across all pairwise comparisons involving a single cluster, yielding a per-cluster DataFrame that can be interpreted in the same manner as discussed previously. all.pairs &lt;- do.call(rbind, all.pairs) combined &lt;- combineMarkers(all.results, all.pairs, pval.field=&quot;P.Value&quot;) # Inspecting results for our cluster of interest again. interesting.voom &lt;- combined[[paste0(&quot;cluster&quot;, chosen)]] colnames(interesting.voom) ## [1] &quot;Top&quot; &quot;p.value&quot; &quot;FDR&quot; ## [4] &quot;logFC.cluster1&quot; &quot;logFC.cluster2&quot; &quot;logFC.cluster3&quot; ## [7] &quot;logFC.cluster4&quot; &quot;logFC.cluster5&quot; &quot;logFC.cluster6&quot; ## [10] &quot;logFC.cluster7&quot; &quot;logFC.cluster8&quot; &quot;logFC.cluster10&quot; ## [13] &quot;logFC.cluster11&quot; &quot;logFC.cluster12&quot; head(interesting.voom[,1:3]) ## DataFrame with 6 rows and 3 columns ## Top p.value FDR ## &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt; ## PDZK1IP1 1 0 0 ## RGS18 1 0 0 ## SLC40A1 1 0 0 ## SDPR 1 0 0 ## PF4 1 0 0 ## PPBP 1 0 0 By default, we do not use custom DE methods to perform marker detection, for several reasons. Many of these methods rely on empirical Bayes shrinkage to share information across genes in the presence of limited replication. However, this is unnecessary when one treats the many cells in a group as “replicates” of each other (see Section ???). These methods also make stronger assumptions about the data (e.g., equal variances for linear models, the distribution of variances during empirical Bayes) that are more likely to be violated in noisy scRNA-seq contexts. From a practical perspective, they require more work to set up and take more time to run. Nonetheless, some custom methods (e.g., MAST) may provide a useful point of difference from the simpler tests, in which case they can be converted into a marker detection scheme as described above. 11.4 Handling blocking factors 11.5 Using the block= argument Large studies may contain factors of variation that are known and not interesting (e.g., batch effects, sex differences). If these are not modelled, they can interfere with marker gene detection - most obviously by inflating the variance within each cluster, but also by distorting the log-fold changes if the cluster composition varies across levels of the blocking factor. To avoid these issues, we set the block= argument in the findMarkers() call, as demonstrated below for the 416B data set. View history ### loading ### library(scRNAseq) sce.416b &lt;- LunSpikeInData(which=&quot;416b&quot;) ### gene-annotation ### library(org.Mm.eg.db) symb &lt;- mapIds(org.Mm.eg.db, keys=rownames(sce.416b), keytype=&quot;ENSEMBL&quot;, column=&quot;SYMBOL&quot;) rowData(sce.416b)$ENSEMBL &lt;- rownames(sce.416b) rowData(sce.416b)$SYMBOL &lt;- symb library(scater) rownames(sce.416b) &lt;- uniquifyFeatureNames(rowData(sce.416b)$ENSEMBL, rowData(sce.416b)$SYMBOL) library(TxDb.Mmusculus.UCSC.mm10.ensGene) location &lt;- mapIds(TxDb.Mmusculus.UCSC.mm10.ensGene, keys=rowData(sce.416b)$ENSEMBL, column=&quot;CDSCHROM&quot;, keytype=&quot;GENEID&quot;) rowData(sce.416b)$CHR &lt;- location ### quality-control ### mito &lt;- which(rowData(sce.416b)$CHR==&quot;chrM&quot;) sce.416b &lt;- calculateQCMetrics(sce.416b, feature_controls=list(Mt=mito)) libsize.drop &lt;- isOutlier(sce.416b$total_counts, nmads=3, type=&quot;lower&quot;, log=TRUE, batch=sce.416b$PlateOnco) feature.drop &lt;- isOutlier(sce.416b$total_features_by_counts, nmads=3, type=&quot;lower&quot;, log=TRUE, batch=sce.416b$PlateOnco) spike.drop &lt;- isOutlier(sce.416b$pct_counts_ERCC, nmads=3, type=&quot;higher&quot;, batch=sce.416b$PlateOnco) keep &lt;- !(libsize.drop | feature.drop | spike.drop) sce.416b &lt;- sce.416b[,keep] ### normalization ### library(scran) sce.416b &lt;- computeSumFactors(sce.416b) sce.416b &lt;- computeSpikeFactors(sce.416b, general.use=FALSE) sce.416b &lt;- normalize(sce.416b) ### variance-modelling ### fit.416b &lt;- trendVar(sce.416b, block=sce.416b$Plate) dec.416b &lt;- decomposeVar(sce.416b, fit.416b) # Taking all of the genes with positive biological components. chosen.hvgs &lt;- rownames(dec.416b)[dec.416b$bio &gt; 0] ### batch-correction ### # Composition of the plates is expected to be the same, # hence the use of a simple &#39;removeBatchEffect&#39;. library(limma) assay(sce.416b, &quot;corrected&quot;) &lt;- removeBatchEffect(logcounts(sce.416b), design=model.matrix(~sce.416b$phenotype), batch=sce.416b$Plate) ### dimensionality-reduction ### sce.416b &lt;- denoisePCA(sce.416b, technical=dec.416b, subset.row=chosen.hvgs, assay.type=&quot;corrected&quot;) set.seed(1010) sce.416b &lt;- runTSNE(sce.416b, use_dimred=&quot;PCA&quot;, perplexity=10) ### clustering ### my.dist &lt;- dist(reducedDim(sce.416b, &quot;PCA&quot;)) my.tree &lt;- hclust(my.dist, method=&quot;ward.D2&quot;) library(dynamicTreeCut) my.clusters &lt;- unname(cutreeDynamic(my.tree, distM=as.matrix(my.dist), minClusterSize=10, verbose=0)) sce.416b$cluster &lt;- factor(my.clusters) m.out &lt;- findMarkers(sce.416b, sce.416b$cluster, block=sce.416b$block, direction=&quot;up&quot;) For each gene, each pairwise comparion between clusters is performed separately in each level of the blocking factor - in this case, the plate of origin. The function will then combine \\(p\\)-values from different plates using Stouffer’s Z method to obtain a single \\(p\\)-value per pairwise comparison. (These \\(p\\)-values are further combined across comparisons to obtain a single \\(p\\)-value per gene, using either Simes’ method or an intersection-union test depending on the value of pval.type=.) This approach favours genes that exhibit consistent DE in the same direction in each plate. demo &lt;- m.out[[&quot;1&quot;]] demo[demo$Top &lt;= 5,1:3] ## NULL The block= argument works with all tests shown above and is robust to difference in the log-fold changes or variance between batches. However, it assumes that each pair of clusters is present in at least one batch. In scenarios where cells from two clusters never co-occur in the same batch, the comparison will be impossible and NAs will be reported in the output. 11.6 Using the design= argument Another approach is to define a design matrix containing the batch of origin as the sole factor. findMarkers() will then fit a linear model to the log-expression values, similar to the use of limma for bulk RNA sequencing data (Ritchie et al. 2015). This handles situations where multiple batches contain unique clusters, as comparisons can be implicitly performed via shared cell types in each batch. There is also a slight increase in power when information is shared across clusters for variance estimation. # Setting up the design matrix (we remove intercept for full rank # in the final design matrix with the cluster-specific terms). design &lt;- model.matrix(~sce.416b$block) design &lt;- design[,-1,drop=FALSE] m.alt &lt;- findMarkers(sce.416b, sce.416b$cluster, design=design, direction=&quot;up&quot;) demo &lt;- m.alt[[&quot;1&quot;]] demo[demo$Top &lt;= 5,1:3] ## DataFrame with 18 rows and 3 columns ## Top p.value FDR ## &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt; ## ENSMUSG00000081664 1 1.91285355952216e-41 8.91466272879706e-37 ## Pmf1 1 1.12621582896182e-36 1.3121540623234e-32 ## Ccna2 1 1.4320290224626e-35 1.11230467604745e-31 ## Hbb-bt 1 1.144389533167e-28 1.90475463584696e-25 ## ENSMUSG00000082610 1 9.85538881311309e-27 1.17769369293929e-23 ## ... ... ... ... ## Tuba8 4 1.12209603758146e-17 3.09432921511517e-15 ## Top2a 5 2.69761684182245e-36 2.51439470592585e-32 ## ENSMUSG00000084220 5 1.41054844370574e-29 2.98805453047556e-26 ## Mfsd2b 5 1.77368012038614e-28 2.85036511484399e-25 ## ENSMUSG00000084340 5 3.14925058189259e-13 4.0882360478697e-11 The use of a linear model makes some strong assumptions, necessitating some caution when interpreting the results. If the batch effect is not consistent across clusters, the variance will be inflated and the log-fold change estimates will be distorted. Variances are also assumed to be equal across groups, which is not true in general. In particular, the presence of clusters in which a gene is silent will shrink the residual variance towards zero, preventing the model from penalizing genes with high variance in other clusters. Thus, we generally recommend the use of block= where possible. 11.7 Invalidity of \\(p\\)-values 11.7.1 From data snooping All of our DE strategies for detecting marker genes between clusters are statistically flawed to some extent. The DE analysis is performed on the same data used to obtain the clusters, which represents “data dredging” (also known as fishing or data snooping). The hypothesis of interest - that are there differences between clusters? - is formulated from the data, so we are more likely to get a positive result when we re-use the data set to test that hypothesis. The practical effect of data dredging is best illustrated with a simple simulation. We simulate i.i.d. normal values, perform \\(k\\)-means clustering and test for DE between clusters of cells with findMarkers(). The resulting distribution of \\(p\\)-values is heavily skewed towards low values (Figure 11.6). Thus, we can detect “significant” differences between clusters even in the absence of any real substructure in the data. This effect arises from the fact that clustering, by definition, yields groups of cells that are separated in expression space. Testing for DE genes between clusters will inevitably yield some significant results as that is how the clusters were defined. library(scran) set.seed(0) y &lt;- matrix(rnorm(100000), ncol=200) clusters &lt;- kmeans(t(y), centers=2)$cluster out &lt;- findMarkers(y, clusters) hist(out[[1]]$p.value, col=&quot;grey80&quot;, xlab=&quot;p-value&quot;) Figure 11.6: Distribution of \\(p\\)-values from a DE analysis between two clusters in a simulation with no true subpopulation structure. For marker gene detection, this effect is largely harmless as the \\(p\\)-values are used only for ranking. However, it becomes an issue when the \\(p\\)-values are used to define “significant differences” between clusters with respect to an error rate threshold. Meaningful interpretation of error rates require consideration of the long-run behaviour, i.e., the rate of incorrect rejections if the experiment were repeated many times. The concept of statistical significance for differences between clusters is not applicable if clusters and their interpretations are not stably reproducible across (hypothetical) replicate experiments. 11.7.2 Nature of replication The naive application of DE analysis methods will treat counts from the same cluster of cells as replicate observations. This is not the most relevant level of replication when cells are derived from the same biological sample (i.e., cell culture, animal or patient). DE analyses that treat cells as replicates fail to properly model the sample-to-sample variability (A. T. L. Lun and Marioni 2017). The latter is arguably the more important level of replication as different samples will necessarily be generated if the experiment is to be replicated. Indeed, the use of cells as replicates only masks the fact that the sample size is actually one in an experiment involving a single biological sample. This reinforces the inappropriateness of using the marker gene \\(p\\)-values to make statements about statistical inference. 11.8 Session Info ## &lt;button class=&quot;aaron-collapse&quot;&gt;View session info&lt;/button&gt; ## &lt;div class=&quot;aaron-content&quot;&gt; ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.6 LTS ## ## Matrix products: default ## BLAS/LAPACK: /app/easybuild/software/OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1/lib/libopenblas_prescottp-r0.2.18.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats4 parallel stats graphics grDevices utils datasets ## [8] methods base ## ## other attached packages: ## [1] limma_3.41.6 scater_1.13.9 ## [3] ggplot2_3.2.0 pheatmap_1.0.12 ## [5] scran_1.13.9 SingleCellExperiment_1.5.3 ## [7] SummarizedExperiment_1.15.5 DelayedArray_0.11.4 ## [9] BiocParallel_1.19.0 matrixStats_0.54.0 ## [11] Biobase_2.45.0 GenomicRanges_1.37.14 ## [13] GenomeInfoDb_1.21.1 IRanges_2.19.10 ## [15] S4Vectors_0.23.17 BiocGenerics_0.31.5 ## [17] BiocStyle_2.13.2 Cairo_1.5-10 ## ## loaded via a namespace (and not attached): ## [1] viridis_0.5.1 dynamicTreeCut_1.63-1 ## [3] edgeR_3.27.6 BiocSingular_1.1.5 ## [5] viridisLite_0.3.0 DelayedMatrixStats_1.7.1 ## [7] assertthat_0.2.1 statmod_1.4.32 ## [9] BiocManager_1.30.4 dqrng_0.2.1 ## [11] GenomeInfoDbData_1.2.1 vipor_0.4.5 ## [13] yaml_2.2.0 pillar_1.4.2 ## [15] lattice_0.20-38 glue_1.3.1 ## [17] digest_0.6.20 RColorBrewer_1.1-2 ## [19] XVector_0.25.0 colorspace_1.4-1 ## [21] htmltools_0.3.6 Matrix_1.2-17 ## [23] pkgconfig_2.0.2 bookdown_0.12 ## [25] zlibbioc_1.31.0 purrr_0.3.2 ## [27] scales_1.0.0 tibble_2.1.3 ## [29] withr_2.1.2 lazyeval_0.2.2 ## [31] magrittr_1.5 crayon_1.3.4 ## [33] evaluate_0.14 beeswarm_0.2.3 ## [35] tools_3.6.0 stringr_1.4.0 ## [37] munsell_0.5.0 locfit_1.5-9.1 ## [39] irlba_2.3.3 compiler_3.6.0 ## [41] rsvd_1.0.1 rlang_0.4.0 ## [43] grid_3.6.0 RCurl_1.95-4.12 ## [45] BiocNeighbors_1.3.2 igraph_1.2.4.1 ## [47] bitops_1.0-6 rmarkdown_1.14 ## [49] gtable_0.3.0 R6_2.4.0 ## [51] gridExtra_2.3 knitr_1.23 ## [53] dplyr_0.8.3 stringi_1.4.3 ## [55] ggbeeswarm_0.6.0 Rcpp_1.0.1 ## [57] tidyselect_0.2.5 xfun_0.8 ## &lt;/div&gt; References "],
["integrating-datasets-1.html", "Chapter 12 Integrating Datasets 12.1 Preprocessing 12.2 Fast MNN 12.3 Harmony 12.4 Advanced MNN Workflow 12.5 Naive Method Without Correction 12.6 Limma Batch Correction 12.7 Session Info", " Chapter 12 Integrating Datasets .aaron-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .aaron-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } As scRNA-seq approaches continue to gain in popularity, projects that combine independently generated datasets will become increasingly common. While generalized linear modeling frameworks can be used to integrate disparate datasets, the performance of these frameworks in the scRNA-seq context may be sub-optimal, often due to an underlying assumption that the composition of cell populations is either known or identical across batches of cells. This has inspired a new class of methods that leverage the unique properties of scRNA-seq to improve the amelioration of batch effects, which we demonstrate here. Here we will work with two PBMC datasets from the TENxPBMCData package - the pbmc3k and pbmc4k datasets - that come from different donors, and integrate them to produce an integrated representation of the data. 12.1 Preprocessing Before proceeding to the integration section, we will first load the data, and subsequently run it through a preprocessing pipeline that ensures we have a common set of genes across both experiments, filters out poor quality cells, and normalizes the expression matrix. The code for this is shown below for completeness. Note that here we make use of the purrr package both for code succinctness and to apply the same function to each PBMC dataset contained within the pbmc_l list. (see base lapply() and purrrr::map() for more details). Furthermore, to speed up computation, we will subsample each of these datasets down to 1000 cells each. ## Load the data library(TENxPBMCData) pbmc3k &lt;- TENxPBMCData(&#39;pbmc3k&#39;) pbmc4k &lt;- TENxPBMCData(&#39;pbmc4k&#39;) pbmc_l &lt;- list(pbmc3k = pbmc3k, pbmc4k = pbmc4k) ## Subsample data to 1000 cells each library(purrr) set.seed(1234) pbmc_l &lt;- map(pbmc_l, function(x) { x[, sample(ncol(x), 1000)] }) ## Retain common genes (rows); reorder rows by common genes common_genes &lt;- reduce(map(pbmc_l, rownames), intersect) pbmc_l &lt;- map(pbmc_l, function(x) { x[common_genes, ] }) ## Save rowData from first object ## remove from each individual object else `cbind` will throw an error rd &lt;- rowData(pbmc_l[[1]]) pbmc_l &lt;- map(pbmc_l, function(x) { rowData(x) &lt;- NULL return(x) }) ## Combine the objects via `cbind` sce &lt;- reduce(pbmc_l, cbind) ## Replace rowData within combined object ## Uniquify row ids to use mostly human readable rownames rowData(sce) &lt;- rd rownames(sce) &lt;- scater::uniquifyFeatureNames(rd$ENSEMBL_ID, rd$Symbol_TENx) ## Calculate and apply QC metrics; filter out cells with low lib or low genes detected library(scater) sce &lt;- calculateQCMetrics(sce) low_lib &lt;- isOutlier(sce$log10_total_counts, type = &quot;lower&quot;, nmad = 3) low_genes &lt;- isOutlier(sce$log10_total_features_by_counts, type = &quot;lower&quot;, nmad = 3) sce &lt;- sce[, !(low_lib | low_genes)] ## normalize data sce &lt;- normalize(sce) ## feature selection library(scran) fit &lt;- trendVar(sce, use.spikes = FALSE) dec &lt;- decomposeVar(sce, fit) hvg &lt;- rownames(dec)[dec$bio &gt; 0] # save gene names For faster calculations, here we will subset the hvg object to the top 2000 genes: hvg &lt;- hvg[1:2000] From the combined dataset encapsulated within sce, we note that the origin of each cell is annotated within colData under the column Sample. This will be used as the batch variable throughout. table(sce$Sample) # alternately: colData(sce)$Sample ## ## pbmc3k pbmc4k ## 990 1000 12.2 Fast MNN The batchelor package provides the fastMNN() function, which calculates a dimension reduced representation based on the original principal components derived directly from the normalized expression data. These MNN coordinates can then be used for further dimensionality reduction via UMAP as shown below. library(batchelor) ## create new object from sce for mnn simple workflow sce_mnn_simple &lt;- sce ## run MNN with no advanced preprocessing ## BSPARAM/BNPARAM/BPPARAM explained in &quot;adaptations to large scale data&quot; mnn_simple_out &lt;- batchelor::fastMNN(sce_mnn_simple, subset.row = hvg, batch = sce_mnn_simple$Sample) ## BSPARAM = BiocSingular::IrlbaParam(), ## BNPARAM = BiocNeighbors::AnnoyParam()) ## add MNN_simple coordinates to the new sce object reducedDim(sce_mnn_simple, &#39;MNN&#39;) &lt;- reducedDim(mnn_simple_out, &#39;corrected&#39;) ## run UMAP on the &quot;simple MNN&quot; version sce_mnn_simple &lt;- runUMAP(sce_mnn_simple, use_dimred = &#39;MNN&#39;) library(patchwork) p1 &lt;- plotReducedDim(sce_mnn_simple, &#39;MNN&#39;, colour_by = &#39;Sample&#39;) p2 &lt;- plotReducedDim(sce_mnn_simple, &#39;UMAP&#39;, colour_by = &#39;Sample&#39;) wrap_plots(p1, p2, nrow = 1) Figure 7.1: Simple MNN method. First two MNN coordinates (left) and UMAP derived from the full MNN coordinate set (right). 12.3 Harmony Harmony provides a unified framework for data visualization, analysis, and interpretation of scRNA-seq data. Included is a method for integrating different batches of scRNA-seq. This produces a new dimension reduction representation, saved as HARMONY within the SingleCellExperiment object, as shown below. Furthermore, this Harmony embedding can be used to produce a UMAP representation as well: library(harmony) # devtools::install_github(&#39;immunogenomics/harmony&#39;) library(BiocSingular) ## create a new sce for harmony sce_harmony &lt;- sce ## Add PCA to sce, then run Harmony as its dependent on PCA sce_harmony &lt;- runPCA(sce_harmony, feature_set = hvg, BSPARAM = BiocSingular::IrlbaParam(), BPPARAM = BiocParallel::MulticoreParam()) sce_harmony &lt;- RunHarmony(sce_harmony, group.by.vars = &quot;Sample&quot;) ## Calculate UMAP on Harmony embeddings sce_harmony &lt;- runUMAP(sce_harmony, use_dimred = &#39;HARMONY&#39;) p1 &lt;- plotReducedDim(sce_harmony, &#39;HARMONY&#39;, colour_by = &#39;Sample&#39;) p2 &lt;- plotReducedDim(sce_harmony, &#39;UMAP&#39;, colour_by = &#39;Sample&#39;) wrap_plots(p1, p2, nrow = 1) Figure 12.1: Plot of the first two components of the Harmony embedding (left). UMAP calculated on the Harmony embeddings. (right) 12.4 Advanced MNN Workflow Here we work through a more advanced workflow using MNN as above, but this time walking through the various steps that are otherwise implicit within the fastMNN() function. While not always necessary, it may be informative for those interested in the finer details of a batch correction workflow. Note that this follows the For more details, we refer to the compareSingleCell vignette vignette by Aaron Lun. ## create a new sce for advanced mnn sce_mnn_advanced &lt;- sce 12.4.1 Dimensionality Reduction Principal components analyses can be used to both reduce computational work and remove high-dimensional noise, as discussed in the simpleSingleCell vignette. To accomplish this, the multiBatchPCA() function from scran ensures that each sample contributes equally to the new coordinate space. For further discussion on this point, see the simpleSingleCell vignette section on hierarchical merging. We then calculate the number of PCs to retain by comparing the variance explained per principal component versus the technical noise and total variance in the data. This will leave us with a subset of PCA components. Note that we will use the features selected from the preproprocessing step earlier in this chapter. library(batchelor) library(BiocSingular) # provide approximate pca via IrlbaParam() ## Calculate PCA by batch set.seed(1234) # for Random/IrlbaParam backends ## run batchelor version of multiBatchPCA (use :: notation to override namespace) pcs &lt;- batchelor::multiBatchPCA(sce_mnn_advanced, batch = sce_mnn_advanced$Sample, subset.row = hvg, get.variance = TRUE, BSPARAM = BiocSingular::IrlbaParam(), BPPARAM = BiocParallel::MulticoreParam()) ## Retain only the top PCs based on variance explained &gt; technical noise retain &lt;- denoisePCANumber( metadata(pcs)$var.explained, # variance explained per PC. sum(dec[hvg, ]$tech), # technical noise in subset of genes. metadata(pcs)$var.total # total variance in the data ) retain ## [1] 3 From the number of PCs to retain, we can filter the PCs as follows: ## Subset PCA number of components up to the number in retain top_pcs &lt;- map(as.list(pcs), ~ .[, 1:retain]) Then, using the retained components as input, we can input them directly into the fastMNN() method as follows, and furthermore produce a UMAP representation based on these MNN coordinates: ## Calculate MNN coordinates, add into reducedDims mnn_advanced_out &lt;- batchelor::fastMNN(top_pcs$pbmc3k, top_pcs$pbmc4k, pc.input = TRUE) reducedDim(sce_mnn_advanced, &#39;MNN&#39;) &lt;- mnn_advanced_out$corrected ## Run UMAP on MNN coordinates sce_mnn_advanced &lt;- runUMAP(sce_mnn_advanced, use_dimred = &#39;MNN&#39;) The MNN coordinates and the UMAP coordinates derived from them are visualized below: p1 &lt;- plotReducedDim(sce_mnn_advanced, &#39;MNN&#39;, colour_by = &#39;Sample&#39;) p2 &lt;- plotReducedDim(sce_mnn_advanced, &#39;UMAP&#39;, colour_by = &#39;Sample&#39;) wrap_plots(p1, p2, nrow = 1) Figure 12.2: Plot of first two MNN coordinates (left). UMAP calculated on the MNN coordinates (right). Colour denotes origin of sample. Comparing to the simpler MNN workflow shown above (which only used the fastMNN() function on the barely processed SingleCellExperiment class object sce), we see that the two results are fairly comparable, differing only by the number of principal components and the selected number of features (here we chose 1000 for computational efficacy; we leave it to the interested reader to explore the results with a larger feature space). 12.5 Naive Method Without Correction For completeness, we show here the results of performing the correction solely on the gene expression matrix. ## create a new sce for naive method sce_naive &lt;- sce ## run PCA on only the normalized gene expression data sce_naive &lt;- runPCA(sce_naive, feature_set = hvg, BSPARAM = BiocSingular::IrlbaParam(), BPPARAM = BiocParallel::MulticoreParam()) ## Run UMAP on the norm gene exp. derived PCA coordinates - use only top PCs sce_naive &lt;- runUMAP(sce_naive, use_dimred = &#39;PCA&#39;) p1 &lt;- plotPCA(sce_naive, colour_by = &#39;Sample&#39;) p2 &lt;- plotReducedDim(sce_naive, &#39;UMAP&#39;, colour_by = &#39;Sample&#39;) wrap_plots(p1, p2, nrow = 1) Figure 12.3: First two components of PCA calculated directly from the gene expression matrix (left). UMAP calculated on the PCA derived from the normalized gene expression matrix (right). While the PCA looks well integrated, the UMAP representation - which encapsulates a fuller PCA space - distinctly shows the residual batch effect present. 12.6 Limma Batch Correction The limma package, a popular framework for the statistical analysis of RNA-seq, has a function removeBatchEffect() which will be used here to correct the normalized expression matrix logcounts across the two batches. The result will be assigned into the assays slot of the sce object as limma_corrected, and then used for PCA, saving the result in the reducedDim slot as &quot;PCA_limma&quot;. ## create a new sce for limma method sce_limma &lt;- sce library(limma) limma_corrected &lt;- removeBatchEffect(logcounts(sce_limma), batch = sce_limma$Sample) assay(sce_limma, &quot;logcounts_limma&quot;) &lt;- limma_corrected ## add new assay ## calc a PCA on limma values; save separate to prevent overwriting sce_limma &lt;- runPCA(sce_limma, feature_set = hvg, exprs_values = &quot;logcounts_limma&quot;, BSPARAM = BiocSingular::IrlbaParam(), BPPARAM = BiocParallel::MulticoreParam()) ## run UMAP; save separate to prevent overwriting sce_limma &lt;- runUMAP(sce_limma, use_dimred = &#39;PCA&#39;) The resulting PCA and UMAP from the limma batch correction method are shown below: p1 &lt;- plotReducedDim(sce_limma, &#39;PCA&#39;, colour_by = &#39;Sample&#39;) p2 &lt;- plotReducedDim(sce_limma, &#39;UMAP&#39;, colour_by = &#39;Sample&#39;) wrap_plots(p1, p2, nrow = 1) Figure 12.4: PCA calculated on the limma batch-corrected gene expression matrix (left). UMAP calculated via the PCA derived from limma batch-corrected gene expression matrix (right). 12.7 Session Info ## &lt;button class=&quot;aaron-collapse&quot;&gt;View session info&lt;/button&gt; ## &lt;div class=&quot;aaron-content&quot;&gt; ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.6 LTS ## ## Matrix products: default ## BLAS/LAPACK: /app/easybuild/software/OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1/lib/libopenblas_prescottp-r0.2.18.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats4 stats graphics grDevices utils datasets ## [8] methods base ## ## other attached packages: ## [1] limma_3.41.6 BiocSingular_1.1.5 ## [3] harmony_0.99.9 Rcpp_1.0.1 ## [5] patchwork_0.0.1 batchelor_1.1.4 ## [7] scran_1.13.9 scater_1.13.9 ## [9] ggplot2_3.2.0 purrr_0.3.2 ## [11] TENxPBMCData_1.3.0 HDF5Array_1.13.4 ## [13] rhdf5_2.29.0 SingleCellExperiment_1.5.3 ## [15] SummarizedExperiment_1.15.5 DelayedArray_0.11.4 ## [17] BiocParallel_1.19.0 matrixStats_0.54.0 ## [19] Biobase_2.45.0 GenomicRanges_1.37.14 ## [21] GenomeInfoDb_1.21.1 IRanges_2.19.10 ## [23] S4Vectors_0.23.17 BiocGenerics_0.31.5 ## [25] BiocStyle_2.13.2 Cairo_1.5-10 ## ## loaded via a namespace (and not attached): ## [1] bitops_1.0-6 bit64_0.9-7 ## [3] httr_1.4.0 dynamicTreeCut_1.63-1 ## [5] tools_3.6.0 backports_1.1.4 ## [7] R6_2.4.0 irlba_2.3.3 ## [9] vipor_0.4.5 DBI_1.0.0 ## [11] lazyeval_0.2.2 colorspace_1.4-1 ## [13] withr_2.1.2 tidyselect_0.2.5 ## [15] gridExtra_2.3 bit_1.1-14 ## [17] curl_4.0 compiler_3.6.0 ## [19] BiocNeighbors_1.3.2 bookdown_0.12 ## [21] scales_1.0.0 rappdirs_0.3.1 ## [23] stringr_1.4.0 digest_0.6.20 ## [25] rmarkdown_1.14 XVector_0.25.0 ## [27] pkgconfig_2.0.2 htmltools_0.3.6 ## [29] dbplyr_1.4.2 rlang_0.4.0 ## [31] RSQLite_2.1.1 shiny_1.3.2 ## [33] DelayedMatrixStats_1.7.1 dplyr_0.8.3 ## [35] RCurl_1.95-4.12 magrittr_1.5 ## [37] GenomeInfoDbData_1.2.1 Matrix_1.2-17 ## [39] ggbeeswarm_0.6.0 munsell_0.5.0 ## [41] Rhdf5lib_1.7.2 viridis_0.5.1 ## [43] edgeR_3.27.6 stringi_1.4.3 ## [45] yaml_2.2.0 zlibbioc_1.31.0 ## [47] BiocFileCache_1.9.1 AnnotationHub_2.17.3 ## [49] grid_3.6.0 blob_1.2.0 ## [51] dqrng_0.2.1 promises_1.0.1 ## [53] ExperimentHub_1.11.1 crayon_1.3.4 ## [55] lattice_0.20-38 cowplot_1.0.0 ## [57] locfit_1.5-9.1 zeallot_0.1.0 ## [59] knitr_1.23 pillar_1.4.2 ## [61] igraph_1.2.4.1 codetools_0.2-16 ## [63] glue_1.3.1 evaluate_0.14 ## [65] BiocManager_1.30.4 vctrs_0.2.0 ## [67] httpuv_1.5.1 gtable_0.3.0 ## [69] assertthat_0.2.1 xfun_0.8 ## [71] rsvd_1.0.1 mime_0.7 ## [73] xtable_1.8-4 later_0.8.0 ## [75] viridisLite_0.3.0 tibble_2.1.3 ## [77] AnnotationDbi_1.47.0 beeswarm_0.2.3 ## [79] memoise_1.1.0 statmod_1.4.32 ## [81] interactiveDisplayBase_1.23.0 ## &lt;/div&gt; "],
["annotation-2.html", "Chapter 13 Annotation", " Chapter 13 Annotation WIP "],
["sharing-and-interactive-interfaces.html", "Chapter 14 Sharing and Interactive Interfaces 14.1 Session Info", " Chapter 14 Sharing and Interactive Interfaces .aaron-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .aaron-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } Overview of the iSEE interactive data visualization package’s relationship to a SingleCellExperiment object WIP 14.1 Session Info ## &lt;button class=&quot;aaron-collapse&quot;&gt;View session info&lt;/button&gt; ## &lt;div class=&quot;aaron-content&quot;&gt; ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.6 LTS ## ## Matrix products: default ## BLAS/LAPACK: /app/easybuild/software/OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1/lib/libopenblas_prescottp-r0.2.18.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] BiocStyle_2.13.2 Cairo_1.5-10 ## ## loaded via a namespace (and not attached): ## [1] BiocManager_1.30.4 compiler_3.6.0 magrittr_1.5 ## [4] bookdown_0.12 tools_3.6.0 htmltools_0.3.6 ## [7] yaml_2.2.0 Rcpp_1.0.1 stringi_1.4.3 ## [10] rmarkdown_1.14 knitr_1.23 stringr_1.4.0 ## [13] xfun_0.8 digest_0.6.20 evaluate_0.14 ## &lt;/div&gt; "],
["trajectory-analysis-1.html", "Chapter 15 Trajectory Analysis 15.1 Session Info", " Chapter 15 Trajectory Analysis .aaron-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .aaron-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } WIP 15.1 Session Info ## &lt;button class=&quot;aaron-collapse&quot;&gt;View session info&lt;/button&gt; ## &lt;div class=&quot;aaron-content&quot;&gt; ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.6 LTS ## ## Matrix products: default ## BLAS/LAPACK: /app/easybuild/software/OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1/lib/libopenblas_prescottp-r0.2.18.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] BiocStyle_2.13.2 Cairo_1.5-10 ## ## loaded via a namespace (and not attached): ## [1] BiocManager_1.30.4 compiler_3.6.0 magrittr_1.5 ## [4] bookdown_0.12 tools_3.6.0 htmltools_0.3.6 ## [7] yaml_2.2.0 Rcpp_1.0.1 stringi_1.4.3 ## [10] rmarkdown_1.14 knitr_1.23 stringr_1.4.0 ## [13] xfun_0.8 digest_0.6.20 evaluate_0.14 ## &lt;/div&gt; "],
["adaptations-for-large-scale-data.html", "Chapter 16 Adaptations for Large-scale Data 16.1 Approximate Methods 16.2 Parallelization 16.3 On-Disk Data 16.4 Session Info", " Chapter 16 Adaptations for Large-scale Data .aaron-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .aaron-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } Large datasets such as the Human Cell Atlas (with over 1.3 million cells) have the potential to benefit from special adaptations that enable analysis in compute-constrained environments (such as personal laptops). Here, we briefly cover topics that aim to ease working with scRNA-seq data to make it faster and more tractable. Do note however that these adaptations do not universally result in improved computational efficiency. For example, parallelization does incur costs, and disk-backed data representation will generally be slower than purely in-memory representations. In both cases, hardware specifics may dictate any potential gains in efficiency, as can be imagined in the case of a solid state drive (SSD) being faster for disk-backed data representations relative to a hard disk drive (HDD). Thus, with the right compute environment, data, and task, these adaptations can yield significant computational improvements. 16.1 Approximate Methods A general approach that works across all scales of data involves changing the task itself. Some methods - such as PCA or nearest neighbor searches - have been extended to include versions that provide approximate results. Generally, these adaptations result in acceptable losses in accuracy for significant computational gains. In some cases, approximate methods may even be desirable for the results themselves, as has been shown by the FIt-SNE approach. Some example packages that provide approximate versions of popular methods include: BiocSingular via the IrlbaParam() and RandomParam() for approximate singular value decomposition (SVD) BiocNeighbors via the AnnoyParam() for approximate nearest neighbor searches These packages provide users (and developers) a common interface, enabling modular swapping of key algorithms within functions. For example, we can see the immediate benefit of using an approximate method for PCA as provided by the BSPARAM argument in the following code, which utilizes the bench package for profiling: library(SingleCellExperiment) library(scater) ## Simulate a dataset with 1k genes and 1k cells mat &lt;- matrix(rpois(1e6, 100), nrow = 1000) tiny_sce &lt;- SingleCellExperiment(assays = list(counts = mat)) tiny_sce &lt;- normalize(tiny_sce) library(bench) library(BiocSingular) ## simple function to show only cols of interest .show_bench &lt;- function(b) { b[, c(&#39;expression&#39;, &#39;min&#39;, &#39;median&#39;, &#39;mem_alloc&#39;)] } bm &lt;- bench::mark( runPCA(tiny_sce, BSPARAM = IrlbaParam()), runPCA(tiny_sce, BSPARAM = ExactParam()), check = FALSE) .show_bench(bm) ## # A tibble: 2 x 4 ## expression min median mem_alloc ## &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt; &lt;bch:byt&gt; ## 1 runPCA(tiny_sce, BSPARAM = IrlbaParam()) 139ms 151ms 0B ## 2 runPCA(tiny_sce, BSPARAM = ExactParam()) 165ms 369ms 0B We can see that the approximate, irlba based implementation is about 6 times faster than the exact version in this case. We can also try this with nearest neighbor searches. Here we provide the BNPARAM argument to build a shared nearest neighbors graph using different algorithms under the hood: library(scran) library(BiocNeighbors) ## Calculate PCA before testing NN back-ends tiny_sce_pca &lt;- runPCA(tiny_sce, BSPARAM = IrlbaParam()) bm &lt;- bench::mark( buildSNNGraph(tiny_sce_pca, BNPARAM = AnnoyParam()), buildSNNGraph(tiny_sce_pca, BNPARAM = KmknnParam()), check = FALSE) .show_bench(bm) ## # A tibble: 2 x 4 ## expression min median ## &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt; ## 1 buildSNNGraph(tiny_sce_pca, BNPARAM = AnnoyParam()) 1.19s 1.19s ## 2 buildSNNGraph(tiny_sce_pca, BNPARAM = KmknnParam()) 801.83ms 801.83ms ## # … with 1 more variable: mem_alloc &lt;bch:byt&gt; We can see from the above benchmark that in our tiny dataset, we don’t see much, if any, benefit of using an approximate method (via the AnnoyParam(), which uses the Annoy library). However, if we increase our dataset to something larger.. ## Simulate a dataset with 1k genes and 10k cells mat &lt;- matrix(rpois(10e6, 100), nrow = 1000) big_sce &lt;- SingleCellExperiment(assays = list(counts = mat)) big_sce &lt;- normalize(big_sce) ## Calculate PCA before testing NN back-ends big_sce &lt;- runPCA(big_sce, BSPARAM = IrlbaParam()) ## NN search bm &lt;- bench::mark( buildSNNGraph(big_sce, BNPARAM = AnnoyParam()), buildSNNGraph(big_sce, BNPARAM = KmknnParam()), check = FALSE) .show_bench(bm) ## # A tibble: 2 x 4 ## expression min median mem_alloc ## &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:t&gt; &lt;bch:byt&gt; ## 1 buildSNNGraph(big_sce, BNPARAM = AnnoyParam()) 7.28s 7.28s 0B ## 2 buildSNNGraph(big_sce, BNPARAM = KmknnParam()) 12.32s 12.32s 0B We see (more consistently) that we have realized some gains from using the approximate-based nearest neighbors search. 16.2 Parallelization Many tasks that arise in the analysis of scRNA-seq data are able to be parallelized. In other words, the problem can be broken down into smaller pieces that can be solved independently. Parallel computing can be performed in computing environments with access to multiple cores. Bioconductor has reduced the complexity of implementing parallelized software via the BiocParallel package. This enables a common interface across Bioconductor software packages for parallel computing. Across many Bioconductor packages, an argument that will often be present in functions that are parallelizable is the BPPARAM argument. Here, it is possible to specify the parallel back-end that can be used for evaluation. Again, note that parallelization does incur an overhead cost in splitting up the data, sending it off, and combining the results, and thus your mileage may vary depending on the specifics. Below is an example usage of the BiocParallel library supplying BPPARAM argument for constructing the UMAP representation from the PCA results: library(BiocParallel) bm &lt;- bench::mark( runUMAP(tiny_sce_pca, BPPARAM = SerialParam()), runUMAP(tiny_sce_pca, BPPARAM = MulticoreParam()), check = FALSE) .show_bench(bm) ## # A tibble: 2 x 4 ## expression min median mem_alloc ## &lt;bch:expr&gt; &lt;bch:&gt; &lt;bch:&gt; &lt;bch:byt&gt; ## 1 runUMAP(tiny_sce_pca, BPPARAM = SerialParam()) 3.86s 3.86s 0B ## 2 runUMAP(tiny_sce_pca, BPPARAM = MulticoreParam()) 3.92s 3.92s 0B On this dataset and equipment, there’s not much, if any, benefit to parallelization, but your mileage may vary dependent on those aspects. 16.3 On-Disk Data The matrix of data from a single-cell experiment can be on the order of tens to hundreds of gigabytes, depending on the number of features and cells measured. A standard approach to import and represent these matrices has been to load the entire data set into memory using either the matrix object in base R or sparse and dense matrix classes from the Matrix R package. This approach however may prove intractable in computing environments with limited memory. Disk-backed representations such as HDF5 free us from having to load an entire dataset into memory, and thus make it possible to work with large-scale scRNA-seq data. library(rhdf5) library(HDF5Array) 16.4 Session Info ## &lt;button class=&quot;aaron-collapse&quot;&gt;View session info&lt;/button&gt; ## &lt;div class=&quot;aaron-content&quot;&gt; ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.6 LTS ## ## Matrix products: default ## BLAS/LAPACK: /app/easybuild/software/OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1/lib/libopenblas_prescottp-r0.2.18.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats4 parallel stats graphics grDevices utils datasets ## [8] methods base ## ## other attached packages: ## [1] HDF5Array_1.13.4 rhdf5_2.29.0 ## [3] BiocNeighbors_1.3.2 scran_1.13.9 ## [5] BiocSingular_1.1.5 bench_1.0.2 ## [7] scater_1.13.9 ggplot2_3.2.0 ## [9] SingleCellExperiment_1.5.3 SummarizedExperiment_1.15.5 ## [11] DelayedArray_0.11.4 BiocParallel_1.19.0 ## [13] matrixStats_0.54.0 Biobase_2.45.0 ## [15] GenomicRanges_1.37.14 GenomeInfoDb_1.21.1 ## [17] IRanges_2.19.10 S4Vectors_0.23.17 ## [19] BiocGenerics_0.31.5 BiocStyle_2.13.2 ## [21] Cairo_1.5-10 ## ## loaded via a namespace (and not attached): ## [1] viridis_0.5.1 dynamicTreeCut_1.63-1 ## [3] edgeR_3.27.6 viridisLite_0.3.0 ## [5] DelayedMatrixStats_1.7.1 assertthat_0.2.1 ## [7] statmod_1.4.32 BiocManager_1.30.4 ## [9] dqrng_0.2.1 GenomeInfoDbData_1.2.1 ## [11] vipor_0.4.5 yaml_2.2.0 ## [13] pillar_1.4.2 lattice_0.20-38 ## [15] glue_1.3.1 limma_3.41.6 ## [17] digest_0.6.20 XVector_0.25.0 ## [19] colorspace_1.4-1 htmltools_0.3.6 ## [21] Matrix_1.2-17 pkgconfig_2.0.2 ## [23] bookdown_0.12 zlibbioc_1.31.0 ## [25] purrr_0.3.2 scales_1.0.0 ## [27] tibble_2.1.3 withr_2.1.2 ## [29] lazyeval_0.2.2 magrittr_1.5 ## [31] crayon_1.3.4 evaluate_0.14 ## [33] beeswarm_0.2.3 tools_3.6.0 ## [35] stringr_1.4.0 Rhdf5lib_1.7.2 ## [37] munsell_0.5.0 locfit_1.5-9.1 ## [39] irlba_2.3.3 compiler_3.6.0 ## [41] rsvd_1.0.1 rlang_0.4.0 ## [43] grid_3.6.0 RCurl_1.95-4.12 ## [45] igraph_1.2.4.1 bitops_1.0-6 ## [47] rmarkdown_1.14 gtable_0.3.0 ## [49] R6_2.4.0 gridExtra_2.3 ## [51] knitr_1.23 dplyr_0.8.3 ## [53] stringi_1.4.3 ggbeeswarm_0.6.0 ## [55] Rcpp_1.0.1 tidyselect_0.2.5 ## [57] xfun_0.8 ## &lt;/div&gt; "],
["data-import-and-export.html", "Chapter 17 Data Import and Export 17.1 Session Info", " Chapter 17 Data Import and Export .aaron-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .aaron-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } WIP 17.1 Session Info ## &lt;button class=&quot;aaron-collapse&quot;&gt;View session info&lt;/button&gt; ## &lt;div class=&quot;aaron-content&quot;&gt; ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.6 LTS ## ## Matrix products: default ## BLAS/LAPACK: /app/easybuild/software/OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1/lib/libopenblas_prescottp-r0.2.18.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] BiocStyle_2.13.2 ## ## loaded via a namespace (and not attached): ## [1] BiocManager_1.30.4 compiler_3.6.0 magrittr_1.5 ## [4] bookdown_0.12 tools_3.6.0 htmltools_0.3.6 ## [7] yaml_2.2.0 Rcpp_1.0.1 stringi_1.4.3 ## [10] rmarkdown_1.14 knitr_1.23 stringr_1.4.0 ## [13] xfun_0.8 digest_0.6.20 evaluate_0.14 ## &lt;/div&gt; "],
["about-the-data.html", "Chapter 18 About the Data 18.1 10X Genomics PBMC Data 18.2 Cellbench_data", " Chapter 18 About the Data 18.1 10X Genomics PBMC Data The TENxPBMCData package provides a Bioconductor resource for representing and manipulating nine different single-cell RNA-seq (scRNA-seq) data sets on peripheral blood mononuclear cells (PBMC) generated by 10X Genomics from various healthy donors. Here we generally make use of the smaller datasets, pbmc3k and pbmc4k, available via the package. For more information, we refer to the 10X Genomics datasets online resource (PBMC data is within the Single Cell Gene Expression table). 18.2 Cellbench_data The 5 cell line scRNA-seq data from the CellBench_data repository is used in examples where having a cell label is important. In the CellBench_data repository, the data is stored within the workspace data/sincell_with_class_5cl.RData as sc_10x_5cl_qc. We have extracted it and saved it under this book’s Github repo within the _rfiles/_data folder for ease of use. The data arrives as a SingleCellExperiment class object, and has already undergone basic preprocessing, including quality control and normalization. The most important aspect to this data - the cell labels pertaining to the 5 cell lines sequenced - can be found within the colData component under the column name cell_line. Note that this data will be made available on Bioconductor via the scRNAseq package in the near future. "],
["about-the-contributors.html", "Chapter 19 About the Contributors", " Chapter 19 About the Contributors 19.0.1 Stephanie Hicks, PhD Stephanie Hicks is an Assistant Professor in the Department of Biostatistics at Johns Hopkins Bloomberg School of Public Health. Her research interests focus around developing statistical methods, tools and software for the analysis of genomics data. Specifically, her research addresses statistical challenges in epigenomics, functional genomics and single-cell genomics such as the pre-processing, normalization, analysis of noisy high-throughput data leading to an improved quantification and understanding of biological variability. She actively contributes software packages to Bioconductor and is involved in teaching courses for data science and the analysis of genomics data. She is also a faculty member of the Johns Hopkins Data Science Lab, co-host of The Corresponding Author podcast and co-founder of R-Ladies Baltimore. For more information, please see http://www.stephaniehicks.com 19.0.2 Raphael Gottardo, PhD Raphael Gottardo is the Scientific Director of the Translational Data Science Integrated Research Center (TDS IRC) at Fred Hutch, J. Ordin Edson Foundation Endowed Chair, and Full Member within the Vaccine and Infectious Disease and Public Health Sciences Division. A pioneer in developing and applying statistical methods and software tools to distill actionable insights from large and complex biological data sets.In partnership with scientists and clinicians, he works to understand such diseases as cancer, HIV, malaria, and tuberculosis and inform the development of vaccines and treatments. He is a leader in forming interdisciplinary collaborations across the Hutch, as well as nationally and internationally, to address important research questions, particularly in the areas of vaccine research, human immunology, and immunotherapy. As director of the Translational Data Science Integrated Research Center, he fosters interaction between the Hutch’s experimental and clinical researchers and their computational and quantitative science colleagues with the goal of transforming patient care through data-driven research. Dr. Gottardo partners closely with the cancer immunotherapy program at Fred Hutch to improve treatments. For example, his team is harnessing cutting-edge computational methods to determine how cancers evade immunotherapy. He has made significant contributions to vaccine research and is the principal investigator of the Vaccine and Immunology Statistical Center of the Collaboration for AIDS Vaccine Discovery. 19.0.3 Robert Amezquita, PhD Robert Amezquita is a Postdoctoral Fellow in the Immunotherapy Integrated Research Center (IIRC) at Fred Hutch under the mentorship of Raphael Gottardo. His current research focuses on utilizing computational approaches leveraging transcriptional and epigenomic profiling at single-cell resolution to understand how novel anti-cancer therapeutics - ranging from small molecule therapies to biologics such as CAR-T cells - affect immune response dynamics. Extending from his graduate work at Yale’s Dept. of Immunobiology, Robert’s research aims to better understand the process of immune cell differentiation under the duress of cancer as a means to inform future immunotherapies. To accomplish this, Robert works collaboratively across the Fred Hutch IIRC with experimental collaborators, extensively utilizing R and Bioconductor for data analysis. 19.0.4 Aaron Lun, PhD When one thinks of single-cell bioinformatics, one thinks of several titans who bestride the field. Unfortunately, they weren’t available, so we had to make do with Aaron instead. He likes long walks on the beach (as long as there’s Wifi) and travelling (but only in business class). His friends say that he is “absolutely insane” and “needs to get a life”, or they would if they weren’t mostly imaginary. His GitHub account is his Facebook and his Slack is his Twitter. He maintains more Bioconductor packages than he has phone numbers on his cell. He has a recurring event on his Google Calender to fold his laundry. He is - the most boring man in the world. (“I don’t often cry when I watch anime, but when I do, my tears taste like Dos Equis.”) He currently works as a Scientist at Genentech after a stint as a research associate in John Marioni’s group at the CRUK Cambridge Institute, which was preceded by a PhD with Gordon Smyth at the Walter and Eliza Hall Institute for Medical Research. "]
]
