[
["index.html", "Orchestrating Single-Cell Analysis with Bioconductor Welcome", " Orchestrating Single-Cell Analysis with Bioconductor Bioconductor 2019-02-28 Welcome This is the website for “Orchestrating Single-Cell Analysis with Bioconductor”, a book that teaches users some common workflows for the analysis of single-cell RNA-seq data (scRNA-seq). This book will teach you how to make use of cutting-edge Bioconductor tools to process, analyze, visualize, and explore scRNA-seq data. Additionally, it serves as an online companion for the manuscript “Orchestrating Single-Cell Analysis with Bioconductor”. While we focus here on scRNA-seq data, a newer technology that profiles transcriptomes at the single-cell level, many of the tools, conventions, and analysis strategies utilized throughout this book are broadly applicable to other types of assays. By learning the grammar of Bioconductor workflows, we hope to provide you a starting point for the exploration of your own data, whether it be scRNA-seq or otherwise. This book is organized into two parts. In the Preamble, we introduce the book and dive into resources for learning R and Bioconductor (both at a beginner and developer level). Part I ends with a tutorial for a key data infrastructure, the SingleCellExperiment class, that is used throughout Bioconductor for single-cell analysis and in the subsequent section. This section can be safely skipped by readers already familiar with R. The second part, Workflows, provides templates for performing single-cell RNA-seq analyses across various objectives. In these templates, we take various datasets from raw data through to preprocessing and finally to the objective at hand, using packages that are referred to in the main manuscript. The book is written in RMarkdown with bookdown. OSCA is a collaborative effort, supported by various folks from the Bioconductor team who have contributed workflows, fixes, and improvements. This website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License. "],
["introduction.html", "Chapter 1 Introduction 1.1 What you will learn 1.2 What you won’t learn 1.3 Who we wrote this for 1.4 Why we wrote this 1.5 Acknowledgements", " Chapter 1 Introduction Bioconductor is an open source, open development software project to provide tools for the analysis and comprehension of high-throughput genomic data. It is based primarily on the R programming language. 1.1 What you will learn The goal of this book is to provide a solid foundation in the usage of Bioconductor tools for single-cell RNA-seq analysis by walking through various steps of typical workflows using example datasets. We strive to tackle key concepts covered in the manuscript, “Orchestrating Single-Cell Analysis with Bioconductor”, with each workflow covering these in varying detail, as well as essential preliminaries that are important for following along with the workflows on your own. 1.1.1 Preliminaries For those unfamiliar with R (and those looking to learn more), we recommend reading the Learning R and More chapter, which first and foremost covers how to get started with R. We point to many great online resources for learning R, as well as related tools that are nice to know for bioinformatic analysis. For advanced users, we also point to some extra resources that go beyond the basics. While we provide an extensive list of learning resources for the interested audience in this chapter, we only ask for some familiarity with R before going to the next section. We then briefly cover getting started with Using R and Bioconductor. Bioconductor, being its own repository, has a unique set of tools, documentation, resources, and practices that benefit from some extra explanation. Data Infrastructure merits a separate chapter. The reason for this is that common data containers are an essential part of Bioconductor workflows because they enable interoperability across packages, allowing for “plug and play” usage of cutting-edge tools. Specifically, here we cover the SingleCellExperiment class in depth, as it has become the working standard for Bioconductor based single-cell analysis packages. Finally, before diving into the various workflows, armed with knowledge about the SingleCellExperiment class, we briefly discuss the datasets that will be used throughout the book in About the Data. 1.1.2 Workflows All workflows begin with data import and subsequent quality control and normalization, going from a raw (count) expression matrix to a clean one. This includes adjusting for experimental factors and possibly even latent factors. Using the clean expression matrix, feature selection strategies can be applied to select the features (genes) driving heterogeneity. Furthermore, these features can then be used to perform dimensionality reduction, which enables downstream analysis that would not otherwise be possible and visualization on 2- or 3-dimensions. From there, the workflows largely focus on differing downstream analyses. Clustering details how to segment a scRNA-seq dataset, and differential expression provides a means to determine what drives the differences between different groups of cells. Integrating datasets walks through merging scRNA-seq datasets, an area of need as the number of scRNA-seq datasets continues to grow and comparisons between datasets must be done. Finally, we touch upon how to work with large scale data, specifically where it becomes impractical or impossible to work with data solely in-memory. As an added bonus, we dedicate a chapter to interactive visualization, which focuses on using the iSEE package to enable active exploration of a single cell experiment’s data. 1.2 What you won’t learn The field of bioinformatic analysis is large and filled with many potential trajectories depending on the biological system being studied and technology being deployed. Here, we only briefly survey some of the many tools available for the analysis of scRNA-seq, focusing on Bioconductor packages. It is impossible to thoroughly review the plethora of tools available through R and Bioconductor for biological analysis in one book, but we hope to provide the means for further exploration on your own. Thus, it goes without saying that you may not learn the optimal workflow for your own data from our examples - while we strive to provide high quality templates, they should be treated as just that - a template from which to extend upon for your own analyses. 1.3 Who we wrote this for We’ve written this book with the interested experimental biologist in mind, and do our best to make few assumptions on previous programming or statistical experience. Likewise, we also welcome more seasoned bioinformaticians who are looking for a starting point from which to dive into single-cell RNA-seq analysis. As such, we welcome any and all feedback for improving this book to help increase accessibility and refine technical details. 1.4 Why we wrote this This book was conceived in the fall of 2018, as single-cell RNA-seq analysis continued its rise in prominence in the field of biology. With its rapid growth, and the ongoing developments within Bioconductor tailored specifically for scRNA-seq, it became apparent that an update to the Orchestrating high-throughput genomic analysis with Bioconductor paper was necessary for the age of single-cell studies. We strive to highlight the fantastic software by people who call Bioconductor home for their tools, and in the process hope to showcase the Bioconductor community at large in continually pushing forward the field of biological analysis. 1.5 Acknowledgements We would like to thank all Bioconductor contributors for their efforts in creating the definitive leading-edge repository of software for biological analysis. It is truly extraordinary to chart the growth of Bioconductor over the years. We are thankful for the wonderful community of scientists and developers alike that together make the Bioconductor community special. We would first and foremost like to thank the Bioconductor core team and the emerging targets subcommittee for commissioning this work, Raphael Gottardo for his continuous mentorship of the writing herein, and all our contributors to the companion manuscript of this book. We’d also like to thank Garret Grolemund and Hadley Wickham for their book, R for Data Science, from which we drew stylistic and teaching inspiration. "],
["learning-r-and-more.html", "Chapter 2 Learning R and Bioconductor 2.1 The Benefits of R and Bioconductor 2.2 Learning R Online 2.3 Running R Locally 2.4 Getting Help In (and Out) of R 2.5 Bioconductor Help", " Chapter 2 Learning R and Bioconductor In this section, we outline various resources for learning R and Bioconductor. We provide a brief set of instructions for installing R on your own machine, and then cover how to get help for functions, packages, and Bioconductor-specific resources for learning more. 2.1 The Benefits of R and Bioconductor R is a high-level programming language that was initially designed for statistical applications. While there is much to be said about R as a programming language, one of the key advantages of using R is that it is highly extensible through packages. Packages are collections of functions, data, and documentation that extends the capabilities of base R. The ease of development and distribution of packages for R has made it a rich environment for many fields of study and application. One of the primary ways in which packages are distributed is through centralized repositories. The first R repository a user typically runs into is the Comprehensive R Archive Network (CRAN), which hosts over 13,000 packages to date, and is home to many of the most popular R packages. Similar to CRAN, Bioconductor is a repository of R packages as well. However, whereas CRAN is a general purpose repository, Bioconductor focuses on software tailored for genomic analysis. Furthermore, Bioconductor has stricter requirements for a package to be accepted into the repository. Of particular interest to us is the inclusion of high quality documentation and the use of common data infrastructure to promote package interoperability. In order to use these packages from CRAN and Bioconductor, and start programming with R to follow along in these workflows, some knowledge of R is helpful. Here we outline resources to guide you through learning the basics. 2.2 Learning R Online To learn more about programming with R, we highly recommend checking out the online courses offered by Datacamp, which includes both introductory and advanced courses within the R track. Datacamp is all online with many free courses, with videos and a code editor/console that promotes an interactive learning experience. What we like about Datacamp is that it is more focused on topics and programming paradigms that center around data science, which is especially helpful for getting started with R. Beyond just Datacamp, a mainstay resource for learning R is the R for Data Science book. This book illustrates R programming through the exploration of various data science concepts - transformation, visualization, exploration, and more. 2.3 Running R Locally While learning R through online resources is a great way to start with R, as it requires minimal knowledge to start up, at some point, it will be desirable to have a local installation - on your own hardware - of R. This will allow you to install and maintain your own software and code, and furthermore allow you to create a personalized workspace. 2.3.1 Installing R Prior to getting started with this book, some prior programming experience with R is helpful. Check out the Learning R and More chapter for a list of resources to get started with R and other useful tools for bioinformatic analysis. To follow along with the analysis workflows in this book on your personal computer, it is first necessary to install the R programming language. Additionally, we recommend a graphical user interface such as RStudio for programming in R and visualization. RStudio features many helpful tools, such as code completion and an interactive data viewer to name but two. For more details, please see the online book R for Data Science prerequisites section for more information about installing R and using RStudio. 2.3.1.1 For MacOS/Linux Users A special note for MacOS/Linux users: we highly recommend using a package manager to manage your R installation. This differs across different Linux distributions, but for MacOS we highly recommend the Homebrew package manager. Follow the website directions to install homebrew, and install R via the commandline with brew install R, and it will automatically configure your installation for you. Upgrading to new R versions can be done by running brew upgrade. 2.3.2 Installing R &amp; Bioconductor Packages After installing R, the next step is to install R packages. In the R console, you can install packages from CRAN via the install.packages() function. In order to install Bioconductor packages, we will first need the BiocManager package which is hosted on CRAN. This can be done by running: install.packages(&quot;BiocManager&quot;) The BiocManager package makes it easy to install packages from the Bioconductor repository. For example, to install the SingleCellExperiment package, we run: ## the command below is a one-line shortcut for: ## library(BiocManager) ## install(&quot;SingleCellExperiment&quot;) BiocManager::install(&quot;SingleCellExperiment&quot;) Throughout the book, we can load packages via the library() function, which by convention usually comes at the top of scripts to alert readers as to what packages are required. For example, to load the SingleCellExperiment package, we run: library(SingleCellExperiment) Many packages will be referenced throughout the book within the workflows, and similar to the above, can be installed using the BiocManager::install() function. 2.4 Getting Help In (and Out) of R One of the most helpful parts of R is being able to get help inside of R. For example, to get the manual associated with a function, class, dataset, or package, you can prepend the code of interest with a ? to retrieve the relevant help page. For example, to get information about the data.frame() function, the SingleCellExperiment class, the in-built iris dataset, or for the BiocManager package, you can type: ?data.frame ?SingleCellExperiment ?iris ?BiocManager Beyond the R console, there are myriad online resources to get help. The R for Data Science book has a great section dedicated to looking for help outside of R. In particular, Stackoverflow’s R tag is a helpful resource for asking and exploring general R programming questions. 2.5 Bioconductor Help One of the key tenets of Bioconductor software that makes it stand out from CRAN is the required documentation of packages and workflows. In addition, Bioconductor hosts a Bioconductor-specific support site that has grown into a valuable resource of its own, thanks to the work of dedicated volunteers. 2.5.1 Bioconductor Packages Each package hosted on Bioconductor has a dedicated page with various resources. For an example, looking at the scater package page on Bioconductor, we see that it contains: a brief description of the package at the top, in addition to the authors, maintainer, and an associated citation installation instructions that can be cut and paste into your R console documentation - vignettes, reference manual, news Here, the most important information comes from the documentation section. Every package in Bioconductor is required to be submitted with a vignette - a document showcasing basic functionality of the package. Typically, these vignettes have a descriptive title that summarizes the main objective of the vignette. These vignettes are a great resource for learning how to operate the essential functionality of the package. The reference manual contains a comprehensive listing of all the functions available in the package. This is a compilation of each function’s manual, aka help pages, which can be accessed programmatically in the R console via ?&lt;function&gt;. Finally, the NEWS file contains notes from the authors which highlight changes across different versions of the package. This is a great way of tracking changes, especially functions that are added, removed, or deprecated, in order to keep your scripts current with new versions of dependent packages. Below this, the Details section covers finer nuances of the package, mostly relating to its relationship to other packages: upstream dependencies (Depends, Imports, Suggests fields): packages that are imported upon loading the given package downstream dependencies (Depends On Me, Imports Me, Suggests Me): packages that import the given package when loaded For example, we can see that an entry called simpleSingle in the Depends On Me field on the scater page takes us to a step-by-step workflow for low-level analysis of single-cell RNA-seq data. One additional Details entry, the biocViews, is helpful for looking at how the authors annotate their package. For example, for the scater package, we see that it is associated with DataImport, DimensionReduction, GeneExpression, RNASeq, and SingleCell, to name but some of its many annotations. We cover biocViews in more detail. 2.5.2 biocViews To find packages via the Bioconductor website, one useful resource is the BiocViews page, which provides a hierarchically organized view of annotations associated with Bioconductor packages. Under the “Software” label for example (which is comprised of most of the Bioconductor packages), there exist many different views to explore packages. For example, we can inspect based on the associated “Technology”, and explore “Sequencing” associated packages, and furthermore subset based on “RNASeq”. Another area of particular interest is the “Workflow” view, which provides Bioconductor packages that illustrate an analytical workflow. For example, the “SingleCellWorkflow” contains the aforementioned tutorial, encapsulated in the simpleSingleCell package. 2.5.3 Bioconductor Forums The Bioconductor support site contains a Stackoverflow-style question and answer support site that is actively contributed to from both users and package developers. Thanks to the work of dedicated volunteers, there are ample questions to explore to learn more about Bioconductor specific workflows. Another way to connect with the Bioconductor community is through Slack, which hosts various channels dedicated to packages and workflows. The Bioc-community Slack is a great way to stay in the loop on the latest developments happening across Bioconductor, and we recommend exploring the “Channels” section to find topics of interest. "],
["beyond-r-basics.html", "Chapter 3 Beyond R Basics 3.1 Becoming an R Expert 3.2 Becoming an R/Bioconductor Developer 3.3 Nice Companions for R", " Chapter 3 Beyond R Basics Here we briefly outline resources for taking your R programming to the next level, including resources for learning about package development. We also outline some companions to R that are good to know not only for package development, but also for running your own bioinformatic pipelines, enabling you to use a broader array of tools to go from raw data to preprocessed data before working in R. 3.1 Becoming an R Expert For a deeper dive into the finer details of the R programming language, the Advanced R. While targeted at more experienced R users and programmers, this book represents a comprehensive compendium of more advanced concepts, and touches on some of the paradigms used extensively by developers throughout Bioconductor, specifically programming with S4. Eventually, you’ll reach the point where you have your own collection of functions, datasets, and reach the point where you will be writing your own packages. Luckily, there’s a guide for just that, with the book R Packages. Packages are great even if just for personal use, and of course, with some polishing, can eventually become a package available on CRAN or Bioconductor. Furthermore, they are also a great way of putting together code associated with a manuscript, promoting reproducible, accessible computing practices, something we all strive for in our work. For many of the little details that are oft forgotten learning about R, the aptly named What They Forgot to Teach You About R is a great read for learning about the little things such as file naming, maintaining an R installation, and reproducible analysis habits. Finally, we save the most intriguing resource for last - another book for those on the road to becoming an R expert is R Inferno, which dives into many of the unique quirks of R. Warning: this book goes very deep into the painstaking details of R. 3.2 Becoming an R/Bioconductor Developer While learning to use Bioconductor tools is a very welcoming experience, unfortunately there is no central resource for navigating the plethora of gotchas and paradigms associated with developing for Bioconductor. Based on conversations with folks involved in developing for Bioconductor, much of this knowledge is hard won and fairly spread out. This however is beginning to change with more recent efforts led by the Bioconductor team, and while this book represents an earnest effort towards addressing the user perspective, it is currently out of scope to include a deep dive about the developer side. For those looking to get started with developing packages for Bioconductor, it is important to first become acquainted with developing standalone R packages. To this end, the R Packages book provides a deep dive into the details of constructing your own package, as well as details regarding submission of a package to CRAN. For programming practices, With that, some resources that are worth looking into to get started are the BiocWorkshops repository under the Bioconductor Github provides a book composed of workshops that have been hosted by Bioconductor team members and contributors. These workshops center around learning, using, and developing for Bioconductor. A host of topics are also available via the Learn module on the Bioconductor website as well. Finally, the Bioconductor developers portal contains a bevy of individual resources and guides for experienced R developers. 3.3 Nice Companions for R While not essential for our purposes, many bioinformatic tools for processing raw sequencing data require knowledge beyond just R to install, run, and import their results into R for further analysis. The most important of which are basic knowledge of the Shell/Bash utilities, for working with bioinformatic pipelines and troubleshooting (R package) installation issues. Additionally, for working with packages or software that are still in development and not hosted on an official repository like CRAN or Bioconductor, knowledge of Git - a version control system - and the popular Github repository is helpful. This enables you to not only work with other people’s code, but also better manage your own code to keep track of changes. 3.3.1 Shell/Bash Datacamp and other interactive online resources such as Codecademy are great places to learn some of these extra skills. We highly recommend learning Shell/Bash, as it is the starting point for most bioinformatic processing pipelines. 3.3.2 Git We would recommend learning Git next, a system for code versioning control which underlies the popular Github repository, where many of the most popular open source tools are hosted. Learning Git is essential for not only keeping track of your own code, but also for using, managing, and contributing to open source software projects. For a more R centric look at using Git (and Github), we highly recommend checking out Happy Git and Github for the useR. 3.3.3 Other Languages A frequent question that comes up is “What else should I learn besides R?” Firstly, we believe that honing your R skills is first and foremost, and beyond just R, learning Shell/Bash and Git covered in the Nice Friends for R section are already a great start. For those just getting started, these skills should become comfortable in practice before moving on. However, there are indeed benefits to going beyond just R. At a basic level, learning other programming languages helps broaden one’s perspective - similar to learning multiple spoken or written languages, learning about other programming languages (even if only in a cursory manner) helps one identify broader patterns that may be applicable across languages. At an applied level, work within and outside of R has made it ever more friendly now than ever before with multi-lingual setups and teams, enabling the use of the best tool for the job at hand. For example, Python is another popular language used in both data science and a broader array of applications as well. R now supports a native Python interface via the reticulate package, enabling access to tools developed originally in Python such as the popular TensorFlow framework for machine learning applications. C++ is frequently used natively in R as well via Rcpp in packages to massively accelerate computations. Finally, multiple langauges are supported in code documents and reports through R Markdown. "],
["data-infrastructure.html", "Chapter 4 Data Infrastructure 4.1 Prerequisites 4.2 The Essentials of sce 4.3 A Brief Recap: From se to sce 4.4 The reducedDims Slot 4.5 One More Thing: metadata Slot 4.6 About Spike-Ins 4.7 Working with SingleCellExperiment", " Chapter 4 Data Infrastructure One of the advantages of using Bioconductor packages is that they utilize common data infrastructures which makes analyses interoperable across various packages. Furthermore, much engineering effort is put into making this infrastructure robust and scalable. Here, we describe the SingleCellExperiment object (or sce in shorthand) in detail to describe how it is constructed, utilized in downstream analysis, and how it stores various types of primary data and metadata. 4.1 Prerequisites The Bioconductor package SingleCellExperiment provides the SingleCellExperiment class for usage. While the package is implicitly installed and loaded when using any package that depends on the sce class, it can be explicitly installed (and loaded) as follows: BiocManager::install(&#39;SingleCellExperiment&#39;) Additionally, we use some functions from the scater and scran packages, as well as the CRAN package uwot (which conveniently can also be installed through BiocManager). These functions will be accessed through the &lt;package&gt;::&lt;function&gt; convention as needed. BiocManager::install(c(&#39;scater&#39;, &#39;scran&#39;, &#39;uwot&#39;)) For this session, all we will need loaded is the SingleCellExperiment package: library(SingleCellExperiment) 4.2 The Essentials of sce Figure 1. Overview of the SingleCellExperiment object 4.2.1 Primary Data: The assays Slot The SingleCellExperiment (sce) object is the basis of single-cell analytical applications based in Bioconductor. The sce object is an S4 object, which in essence provides a more formalized approach towards construction and accession of data compared to other methods available in R. The utility of S4 comes from validity checks that ensure that safe data manipulation, and most important for our discussion, from its extensibility through slots. If we imagine the sce object to be a ship, the slots of sce can be thought of as individual cargo boxes - each exists as a separate entity within the sce object. Furthermore, each slot contains data that arrives in its own format. To extend the metaphor, we can imagine that different variations of cargo boxes are required for fruits versus bricks. In the case of sce, certain slots expect numeric matrices, whereas others may expect data frames. To construct a rudimentary sce object, all we need is a single slot: assays slot: contains primary data such as counts in list, where each entry of the list is in a matrix format, where rows correspond to features (genes) and columns correspond to samples (cells) (Figure 1A, blue box) Let’s start simple by generating three cells worth of count data across ten genes. counts_matrix &lt;- data.frame(cell_1 = rpois(10, 10), cell_2 = rpois(10, 10), cell_3 = rpois(10, 30)) rownames(counts_matrix) &lt;- paste0(&quot;gene_&quot;, 1:10) counts_matrix &lt;- as.matrix(counts_matrix) # must be a matrix object! From this, we can now construct our first SingleCellExperiment object, using the defined constructor, SingleCellExperiment(). Note that we provide our data as a named list, and each entry of the list is a matrix. Here, we name the counts_matrix entry as simply counts within the list. sce &lt;- SingleCellExperiment(assays = list(counts = counts_matrix)) To inspect the object, we can simply type sce into the console to see some pertinent information, which will display an overview of the various slots available to us (which may or may not have any data). sce ## class: SingleCellExperiment ## dim: 10 3 ## metadata(0): ## assays(1): counts ## rownames(10): gene_1 gene_2 ... gene_9 gene_10 ## rowData names(0): ## colnames(3): cell_1 cell_2 cell_3 ## colData names(0): ## reducedDimNames(0): ## spikeNames(0): To access the count data we just supplied, we can do any one of the following: assay(sce, \"counts\") - this is the most general method, where we can supply the name of the assay as the second argument. counts(sce) - this is the same as the above, but only works for assays with the special name \"counts\". counts(sce) ## cell_1 cell_2 cell_3 ## gene_1 4 11 36 ## gene_2 12 15 31 ## gene_3 6 6 28 ## gene_4 11 9 44 ## gene_5 8 7 36 ## gene_6 10 13 28 ## gene_7 7 10 31 ## gene_8 14 5 30 ## gene_9 13 10 42 ## gene_10 12 4 18 ## assay(sce, &quot;counts&quot;) ## same as above in this special case 4.2.2 Extending the assays Slot What makes the assay slot especially powerful is that it can hold multiple representations of the primary data. This is especially useful for storing a normalized version of the data. We can do just that as shown below, using the scran and scater packages to compute a log-count normalized representation of the initial primary data. Note that here, we overwrite our previous sce upon reassigning the results to sce - this is because these functions return a SingleCellExperiment object. Some functions - especially those outside of single-cell oriented Bioconductor packages - do not, in which case you will need to append your results to the sce object (see below). sce &lt;- scran::computeSumFactors(sce) sce &lt;- scater::normalize(sce) Viewing the object again, we see that these functions added some new entries: sce ## class: SingleCellExperiment ## dim: 10 3 ## metadata(1): log.exprs.offset ## assays(2): counts logcounts ## rownames(10): gene_1 gene_2 ... gene_9 gene_10 ## rowData names(0): ## colnames(3): cell_1 cell_2 cell_3 ## colData names(0): ## reducedDimNames(0): ## spikeNames(0): Specifically, we see that the assays slot has grown to be comprised of two entries: counts (our initial data) and logcounts (the normalized data). Similar to counts, the logcounts name is a special name which lets us access it simply by typing logcounts(sce), although the longhand version works just as well. logcounts(sce) ## cell_1 cell_2 cell_3 ## gene_1 3.00 4.45 4.32 ## gene_2 4.46 4.88 4.11 ## gene_3 3.53 3.63 3.97 ## gene_4 4.34 4.17 4.59 ## gene_5 3.91 3.83 4.32 ## gene_6 4.21 4.68 3.97 ## gene_7 3.73 4.32 4.11 ## gene_8 4.68 3.39 4.07 ## gene_9 4.57 4.32 4.53 ## gene_10 4.46 3.10 3.39 ## assay(sce, &quot;logcounts&quot;) ## same as above Notice that the data before had a severe discrepancy in counts between cells 1/2 versus 3, and that normalization has ameliorated this difference. To look at all the available assays within sce, we can type: assays(sce) ## List of length 2 ## names(2): counts logcounts While the functions above demonstrate automatic addition of assays to our sce object, there may be cases where we want to perform our own calculations and save the result into the assays slot. In particular, this is important for using functions that do not return your SingleCellExperiment object. Let’s append a new version of the data that has been offset by +100. counts_100 &lt;- assay(sce, &quot;counts&quot;) + 100 assay(sce, &quot;counts_100&quot;) &lt;- counts_100 # assign a new entry to assays slot Then we can use the accessor assays() (notice this is plural!) to see all our entries into the assay slot that we have made so far. Note that to see the names of all the assays, we use the plural assays() accessor, and to retrieve a single assay entry (as a matrix) we use the singular assay() accessor, providing the name of the assay we wish to retrieve as above. assays(sce) ## List of length 3 ## names(3): counts logcounts counts_100 These entries are also seen on the default view of sce: sce ## class: SingleCellExperiment ## dim: 10 3 ## metadata(1): log.exprs.offset ## assays(3): counts logcounts counts_100 ## rownames(10): gene_1 gene_2 ... gene_9 gene_10 ## rowData names(0): ## colnames(3): cell_1 cell_2 cell_3 ## colData names(0): ## reducedDimNames(0): ## spikeNames(0): This sort of extension of the assays slot is represented graphically in Figure 1B (dark blue box), showing the addition of the logcounts matrix into the assays slot. In a similar manner, many of the slots of sce are extendable through assignment as shown above, thus allowing for myriad custom functionality as needed for interoperability with functions outside of single-cell oriented Bioconductor packages. 4.2.3 Column (Meta)Data: colData Slot To further annotate our sce object, one of the first and most useful pieces of information is adding on metadata that describes the columns of our primary data, e.g. describing the samples or cells of our experiment. This data is entered into the colData slot: colData slot: metadata that describes that samples (cells) provided as a data.frame or (DataFrame if appending), where rows correspond to cells, and columns correspond to the sample (cells) metadata features (e.g. id, batch, author, etc.) (Figure 1A, orange box). So, let’s come up with some metadata for the cells, starting with a batch variable, where cells 1 and 2 are in batch 1, and cell 3 is from batch 2. cell_metadata &lt;- data.frame(batch = c(1, 1, 2)) rownames(cell_metadata) &lt;- paste0(&quot;cell_&quot;, 1:3) Now, we can take two approaches - either append the cell_metadata to our existing sce, or start from scratch via the SingleCellExperiment() constructor and provide it from the get go. We’ll start from scratch for now, but will also show how to append the data as well: ## From scratch: sce &lt;- SingleCellExperiment(assays = list(counts = counts_matrix), colData = cell_metadata) ## Appending to existing object (requires DataFrame() coercion) ## colData(sce) &lt;- DataFrame(cell_metadata) Similar to assays, we can see our colData is now populated from the default view of sce: sce ## class: SingleCellExperiment ## dim: 10 3 ## metadata(0): ## assays(1): counts ## rownames(10): gene_1 gene_2 ... gene_9 gene_10 ## rowData names(0): ## colnames(3): cell_1 cell_2 cell_3 ## colData names(1): batch ## reducedDimNames(0): ## spikeNames(0): And furthermore access our column (meta)data with the accessor, colData(): colData(sce) ## DataFrame with 3 rows and 1 column ## batch ## &lt;numeric&gt; ## cell_1 1 ## cell_2 1 ## cell_3 2 Finally, some packages automatically add to the colData slot, for example, the scater package features a function, calculateQCMetrics(), which appends a lot of quality control data. Here we show the first five columns of colData(sce) with the quality control metrics appended to it. sce &lt;- scater::calculateQCMetrics(sce) colData(sce)[, 1:5] ## DataFrame with 3 rows and 5 columns ## batch is_cell_control total_features_by_counts ## &lt;numeric&gt; &lt;logical&gt; &lt;integer&gt; ## cell_1 1 FALSE 10 ## cell_2 1 FALSE 10 ## cell_3 2 FALSE 10 ## log10_total_features_by_counts total_counts ## &lt;numeric&gt; &lt;integer&gt; ## cell_1 1.04139268515823 97 ## cell_2 1.04139268515823 90 ## cell_3 1.04139268515823 324 4.2.3.1 Using colData for Subsetting A common operation with colData is its use in subsetting. One simple way to access colData is through the use of the $ operator, which is a shortcut for accessing a variable within the colData slot: sce$batch ## [1] 1 1 2 ## colData(sce)$batch # same as above If we only wanted cells within batch 1, we could subset our sce object as follows (remember, we subset on the columns in this case because we are filtering by cells/samples here). sce[, sce$batch == 1] ## class: SingleCellExperiment ## dim: 10 2 ## metadata(0): ## assays(1): counts ## rownames(10): gene_1 gene_2 ... gene_9 gene_10 ## rowData names(7): is_feature_control mean_counts ... total_counts ## log10_total_counts ## colnames(2): cell_1 cell_2 ## colData names(10): batch is_cell_control ... ## pct_counts_in_top_200_features pct_counts_in_top_500_features ## reducedDimNames(0): ## spikeNames(0): 4.2.4 Feature Metadata: rowData/rowRanges Lastly, the rows also have their own metadata slot to store information that pertains to the features of the sce object: rowData slot: contains data in a data.frame (DataFrame) format that describes aspects of the data corresponding to the rows of the primary data (Figure 1A, green box). Furthermore, there is a special slot which pertains to features with genomic coordinates: rowRanges slot: contains data in a GRangesList (where each entry is a GenomicRanges format) that describes the chromosome, start, and end coordinates of the features (genes, genomic regions). Both of these can be accessed via their respective accessors, rowRanges() and rowData(). In our case, rowRanges(sce) produces an empty list: rowRanges(sce) # empty ## GRangesList object of length 10: ## $gene_1 ## GRanges object with 0 ranges and 0 metadata columns: ## seqnames ranges strand ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; ## ## $gene_2 ## GRanges object with 0 ranges and 0 metadata columns: ## seqnames ranges strand ## ## $gene_3 ## GRanges object with 0 ranges and 0 metadata columns: ## seqnames ranges strand ## ## ... ## &lt;7 more elements&gt; ## ------- ## seqinfo: no sequences However, our call to calculateQCMetrics(sce) in the prior section filled in the rowData slot of our sce object, as we can see below (only the first three columns are shown for brevity): rowData(sce)[, 1:3] ## DataFrame with 10 rows and 3 columns ## is_feature_control mean_counts log10_mean_counts ## &lt;logical&gt; &lt;numeric&gt; &lt;numeric&gt; ## gene_1 FALSE 17 1.25527250510331 ## gene_2 FALSE 19.3333333333333 1.3082085802911 ## gene_3 FALSE 13.3333333333333 1.15634720085992 ## gene_4 FALSE 21.3333333333333 1.34895354798116 ## gene_5 FALSE 17 1.25527250510331 ## gene_6 FALSE 17 1.25527250510331 ## gene_7 FALSE 16 1.23044892137827 ## gene_8 FALSE 16.3333333333333 1.23888208891514 ## gene_9 FALSE 21.6666666666667 1.35538765798657 ## gene_10 FALSE 11.3333333333333 1.09108046934733 In a similar fashion to the colData slot, such feature metadata could be provided at the onset when creating the SingleCellExperiment object, which we leave up to the reader as an exercise. 4.2.4.1 Subsetting with on Rows To subset an sce object down at the feature/gene level, we can do a row subsetting operation similar to other R objects, by supplying either numeric indices or a vector of names: sce[c(&quot;gene_1&quot;, &quot;gene_4&quot;), ] ## class: SingleCellExperiment ## dim: 2 3 ## metadata(0): ## assays(1): counts ## rownames(2): gene_1 gene_4 ## rowData names(7): is_feature_control mean_counts ... total_counts ## log10_total_counts ## colnames(3): cell_1 cell_2 cell_3 ## colData names(10): batch is_cell_control ... ## pct_counts_in_top_200_features pct_counts_in_top_500_features ## reducedDimNames(0): ## spikeNames(0): ## sce[c(1, 4), ] # same as above in this case 4.2.5 Size Factors Slot: sizeFactors Briefly, we already encountered this via the scran::computeSumFactors(sce) call, which adds a sizeFactors slot: sizeFactors slot: contains information in a numeric vector regarding the sample/cell normalization factors used to produce a normalize data representation (Figure 1B, brown box) sce &lt;- scran::computeSumFactors(sce) sce &lt;- scater::normalize(sce) sizeFactors(sce) ## [1] 0.569 0.528 1.902 4.3 A Brief Recap: From se to sce So far, we have covered the assays (primary data), colData (sample metadata), rowData/rowRanges (feature metadata), and sizeFactors slots of SingleCellExperiment. What is important to note is that the SingleCellExperiment class derives from the SummarizedExperiment (se) class, its predecessor, and in particular inherits the aforementioned slots. As such, much of the SummarizedExperiment functionality is retained in SingleCellExperiment. This allows existing methods that work with SummarizedExperiment to work similarly on SingleCellExperiment objects. So what’s new about the SingleCellExperiment class then? For our discussion, the most important change is the addition of a new slot called reducedDims. 4.4 The reducedDims Slot The reducedDims slot is a new addition which is specially designed to store the reduced dimensionality representations of primary data, such as PCA, tSNE, UMAP, and others. reducedDims slot: contains a list of numeric matrix entries which describe dimensionality reduced representations of the primary data, such that rows represent the columns of the primary data (aka the samples/cells), and columns represent the dimensions Most importantly, just like the assays slot, the reducedDims slot can hold a list of many entries. So, it can hold a PCA, TSNE, and UMAP representation of a given dataset all within the reducedDims slot. In our example, we can calculate a PCA representation of our data as follows using the scater package function runPCA(). We see that the sce now shows a new reducedDim and that the accessor reducedDim() produces the results of running PCA on the normalized data from logcounts(sce). sce &lt;- scater::runPCA(sce) reducedDim(sce, &quot;PCA&quot;) ## PC1 PC2 ## cell_1 1.48 1.9812 ## cell_2 -2.81 0.0748 ## cell_3 1.33 -2.0561 ## attr(,&quot;percentVar&quot;) ## [1] 5.92e-01 4.08e-01 3.98e-31 From this, we can also calculate a tSNE representation using the scater package function runTSNE(), and see that it can be seen both in the default view of sce and via accession: sce &lt;- scater::runTSNE(sce, perplexity = 0.1) ## Perplexity should be lower than K! reducedDim(sce, &quot;TSNE&quot;) ## [,1] [,2] ## cell_1 5667 -558 ## cell_2 -2349 5188 ## cell_3 -3318 -4630 We can view the names of all our entries in the reducedDims slot via the accessor, reducedDims() (notice that this is plural, and thus not the same as reducedDim(): reducedDims(sce) ## List of length 2 ## names(2): PCA TSNE Now, say we have a different dimensionality reduction approach which has not yet been implemented with SingleCellExperiment objects in mind. For example, let’s say we want to try the umap() function as implemented in the uwot package (which is a much faster version of the default umap implementation currently in scater). Similar to how we extended the assays slot with our own custom entry of counts_100, we can do similarly for the reducedDims slot: u &lt;- uwot::umap(t(logcounts(sce)), n_neighbors = 2) reducedDim(sce, &quot;UMAP_uwot&quot;) &lt;- u reducedDim(sce, &quot;UMAP_uwot&quot;) ## [,1] [,2] ## cell_1 -0.617 -0.639 ## cell_2 1.719 1.734 ## cell_3 -1.102 -1.095 ## attr(,&quot;scaled:center&quot;) ## [1] -6.36 -22.33 And we can also see its entry when we look at the reducedDims() accessor output: reducedDims(sce) ## List of length 3 ## names(3): PCA TSNE UMAP_uwot 4.5 One More Thing: metadata Slot Some analyses produce results that do not fit into the aforementioned slots. Thankfully, there is a slot just for this type of messy data, and in fact, can accommodate any type of data, so long as it is in a named list: metadata slot: a named list of entries, where each entry in the list can be anything you want it to be For example, say we have some favorite genes, such as highly variable genes, we want to save inside of sce for use in our analysis at a later point. We can do this simply by appending to the metadata slot as follows: my_genes &lt;- c(&quot;gene_1&quot;, &quot;gene_5&quot;) metadata(sce) &lt;- list(favorite_genes = my_genes) metadata(sce) ## $favorite_genes ## [1] &quot;gene_1&quot; &quot;gene_5&quot; Similarly, we can append more information via the $ operator: your_genes &lt;- c(&quot;gene_4&quot;, &quot;gene_8&quot;) metadata(sce)$your_genes &lt;- your_genes metadata(sce) ## $favorite_genes ## [1] &quot;gene_1&quot; &quot;gene_5&quot; ## ## $your_genes ## [1] &quot;gene_4&quot; &quot;gene_8&quot; 4.6 About Spike-Ins You might have noticed that the sce default view produces an entry with spikeNames. The SingleCellExperiment object contains some special considerations for experiments with spike-in (ERCC) controls. We leave this to the interested reader to learn more about in the SingleCellExperiment introductory vignette. 4.7 Working with SingleCellExperiment Figure 1C shows an example workflow that uses the SingleCellExperiment object as its base, and similar to our walkthrough of the sce class above, continually appends new entries to save the results of the analysis. In the workflows that follow in this book, we will be similarly appending to an initial sce object many of our analytical results. "],
["about-the-data.html", "Chapter 5 About the Data 5.1 10X Genomics PBMC Data 5.2 Human Cell Atlas", " Chapter 5 About the Data 5.1 10X Genomics PBMC Data Bioconductor versions, PBMC4k/PBMC3k. 5.2 Human Cell Atlas Bioconductor access. d = c(&#39;a&#39;, &#39;b&#39;) "],
["workflow-integrating-datasets.html", "Chapter 6 Workflow: Integrating Datasets 6.1 Package Requirements 6.2 Loading the Data 6.3 Preprocessing 6.4 Feature Selection 6.5 Combining the Datasets 6.6 Integrating Datasets 6.7 Session Info", " Chapter 6 Workflow: Integrating Datasets The purpose of this case study is to demonstrate how to integrate multiple scRNA-seq datasets using R/Bioconductor packages. In this workflow, we go from preprocessing the data to integrating the data and visualizing it in a reduced dimensionality space to showcase the success of the integration approach using mutual nearest neighbors relative to a naive approach. This approach is helpful for ameliorating batch effects that can be introduced when combining data from different sequencing runs and/or platforms. Here, we will be combining two datasets from 10X Genomics PBMC Data. One dataset is comprised of 3000 PBMCs from a healthy donor, and the other dataset is comprised of 4000 PBMCs from a different healthy donor. Our goal is to produce an integrated representation of this data to facilitate downstream analysis, such as clustering. 6.1 Package Requirements These packages will be required for working through the vignette, and can be installed by running the code below. The data that we will be using comes from the TENxPBMCData package. ## required BiocManager::install(c(&#39;scater&#39;, &#39;scran&#39;, &#39;limma&#39;, &#39;TENxPBMCData&#39;)) ## suggested BiocManager::install(c(&#39;BiocParallel&#39;, &#39;BiocNeighbors&#39;)) library(scater) library(scran) library(limma) library(TENxPBMCData) library(BiocParallel) library(BiocNeighbors) 6.2 Loading the Data Here we will be combining two different runs of scRNA-seq data - each from different healthy donors, and comprised of either 3000 cells (pbmc3k) or 4000 cells (pbmc4k). Note that these objects are already SingleCellExperiment objects. pbmc3k &lt;- TENxPBMCData(&#39;pbmc3k&#39;) pbmc4k &lt;- TENxPBMCData(&#39;pbmc4k&#39;) 6.3 Preprocessing Here we walk through the steps required to produce a clean expression matrix, taking the raw count data through to a normalized representation. 6.3.1 Working with Common Genes First, we find intersection of gene names and keep only the entries that are in common between the two datasets. We then reduce each of the individual datasets down to these matching entries (keep_genes) by subsetting. keep_genes &lt;- intersect(rownames(pbmc3k), rownames(pbmc4k)) pbmc3k &lt;- pbmc3k[match(keep_genes, rownames(pbmc3k)), ] pbmc4k &lt;- pbmc4k[na.omit(match(keep_genes, rownames(pbmc4k))), ] 6.3.2 Cell and Gene Quality Control First, for the combined data sce and the individual datasets pbmc3k and pbmc4k, we calculate essential quality control characteristics using the scater function calculateQCMetrics(). We then determine cells low quality cells by finding outliers with uncharacteristically low total counts or total number of features (genes) detected. We automate this into a function. ## For pbmc3k pbmc3k &lt;- calculateQCMetrics(pbmc3k) low_lib_pbmc3k &lt;- isOutlier(pbmc3k$log10_total_counts, type=&quot;lower&quot;, nmad=3) low_genes_pbmc3k &lt;- isOutlier(pbmc3k$log10_total_features_by_counts, type=&quot;lower&quot;, nmad=3) ## For pbmc4k pbmc4k &lt;- calculateQCMetrics(pbmc4k) low_lib_pbmc4k &lt;- isOutlier(pbmc4k$log10_total_counts, type=&quot;lower&quot;, nmad=3) low_genes_pbmc4k &lt;- isOutlier(pbmc4k$log10_total_features_by_counts, type=&quot;lower&quot;, nmad=3) These results flag approximately 30 to 100 cells for removal from each of the datasets. We can then further subset our data to remove these cells by running the following: pbmc3k &lt;- pbmc3k[,!(low_lib_pbmc3k | low_genes_pbmc3k)] pbmc4k &lt;- pbmc4k[,!(low_lib_pbmc4k | low_genes_pbmc4k)] 6.3.3 Normalization From here, we now compute the size factors using the scran package’s computeSumFactors() function, and apply the size factors via the scran package’s normalize() function to produce a new assay, logcounts, within each SingleCellExperiment object. ## compute the sizeFactors pbmc3k &lt;- computeSumFactors(pbmc3k) pbmc4k &lt;- computeSumFactors(pbmc4k) ## Normalize (using already calculated size factors) pbmc3k &lt;- normalize(pbmc3k) pbmc4k &lt;- normalize(pbmc4k) 6.3.4 Multibatch Normalization We also rescale each batch to adjust for differences in sequencing depth between batches. The multiBatchNorm() function from the scran package recomputes log-normalized expression values after adjusting the size factors for systematic differences in coverage between SingleCellExperiment objects. The previously computed size factors only remove biases between cells within a single batch. This improves the quality of the correction step by removing one aspect of the technical differences between batches. rescaled &lt;- multiBatchNorm(pbmc3k, pbmc4k) pbmc3k &lt;- rescaled[[1]] pbmc4k &lt;- rescaled[[2]] 6.4 Feature Selection A key step across many data integration methods is the selection of informative features across the different experiments. This helps to speed up computation and possibly improve the resulting integration. Once again, we rely on the scran package to identify the genes with the highest biological coefficient of variation, using the trendVar() and decomposeVar() functions to calculate the per gene variance and separate it into technical versus biological components. We perform this for each individual dataset: fit_pbmc3k &lt;- trendVar(pbmc3k, use.spikes=FALSE) dec_pbmc3k &lt;- decomposeVar(pbmc3k, fit_pbmc3k) dec_pbmc3k$Symbol_TENx &lt;- rowData(pbmc3k)$Symbol_TENx dec_pbmc3k &lt;- dec_pbmc3k[order(dec_pbmc3k$bio, decreasing = TRUE), ] fit_pbmc4k &lt;- trendVar(pbmc4k, use.spikes=FALSE) dec_pbmc4k &lt;- decomposeVar(pbmc4k, fit_pbmc4k) dec_pbmc4k$Symbol_TENx &lt;- rowData(pbmc4k)$Symbol_TENx dec_pbmc4k &lt;- dec_pbmc4k[order(dec_pbmc4k$bio, decreasing = TRUE), ] Then select the most informative genes that are shared across both datasets: universe &lt;- intersect(rownames(dec_pbmc3k), rownames(dec_pbmc4k)) mean.bio &lt;- (dec_pbmc3k[universe,&quot;bio&quot;] + dec_pbmc4k[universe,&quot;bio&quot;])/2 hvg_genes &lt;- universe[mean.bio &gt; 0] 6.5 Combining the Datasets Finally, we combine the datasets into a unified SingleCellExperiment object for the downstream integration approaches, now that the data has been normalized (both within and between datasets) and the shared most informative features have been identified. ## total raw counts counts_pbmc &lt;- cbind(counts(pbmc3k), counts(pbmc4k)) ## total normalized counts (with multibatch normalization) logcounts_pbmc &lt;- cbind(logcounts(pbmc3k), logcounts(pbmc4k)) sce &lt;- SingleCellExperiment( assays = list(counts = counts_pbmc, logcounts = logcounts_pbmc), rowData = rowData(pbmc3k), # same as rowData(pbmc4k) colData = rbind(colData(pbmc3k), colData(pbmc4k)) ) For safekeeping, we will also store the hvg_genes from the prior section into the sce object via: metadata(sce)$hvg_genes &lt;- hvg_genes 6.6 Integrating Datasets Here we will now be comparing the results of different approaches to integration. 6.6.1 Naive Approach The naive approach simply entails visualizing the combined sce object post-normalization with no attempt at batch correction. Here we manually calculate the PCA on the normalized data (retrieved via logcounts(sce) or, similarly, via assay(sce, \"logcounts\"), and then assign the result into the reducedDim slot of sce, naming it \"PCA_naive\". px &lt;- prcomp(t(logcounts(sce)[hvg_genes, ])) reducedDim(sce, &quot;PCA_naive&quot;) &lt;- px$x[, 1:20] plotReducedDim(sce, use_dimred = &quot;PCA_naive&quot;, colour_by=&quot;Sample&quot;) + ggtitle(&quot;PCA Without batch correction&quot;) 6.6.2 Limma Batch Correction The limma package, a popular framework for the statistical analysis of RNA-seq, has a function removeBatchEffect() which will be used here to correct the normalized expression matrix logcounts across the two batches. The result will be assigned into the assays slot of the sce object as limma_corrected, and then used for PCA, saving the result in the reducedDim slot as \"PCA_limma\". limma_corrected &lt;- limma::removeBatchEffect(logcounts(sce), batch = sce$Sample) assay(sce, &quot;logcounts_limma&quot;) &lt;- limma_corrected ## add new assay pl &lt;- prcomp(t(assay(sce, &#39;logcounts_limma&#39;)[hvg_genes, ])) reducedDim(sce, &quot;PCA_limma&quot;) &lt;- pl$x[, 1:20] plotReducedDim(sce, use_dimred = &quot;PCA_limma&quot;, colour_by=&quot;Sample&quot;) + ggtitle(&quot;PCA With limma removeBatchEffect() correction&quot;) 6.6.3 MNN Approach The mutual nearest neighbors (MNN) approach within the scran package utilizes a novel approach to adjust for batch effects. The fastMNN() function returns a representation of the data with reduced dimensionality, which can be used in a similar fashion to other lower-dimensional representations such as PCA. In particular, this representation can be used for downstream methods such as clustering. Where fastMNN() differs from other integration methods such as the limma approach above is that it does not produce a batch-corrected expression matrix. Thus, the result from fastMNN() should solely be treated as a reduced dimensionality representation, suitable for direct plotting, TSNE/UMAP, clustering, and trajectory analysis that relies on such results. The already (batch) normalized (via normalize() and multiBatchNorm()) can be supplied to other statistical frameworks that are better suited to handle batch effects, such as in the case of differential expression. ## Basic method to run - not run mnn_out &lt;- fastMNN(sce[hvg_genes, sce$Sample == &quot;pbmc3k&quot;], sce[hvg_genes, sce$Sample == &quot;pbmc4k&quot;], ## subset.row = hvg_genes, ## same as subsetting above k = 20, d = 50, approximate = TRUE, BNPARAM = BiocNeighbors::AnnoyParam(), BPPARAM = BiocParallel::multiCoreParam()) ## Adding parallelization and Annoy method for approximate nearest neighbors ## this makes fastMNN significantly faster on large data mnn_out &lt;- fastMNN(sce[hvg_genes, sce$Sample == &quot;pbmc3k&quot;], sce[hvg_genes, sce$Sample == &quot;pbmc4k&quot;], ## subset.row = hvg_genes, ## same as subsetting above k = 20, d = 50, approximate = TRUE, BNPARAM = BiocNeighbors::AnnoyParam(), BPPARAM = BiocParallel::MulticoreParam(8)) reducedDim(sce, &quot;MNN&quot;) &lt;- mnn_out$correct plotReducedDim(sce, use_dimred = &quot;MNN&quot;, colour_by=&quot;Sample&quot;) + ggtitle(&quot;MNN Ouput Reduced Dimensions&quot;) 6.7 Session Info sessionInfo() ## R Under development (unstable) (2019-01-13 r75986) ## Platform: x86_64-apple-darwin18.2.0 (64-bit) ## Running under: macOS Mojave 10.14.3 ## ## Matrix products: default ## BLAS/LAPACK: /usr/local/Cellar/openblas/0.3.5/lib/libopenblasp-r0.3.5.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats4 stats graphics grDevices utils datasets ## [8] methods base ## ## other attached packages: ## [1] BiocNeighbors_1.1.12 TENxPBMCData_1.1.0 ## [3] HDF5Array_1.11.10 rhdf5_2.27.12 ## [5] limma_3.39.12 scran_1.11.20 ## [7] scater_1.11.10 ggplot2_3.1.0 ## [9] SingleCellExperiment_1.5.2 SummarizedExperiment_1.13.0 ## [11] DelayedArray_0.9.8 BiocParallel_1.17.17 ## [13] matrixStats_0.54.0 Biobase_2.43.1 ## [15] GenomicRanges_1.35.1 GenomeInfoDb_1.19.2 ## [17] IRanges_2.17.4 S4Vectors_0.21.10 ## [19] BiocGenerics_0.29.1 ## ## loaded via a namespace (and not attached): ## [1] bitops_1.0-6 bit64_0.9-7 ## [3] httr_1.4.0 dynamicTreeCut_1.63-1 ## [5] tools_3.6.0 R6_2.4.0 ## [7] vipor_0.4.5 DBI_1.0.0 ## [9] lazyeval_0.2.1 colorspace_1.4-0 ## [11] withr_2.1.2 tidyselect_0.2.5 ## [13] gridExtra_2.3 bit_1.1-14 ## [15] compiler_3.6.0 labeling_0.3 ## [17] bookdown_0.9 scales_1.0.0 ## [19] stringr_1.4.0 digest_0.6.18 ## [21] rmarkdown_1.11 XVector_0.23.0 ## [23] pkgconfig_2.0.2 htmltools_0.3.6 ## [25] rlang_0.3.1 RSQLite_2.1.1 ## [27] shiny_1.2.0 DelayedMatrixStats_1.5.2 ## [29] dplyr_0.8.0.1 RCurl_1.95-4.11 ## [31] magrittr_1.5 GenomeInfoDbData_1.2.0 ## [33] Matrix_1.2-15 Rcpp_1.0.0 ## [35] ggbeeswarm_0.6.0 munsell_0.5.0 ## [37] Rhdf5lib_1.5.1 viridis_0.5.1 ## [39] stringi_1.3.1 yaml_2.2.0 ## [41] edgeR_3.25.3 zlibbioc_1.29.0 ## [43] plyr_1.8.4 AnnotationHub_2.15.7 ## [45] grid_3.6.0 blob_1.1.1 ## [47] promises_1.0.1 ExperimentHub_1.9.1 ## [49] crayon_1.3.4 lattice_0.20-38 ## [51] cowplot_0.9.4 locfit_1.5-9.1 ## [53] knitr_1.21 pillar_1.3.1 ## [55] igraph_1.2.4 glue_1.3.0 ## [57] evaluate_0.13 BiocManager_1.30.4 ## [59] httpuv_1.4.5.1 gtable_0.2.0 ## [61] purrr_0.3.0 assertthat_0.2.0 ## [63] xfun_0.5 mime_0.6 ## [65] xtable_1.8-3 later_0.8.0 ## [67] viridisLite_0.3.0 tibble_2.0.1 ## [69] AnnotationDbi_1.45.0 beeswarm_0.2.3 ## [71] memoise_1.1.0 statmod_1.4.30 ## [73] interactiveDisplayBase_1.21.0 saveRDS(sce, &quot;_rfiles/_data/integration_sce.rds&quot;, compress = &quot;xz&quot;) "],
["workflow-clustering.html", "Chapter 7 Workflow: Clustering", " Chapter 7 Workflow: Clustering "]
]
